```markdown
---
title: Layerup Security
---

[Layerup Security](https://uselayerup.com) 통합을 사용하면 모든 LangChain LLM, LLM chain 또는 LLM agent 호출을 보호할 수 있습니다. LLM 객체는 기존 LLM 객체를 감싸서 사용자와 LLM 사이에 보안 계층을 제공합니다.

Layerup Security 객체는 LLM으로 설계되었지만, 실제로는 LLM 자체가 아니며 단순히 LLM을 감싸서 기본 LLM과 동일한 기능을 적용할 수 있도록 합니다.

## Setup

먼저 Layerup [웹사이트](https://uselayerup.com)에서 Layerup Security 계정이 필요합니다.

다음으로 [대시보드](https://dashboard.uselayerup.com)를 통해 프로젝트를 생성하고 API key를 복사하세요. API key는 프로젝트의 환경 변수에 저장하는 것을 권장합니다.

Layerup Security SDK를 설치하세요:

```bash npm
npm install @layerup/layerup-security
```
그리고 LangChain Community를 설치하세요:

```bash npm
npm install @langchain/community @langchain/core
```

이제 Layerup Security로 LLM 호출을 보호할 준비가 되었습니다!

```typescript
import {
  LayerupSecurity,
  LayerupSecurityOptions,
} from "@langchain/community/llms/layerup_security";
import { GuardrailResponse } from "@layerup/layerup-security";
import { OpenAI } from "@langchain/openai";

// Create an instance of your favorite LLM
const openai = new OpenAI({
  modelName: "gpt-3.5-turbo",
  openAIApiKey: process.env.OPENAI_API_KEY,
});

// Configure Layerup Security
const layerupSecurityOptions: LayerupSecurityOptions = {
  // Specify a LLM that Layerup Security will wrap around
  llm: openai,

  // Layerup API key, from the Layerup dashboard
  layerupApiKey: process.env.LAYERUP_API_KEY,

  // Custom base URL, if self hosting
  layerupApiBaseUrl: "https://api.uselayerup.com/v1",

  // List of guardrails to run on prompts before the LLM is invoked
  promptGuardrails: [],

  // List of guardrails to run on responses from the LLM
  responseGuardrails: ["layerup.hallucination"],

  // Whether or not to mask the prompt for PII & sensitive data before it is sent to the LLM
  mask: false,

  // Metadata for abuse tracking, customer tracking, and scope tracking.
  metadata: { customer: "example@uselayerup.com" },

  // Handler for guardrail violations on the response guardrails
  handlePromptGuardrailViolation: (violation: GuardrailResponse) => {
    if (violation.offending_guardrail === "layerup.sensitive_data") {
      // Custom logic goes here
    }

    return {
      role: "assistant",
      content: `There was sensitive data! I cannot respond. Here's a dynamic canned response. Current date: ${Date.now()}`,
    };
  },

  // Handler for guardrail violations on the response guardrails
  handleResponseGuardrailViolation: (violation: GuardrailResponse) => ({
    role: "assistant",
    content: `Custom canned response with dynamic data! The violation rule was ${violation.offending_guardrail}.`,
  }),
};

const layerupSecurity = new LayerupSecurity(layerupSecurityOptions);
const response = await layerupSecurity.invoke(
  "Summarize this message: my name is Bob Dylan. My SSN is 123-45-6789."
);
```

## Related


- [Models 가이드](/oss/javascript/langchain/models)
```

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/llms/layerup_security.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
