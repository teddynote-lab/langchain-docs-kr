---
title: WebLLM
---

<Tip>
**호환성**

웹 환경에서만 사용 가능합니다.
</Tip>

LangChain의 [WebLLM](https://webllm.mlc.ai) 통합을 사용하여 웹 브라우저에서 직접 LLM을 실행할 수 있습니다.

## Setup

로컬 모델과 통신하려면 [WebLLM SDK](https://www.npmjs.com/package/@mlc-ai/web-llm) 모듈을 설치해야 합니다.

<Tip>
[LangChain 패키지 설치에 대한 일반적인 지침은 이 섹션을 참조하세요](/oss/javascript/langchain/install).
</Tip>

```bash npm
npm install -S @mlc-ai/web-llm @langchain/community @langchain/core
```

## Usage

모델이 처음 호출될 때 WebLLM은 해당 모델의 전체 가중치를 다운로드합니다. 이는 수 기가바이트에 달할 수 있으며, 인터넷 연결 및 컴퓨터 사양에 따라 애플리케이션의 모든 최종 사용자에게 가능하지 않을 수 있습니다.
브라우저가 해당 모델의 향후 호출을 캐시하지만, 가능한 한 가장 작은 모델을 사용하는 것이 좋습니다.

또한 실행을 차단하지 않도록 모델을 호출하고 로드할 때 [별도의 web worker](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers)를 사용하는 것이 좋습니다.

```typescript
// Must be run in a web environment, e.g. a web worker

import { ChatWebLLM } from "@langchain/community/chat_models/webllm";
import { HumanMessage } from "@langchain/core/messages";

// Initialize the ChatWebLLM model with the model record and chat options.
// Note that if the appConfig field is set, the list of model records
// must include the selected model record for the engine.

// You can import a list of models available by default here:
// https://github.com/mlc-ai/web-llm/blob/main/src/config.ts
//
// Or by importing it via:
// import { prebuiltAppConfig } from "@mlc-ai/web-llm";
const model = new ChatWebLLM({
  model: "Phi-3-mini-4k-instruct-q4f16_1-MLC",
  chatOptions: {
    temperature: 0.5,
  },
});

await model.initialize((progress: Record<string, unknown>) => {
  console.log(progress);
});

// Call the model with a message and await the response.
const response = await model.invoke([
  new HumanMessage({ content: "What is 1 + 1?" }),
]);

console.log(response);

/*
AIMessage {
  content: ' 2\n',
}
*/
```

Streaming도 지원됩니다.

## Example

전체 엔드투엔드 예제는 [이 프로젝트](https://github.com/jacoblee93/fully-local-pdf-chatbot)를 확인하세요.

## Related

- Chat model [개념 가이드](/oss/javascript/langchain/models)
- Chat model [사용 방법 가이드](/oss/javascript/langchain/models)

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/chat/web_llm.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
