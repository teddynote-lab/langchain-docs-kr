---
title: ChatMinimax
---

[Minimax](https://api.minimax.chat)는 기업과 개인을 위한 자연어 처리 모델을 제공하는 중국 스타트업입니다.

이 예제는 LangChain.js를 사용하여 Minimax와 상호작용하는 방법을 보여줍니다.

## Setup

Minimax 모델을 사용하려면 Minimax 계정, API key, 그리고 Group ID가 필요합니다.

<Tip>
LangChain 패키지 설치에 대한 일반적인 지침은 [이 섹션](/oss/javascript/langchain/install)을 참조하세요.
</Tip>

```bash npm
npm install @langchain/community @langchain/core
```

<Tip>
모든 패키지에서 model parameter를 통합하고 있습니다. 이제 `modelName` 대신 `model`을 사용하고, API key는 `apiKey`를 사용할 것을 권장합니다.
</Tip>

## Basic usage

```typescript
import { ChatMinimax } from "@langchain/community/chat_models/minimax";
import { HumanMessage } from "@langchain/core/messages";

// Use abab5.5
const abab5_5 = new ChatMinimax({
  model: "abab5.5-chat",
  botSetting: [
    {
      bot_name: "MM Assistant",
      content: "MM Assistant is an AI Assistant developed by minimax.",
    },
  ],
});
const messages = [
  new HumanMessage({
    content: "Hello",
  }),
];

const res = await abab5_5.invoke(messages);
console.log(res);

/*
AIChatMessage {
  text: 'Hello! How may I assist you today?',
  name: undefined,
  additional_kwargs: {}
  }
}
*/

// use abab5
const abab5 = new ChatMinimax({
  proVersion: false,
  model: "abab5-chat",
  minimaxGroupId: process.env.MINIMAX_GROUP_ID, // In Node.js defaults to process.env.MINIMAX_GROUP_ID
  minimaxApiKey: process.env.MINIMAX_API_KEY, // In Node.js defaults to process.env.MINIMAX_API_KEY
});

const result = await abab5.invoke([
  new HumanMessage({
    content: "Hello",
    name: "XiaoMing",
  }),
]);
console.log(result);

/*
AIMessage {
  lc_serializable: true,
  lc_kwargs: {
    content: 'Hello! Can I help you with anything?',
    additional_kwargs: { function_call: undefined }
  },
  lc_namespace: [ 'langchain', 'schema' ],
  content: 'Hello! Can I help you with anything?',
  name: undefined,
  additional_kwargs: { function_call: undefined }
}
 */
```

## Chain model calls

```typescript
import { LLMChain } from "@langchain/classic/chains";
import { ChatMinimax } from "@langchain/community/chat_models/minimax";
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
} from "@langchain/core/prompts";

// We can also construct an LLMChain from a ChatPromptTemplate and a chat model.
const chat = new ChatMinimax({ temperature: 0.01 });

const chatPrompt = ChatPromptTemplate.fromMessages([
  SystemMessagePromptTemplate.fromTemplate(
    "You are a helpful assistant that translates {input_language} to {output_language}."
  ),
  HumanMessagePromptTemplate.fromTemplate("{text}"),
]);
const chainB = new LLMChain({
  prompt: chatPrompt,
  llm: chat,
});

const resB = await chainB.invoke({
  input_language: "English",
  output_language: "Chinese",
  text: "I love programming.",
});
console.log({ resB });
```

## With function calls

```typescript
import { ChatMinimax } from "@langchain/community/chat_models/minimax";
import { HumanMessage } from "@langchain/core/messages";

const functionSchema = {
  name: "get_weather",
  description: " Get weather information.",
  parameters: {
    type: "object",
    properties: {
      location: {
        type: "string",
        description: " The location to get the weather",
      },
    },
    required: ["location"],
  },
};

// Bind function arguments to the model.
// All subsequent invoke calls will use the bound parameters.
// "functions.parameters" must be formatted as JSON Schema
const model = new ChatMinimax({
  botSetting: [
    {
      bot_name: "MM Assistant",
      content: "MM Assistant is an AI Assistant developed by minimax.",
    },
  ],
}).withConfig({
  functions: [functionSchema],
});

const result = await model.invoke([
  new HumanMessage({
    content: " What is the weather like in NewYork tomorrow?",
    name: "I",
  }),
]);

console.log(result);

/*
AIMessage {
  lc_serializable: true,
  lc_kwargs: { content: '', additional_kwargs: { function_call: [Object] } },
  lc_namespace: [ 'langchain', 'schema' ],
  content: '',
  name: undefined,
  additional_kwargs: {
    function_call: { name: 'get_weather', arguments: '{"location": "NewYork"}' }
  }
}
*/

// Alternatively, you can pass function call arguments as an additional argument as a one-off:

const minimax = new ChatMinimax({
  model: "abab5.5-chat",
  botSetting: [
    {
      bot_name: "MM Assistant",
      content: "MM Assistant is an AI Assistant developed by minimax.",
    },
  ],
});

const result2 = await minimax.invoke(
  [new HumanMessage("What is the weather like in NewYork tomorrow?")],
  {
    functions: [functionSchema],
  }
);
console.log(result2);

/*
AIMessage {
  lc_serializable: true,
  lc_kwargs: { content: '', additional_kwargs: { function_call: [Object] } },
  lc_namespace: [ 'langchain', 'schema' ],
  content: '',
  name: undefined,
  additional_kwargs: {
    function_call: { name: 'get_weather', arguments: '{"location": "NewYork"}' }
  }
}
 */
```

## Functions with Zod

<MinimaxFunctionsZod />

## With glyph

이 기능은 사용자가 요청한 형식으로 콘텐츠를 반환하도록 모델을 강제하는 데 도움이 됩니다.

```typescript
import { ChatMinimax } from "@langchain/community/chat_models/minimax";
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
} from "@langchain/core/prompts";
import { HumanMessage } from "@langchain/core/messages";

const model = new ChatMinimax({
  model: "abab5.5-chat",
  botSetting: [
    {
      bot_name: "MM Assistant",
      content: "MM Assistant is an AI Assistant developed by minimax.",
    },
  ],
}).withConfig({
  replyConstraints: {
    sender_type: "BOT",
    sender_name: "MM Assistant",
    glyph: {
      type: "raw",
      raw_glyph: "The translated text：{{gen 'content'}}",
    },
  },
});

const messagesTemplate = ChatPromptTemplate.fromMessages([
  HumanMessagePromptTemplate.fromTemplate(
    " Please help me translate the following sentence in English： {text}"
  ),
]);

const messages = await messagesTemplate.formatMessages({ text: "我是谁" });
const result = await model.invoke(messages);

console.log(result);

/*
AIMessage {
  lc_serializable: true,
  lc_kwargs: {
    content: 'The translated text： Who am I\x02',
    additional_kwargs: { function_call: undefined }
  },
  lc_namespace: [ 'langchain', 'schema' ],
  content: 'The translated text： Who am I\x02',
  name: undefined,
  additional_kwargs: { function_call: undefined }
}
*/

// use json_value

const modelMinimax = new ChatMinimax({
  model: "abab5.5-chat",
  botSetting: [
    {
      bot_name: "MM Assistant",
      content: "MM Assistant is an AI Assistant developed by minimax.",
    },
  ],
}).withConfig({
  replyConstraints: {
    sender_type: "BOT",
    sender_name: "MM Assistant",
    glyph: {
      type: "json_value",
      json_properties: {
        name: {
          type: "string",
        },
        age: {
          type: "number",
        },
        is_student: {
          type: "boolean",
        },
        is_boy: {
          type: "boolean",
        },
        courses: {
          type: "object",
          properties: {
            name: {
              type: "string",
            },
            score: {
              type: "number",
            },
          },
        },
      },
    },
  },
});

const result2 = await modelMinimax.invoke([
  new HumanMessage({
    content:
      " My name is Yue Wushuang, 18 years old this year, just finished the test with 99.99 points.",
    name: "XiaoMing",
  }),
]);

console.log(result2);

/*
AIMessage {
  lc_serializable: true,
  lc_kwargs: {
    content: '{\n' +
      '  "name": "Yue Wushuang",\n' +
      '  "is_student": true,\n' +
      '  "is_boy": false,\n' +
      '  "courses":   {\n' +
      '    "name": "Mathematics",\n' +
      '    "score": 99.99\n' +
      '   },\n' +
      '  "age": 18\n' +
      ' }',
    additional_kwargs: { function_call: undefined }
  }
}

 */
```

## With sample messages

이 기능은 모델이 사용자가 얻고자 하는 반환 정보를 더 잘 이해하도록 도와줍니다.
여기에는 정보의 내용, 형식 및 응답 모드가 포함되지만 이에 국한되지 않습니다.

```typescript
import { ChatMinimax } from "@langchain/community/chat_models/minimax";
import { AIMessage, HumanMessage } from "@langchain/core/messages";

const model = new ChatMinimax({
  model: "abab5.5-chat",
  botSetting: [
    {
      bot_name: "MM Assistant",
      content: "MM Assistant is an AI Assistant developed by minimax.",
    },
  ],
}).withConfig({
  sampleMessages: [
    new HumanMessage({
      content: " Turn A5 into red and modify the content to minimax.",
    }),
    new AIMessage({
      content: "select A5 color red change minimax",
    }),
  ],
});

const result = await model.invoke([
  new HumanMessage({
    content:
      ' Please reply to my content according to the following requirements: According to the following interface list, give the order and parameters of calling the interface for the content I gave. You just need to give the order and parameters of calling the interface, and do not give any other output. The following is the available interface list: select: select specific table position, input parameter use letters and numbers to determine, for example "B13"; color: dye the selected table position, input parameters use the English name of the color, for example "red"; change: modify the selected table position, input parameters use strings.',
  }),
  new HumanMessage({
    content: " Process B6 to gray and modify the content to question.",
  }),
]);

console.log(result);
```

## With plugins

이 기능은 검색 엔진과 같은 도구를 호출하여 모델을 지원할 수 있는 추가 데이터를 가져오는 것을 지원합니다.

```typescript
import { ChatMinimax } from "@langchain/community/chat_models/minimax";
import { HumanMessage } from "@langchain/core/messages";

const model = new ChatMinimax({
  model: "abab5.5-chat",
  botSetting: [
    {
      bot_name: "MM Assistant",
      content: "MM Assistant is an AI Assistant developed by minimax.",
    },
  ],
}).withConfig({
  plugins: ["plugin_web_search"],
});

const result = await model.invoke([
  new HumanMessage({
    content: " What is the weather like in NewYork tomorrow?",
  }),
]);

console.log(result);

/*
AIMessage {
  lc_serializable: true,
  lc_kwargs: {
    content: 'The weather in Shanghai tomorrow is expected to be hot. Please note that this is just a forecast and the actual weather conditions may vary.',
    additional_kwargs: { function_call: undefined }
  },
  lc_namespace: [ 'langchain', 'schema' ],
  content: 'The weather in Shanghai tomorrow is expected to be hot. Please note that this is just a forecast and the actual weather conditions may vary.',
  name: undefined,
  additional_kwargs: { function_call: undefined }
}
*/
```

## Related

- Chat model [개념 가이드](/oss/javascript/langchain/models)
- Chat model [사용 방법 가이드](/oss/javascript/langchain/models)

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/chat/minimax.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
