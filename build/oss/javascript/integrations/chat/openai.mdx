---
title: ChatOpenAI
---

[OpenAI](https://en.wikipedia.org/wiki/OpenAI)는 인공지능(AI) 연구소입니다.

이 가이드는 ChatOpenAI [chat models](/oss/javascript/langchain/models) 시작하기를 도와드립니다. 모든 ChatOpenAI 기능과 구성에 대한 자세한 문서는 [API reference](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html)를 참조하세요.

## Overview

### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/openai) | Downloads | Version |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatOpenAI](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html) | [`@langchain/openai`](https://www.npmjs.com/package/@langchain/openai) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square&label=%20&) |

### Model features

특정 기능 사용 방법에 대한 가이드는 아래 표 헤더의 링크를 참조하세요.

| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ | ✅ |

## Setup

OpenAI chat model에 액세스하려면 OpenAI 계정을 생성하고 API key를 받은 다음 `@langchain/openai` integration package를 설치해야 합니다.

### Credentials

[OpenAI 웹사이트](https://platform.openai.com/)로 이동하여 OpenAI에 가입하고 API key를 생성하세요. 완료한 후 `OPENAI_API_KEY` environment variable을 설정하세요:

```bash
export OPENAI_API_KEY="your-api-key"
```

model 호출에 대한 자동 추적을 원하시면 아래 주석을 해제하여 [LangSmith](https://docs.smith.langchain.com/) API key를 설정할 수도 있습니다:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

LangChain @[`ChatOpenAI`] integration은 `@langchain/openai` package에 있습니다:

<CodeGroup>
```bash npm
npm install @langchain/openai @langchain/core
```
```bash yarn
yarn add @langchain/openai @langchain/core
```
```bash pnpm
pnpm add @langchain/openai @langchain/core
```
</CodeGroup>

## Instantiation

이제 model object를 인스턴스화하고 chat completion을 생성할 수 있습니다:

```typescript
import { ChatOpenAI } from "@langchain/openai"

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
  // other params...
})
```

## Invocation

```typescript
const aiMsg = await llm.invoke([
  {
    role: "system",
    content: "You are a helpful assistant that translates English to French. Translate the user sentence.",
  },
  {
    role: "user",
    content: "I love programming."
  },
])
aiMsg
```

```output
AIMessage {
  "id": "chatcmpl-ADItECqSPuuEuBHHPjeCkh9wIO1H5",
  "content": "J'adore la programmation.",
  "additional_kwargs": {},
  "response_metadata": {
    "tokenUsage": {
      "completionTokens": 5,
      "promptTokens": 31,
      "totalTokens": 36
    },
    "finish_reason": "stop",
    "system_fingerprint": "fp_5796ac6771"
  },
  "tool_calls": [],
  "invalid_tool_calls": [],
  "usage_metadata": {
    "input_tokens": 31,
    "output_tokens": 5,
    "total_tokens": 36
  }
}
```

```typescript
console.log(aiMsg.content)
```

```output
J'adore la programmation.
```

## Custom URLs

다음과 같이 `configuration` parameter를 전달하여 SDK가 요청을 보내는 base URL을 사용자 정의할 수 있습니다:

```typescript
import { ChatOpenAI } from "@langchain/openai";

const llmWithCustomURL = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0.9,
  configuration: {
    baseURL: "https://your_custom_url.com",
  },
});

await llmWithCustomURL.invoke("Hi there!");
```

`configuration` field는 공식 SDK에서 허용하는 다른 `ClientOptions` parameter도 허용합니다.

Azure OpenAI에서 호스팅하는 경우 [전용 페이지](/oss/javascript/integrations/chat/azure)를 참조하세요.

## Custom headers

동일한 `configuration` field에서 custom header를 지정할 수 있습니다:

```typescript
import { ChatOpenAI } from "@langchain/openai";

const llmWithCustomHeaders = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0.9,
  configuration: {
    defaultHeaders: {
      "Authorization": `Bearer SOME_CUSTOM_VALUE`,
    },
  },
});

await llmWithCustomHeaders.invoke("Hi there!");
```

## Disabling streaming usage metadata

일부 proxy 또는 third-party provider는 OpenAI와 거의 동일한 API interface를 제공하지만, streaming usage를 반환하기 위해 최근에 추가된 `stream_options` parameter를 지원하지 않습니다. 다음과 같이 streaming usage를 비활성화하여 @[`ChatOpenAI`]를 사용하여 이러한 provider에 액세스할 수 있습니다:

```typescript
import { ChatOpenAI } from "@langchain/openai";

const llmWithoutStreamUsage = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0.9,
  streamUsage: false,
  configuration: {
    baseURL: "https://proxy.com",
  },
});

await llmWithoutStreamUsage.invoke("Hi there!");
```

## Calling fine-tuned models

해당하는 `modelName` parameter를 전달하여 fine-tuned OpenAI model을 호출할 수 있습니다.

일반적으로 `ft:{OPENAI_MODEL_NAME}:{ORG_NAME}::{MODEL_ID}` 형식을 사용합니다. 예를 들어:

```typescript
import { ChatOpenAI } from "@langchain/openai";

const fineTunedLlm = new ChatOpenAI({
  temperature: 0.9,
  model: "ft:gpt-3.5-turbo-0613:{ORG_NAME}::{MODEL_ID}",
});

await fineTunedLlm.invoke("Hi there!");
```

## Generation metadata

logprobs 또는 token usage와 같은 추가 정보가 필요한 경우, message의 `response_metadata` field 내에서 `invoke` response에 직접 반환됩니다.

<Info>
    **`@langchain/core` version >=0.1.48이 필요합니다.**
</Info>

```typescript
import { ChatOpenAI } from "@langchain/openai";

// See https://cookbook.openai.com/examples/using_logprobs for details
const llmWithLogprobs = new ChatOpenAI({
  model: "gpt-4o",
  logprobs: true,
  // topLogprobs: 5,
});

const responseMessageWithLogprobs = await llmWithLogprobs.invoke("Hi there!");
console.dir(responseMessageWithLogprobs.response_metadata.logprobs, { depth: null });
```

```output
{
  content: [
    {
      token: 'Hello',
      logprob: -0.0004740447,
      bytes: [ 72, 101, 108, 108, 111 ],
      top_logprobs: []
    },
    {
      token: '!',
      logprob: -0.00004334534,
      bytes: [ 33 ],
      top_logprobs: []
    },
    {
      token: ' How',
      logprob: -0.000030113732,
      bytes: [ 32, 72, 111, 119 ],
      top_logprobs: []
    },
    {
      token: ' can',
      logprob: -0.0004797665,
      bytes: [ 32, 99, 97, 110 ],
      top_logprobs: []
    },
    {
      token: ' I',
      logprob: -7.89631e-7,
      bytes: [ 32, 73 ],
      top_logprobs: []
    },
    {
      token: ' assist',
      logprob: -0.114006,
      bytes: [
         32,  97, 115,
        115, 105, 115,
        116
      ],
      top_logprobs: []
    },
    {
      token: ' you',
      logprob: -4.3202e-7,
      bytes: [ 32, 121, 111, 117 ],
      top_logprobs: []
    },
    {
      token: ' today',
      logprob: -0.00004501419,
      bytes: [ 32, 116, 111, 100, 97, 121 ],
      top_logprobs: []
    },
    {
      token: '?',
      logprob: -0.000010206721,
      bytes: [ 63 ],
      top_logprobs: []
    }
  ],
  refusal: null
}
```

## Custom Tools

[Custom tools](https://platform.openai.com/docs/guides/function-calling#custom-tools)는 임의의 string input을 가진 tool을 지원합니다. string argument가 길거나 복잡할 것으로 예상되는 경우 특히 유용할 수 있습니다.

custom tool을 지원하는 model을 사용하는 경우 @[`ChatOpenAI`] class와 `customTool` function을 사용하여 custom tool을 만들 수 있습니다.

```typescript
import { ChatOpenAI, customTool } from "@langchain/openai";
import { createAgent, HumanMessage } from "langchain";

const codeTool = customTool(
  async () => {
    // ... Add code to execute the input
    return "Code executed successfully";
  },
  {
    name: "execute_code",
    description: "Execute a code snippet",
    format: { type: "text" },
  }
);

const model = new ChatOpenAI({ model: "gpt-5" });

const agent = createAgent({
  model,
  tools: [codeTool],
});

const result = await agent.invoke({
  messages: [new HumanMessage("Use the tool to execute the code")],
});
console.log(result);
```

<details>
<summary>Context-free grammars</summary>

OpenAI는 `lark` 또는 `regex` 형식의 custom tool input에 대한 [context-free grammar](https://platform.openai.com/docs/guides/function-calling#context-free-grammars) 사양을 지원합니다. 자세한 내용은 [OpenAI docs](https://platform.openai.com/docs/guides/function-calling#context-free-grammars)를 참조하세요. `format` parameter는 아래와 같이 `customTool`에 전달할 수 있습니다:

```typescript
import { ChatOpenAI, customTool } from "@langchain/openai";
import { createAgent, HumanMessage } from "langchain";

const MATH_GRAMMAR = `
start: expr
expr: term (SP ADD SP term)* -> add
| term
term: factor (SP MUL SP factor)* -> mul
| factor
factor: INT
SP: \" \"
ADD: \"+\"
MUL: \"*\"
%import common.INT
`;

const doMath = customTool(
  async () => {
    // ... Add code to parse and execute the input
    return "27";
  },
  {
    name: "do_math",
    description: "Evaluate a math expression",
    format: { type: "grammar", definition: MATH_GRAMMAR, syntax: "lark" },
  }
);

const model = new ChatOpenAI({ model: "gpt-5" });

const agent = createAgent({
  model,
  tools: [doMath],
});

const result = await agent.invoke({
  messages: [new HumanMessage("Use the tool to calculate 3^3")],
});
console.log(result);
```

</details>

## `strict: true`

2024년 8월 6일부터 OpenAI는 tool을 호출할 때 `strict` argument를 지원하여 model이 tool argument schema를 준수하도록 강제합니다. [자세히 보기](https://platform.openai.com/docs/guides/function-calling).

<Info>
    `@langchain/openai >= 0.2.6`이 필요합니다
</Info>

<Warning>
    `strict: true`인 경우 tool definition도 검증되며 JSON schema의 하위 집합이 허용됩니다. 중요한 점은 schema에 optional arg(기본값이 있는 arg)가 있을 수 없다는 것입니다. 지원되는 schema 유형에 대한 [전체 문서](https://platform.openai.com/docs/guides/structured-outputs/supported-schemas)를 읽어보세요.
</Warning>

다음은 tool calling 예제입니다. `.bindTools`에 추가 `strict: true` argument를 전달하면 모든 tool definition에 param이 전달됩니다:

```typescript
import { ChatOpenAI } from "@langchain/openai";
import { tool } from "@langchain/core/tools";
import * as z from "zod";

const weatherTool = tool((_) => "no-op", {
  name: "get_current_weather",
  description: "Get the current weather",
  schema: z.object({
    location: z.string(),
  }),
})

const llmWithStrictTrue = new ChatOpenAI({
  model: "gpt-4o",
}).bindTools([weatherTool], {
  strict: true,
  tool_choice: weatherTool.name,
});

// Although the question is not about the weather, it will call the tool with the correct arguments
// because we passed `tool_choice` and `strict: true`.
const strictTrueResult = await llmWithStrictTrue.invoke("What is 127862 times 12898 divided by 2?");

console.dir(strictTrueResult.tool_calls, { depth: null });
```

```output
[
  {
    name: 'get_current_weather',
    args: { location: 'current' },
    type: 'tool_call',
    id: 'call_hVFyYNRwc6CoTgr9AQFQVjm9'
  }
]
```

이 parameter를 선택한 수의 tool에만 적용하려면 OpenAI 형식의 tool schema를 직접 전달할 수도 있습니다:

```typescript
import { zodToJsonSchema } from "zod-to-json-schema";

const toolSchema = {
  type: "function",
  function: {
    name: "get_current_weather",
    description: "Get the current weather",
    strict: true,
    parameters: zodToJsonSchema(
      z.object({
        location: z.string(),
      })
    ),
  },
};

const llmWithStrictTrueTools = new ChatOpenAI({
  model: "gpt-4o",
}).bindTools([toolSchema], {
  strict: true,
});

const weatherToolResult = await llmWithStrictTrueTools.invoke([{
  role: "user",
  content: "What is the current weather in London?"
}])

weatherToolResult.tool_calls;
```

```output
[
  {
    name: 'get_current_weather',
    args: { location: 'London' },
    type: 'tool_call',
    id: 'call_EOSejtax8aYtqpchY8n8O82l'
  }
]
```

## Structured output

[`.withStructuredOutput()`](https://js.langchain.com/docs/how_to/structured_output/#the-.withstructuredoutput-method)에 `strict: true`를 전달할 수도 있습니다. 다음은 예제입니다:

```typescript
import { ChatOpenAI } from "@langchain/openai";

const traitSchema = z.object({
  traits: z.array(z.string()).describe("A list of traits contained in the input"),
});

const structuredLlm = new ChatOpenAI({
  model: "gpt-4o-mini",
}).withStructuredOutput(traitSchema, {
  name: "extract_traits",
  strict: true,
});

await structuredLlm.invoke([{
  role: "user",
  content: `I am 6'5" tall and love fruit.`
}]);
```

```output
{ traits: [ `6'5" tall`, 'love fruit' ] }
```

## Responses API

<Warning>
**호환성**

아래 내용은 `@langchain/openai>=0.4.5-rc.0`에 적용됩니다.

</Warning>

OpenAI는 [agentic](/oss/javascript/langchain/agents) 애플리케이션 구축을 지향하는 [Responses](https://platform.openai.com/docs/guides/responses-vs-chat-completions) API를 지원합니다. 여기에는 web 및 file search를 포함한 [built-in tools](https://platform.openai.com/docs/guides/tools?api-mode=responses) 모음이 포함됩니다. 또한 [conversation state](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses) 관리를 지원하여 이전 message를 명시적으로 전달하지 않고도 대화 스레드를 계속할 수 있습니다.

`ChatOpenAI`는 이러한 기능 중 하나가 사용되면 Responses API로 라우팅됩니다. `ChatOpenAI`를 인스턴스화할 때 `useResponsesApi: true`를 지정할 수도 있습니다.

### Built-in tools

@[`ChatOpenAI`]에 built-in tool을 장착하면 file이나 web의 context와 같은 외부 정보로 응답을 기반으로 합니다. model에서 생성된 [AIMessage](/oss/javascript/langchain/messages/#aimessage)에는 built-in tool 호출에 대한 정보가 포함됩니다.

#### Web search

web search를 트리거하려면 다른 tool과 마찬가지로 `{"type": "web_search_preview"}`를 model에 전달하세요.

<Tip>
**built-in tool을 invocation param으로 전달할 수도 있습니다:**

```ts
llm.invoke("...", { tools: [{ type: "web_search_preview" }] });
```

</Tip>

```typescript
import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({ model: "gpt-4o-mini" }).bindTools([
  { type: "web_search_preview" },
]);

await llm.invoke("What was a positive news story from today?");

```

response에는 응답 텍스트와 출처를 인용하는 OpenAI [annotations](https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses#output-and-citations)를 모두 포함하는 구조화된 [content blocks](/oss/javascript/langchain/messages/#message-content)가 포함됩니다. output message에는 tool 호출의 정보도 포함됩니다.

#### File search

file search를 트리거하려면 다른 tool과 마찬가지로 [file search tool](https://platform.openai.com/docs/guides/tools-file-search)을 model에 전달하세요. OpenAI 관리 vector store를 채우고 tool definition에 vector store ID를 포함해야 합니다. 자세한 내용은 [OpenAI documentation](https://platform.openai.com/docs/guides/tools-file-search)을 참조하세요.

```typescript
import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({ model: "gpt-4o-mini" }).bindTools([
  { type: "file_search", vector_store_ids: ["vs..."] },
]);

await llm.invoke("Is deep research by OpenAI?");

```

[web search](#web-search)와 마찬가지로 response에는 인용이 포함된 content block이 포함됩니다. 또한 built-in tool 호출의 정보도 포함됩니다.

#### Computer Use

ChatOpenAI는 built-in computer use tool을 위한 특수 model인 `computer-use-preview` model을 지원합니다. 활성화하려면 다른 tool을 전달하는 것처럼 [computer use tool](https://platform.openai.com/docs/guides/tools-computer-use)을 전달하세요.

현재 computer use에 대한 tool output은 `AIMessage.additional_kwargs.tool_outputs`에 있습니다. computer use tool call에 응답하려면 해당 `ToolMessage`를 생성할 때 `additional_kwargs.type: "computer_call_output"`을 설정해야 합니다.

자세한 내용은 [OpenAI documentation](https://platform.openai.com/docs/guides/tools-computer-use)을 참조하세요.

```typescript
import { AIMessage, ToolMessage } from "@langchain/core/messages";
import { ChatOpenAI } from "@langchain/openai";
import * as fs from "node:fs/promises";

const findComputerCall = (message: AIMessage) => {
  const toolOutputs = message.additional_kwargs.tool_outputs as
    | { type: "computer_call"; call_id: string; action: { type: string } }[]
    | undefined;

  return toolOutputs?.find((toolOutput) => toolOutput.type === "computer_call");
};

const llm = new ChatOpenAI({ model: "computer-use-preview" })
  .bindTools([
    {
      type: "computer-preview",
      display_width: 1024,
      display_height: 768,
      environment: "browser",
    },
  ])
  .bind({ truncation: "auto" });

let message = await llm.invoke("Check the latest OpenAI news on bing.com.");
const computerCall = findComputerCall(message);

if (computerCall) {
  // Act on a computer call action
  const screenshot = await fs.readFile("./screenshot.png", {
    encoding: "base64",
  });

  message = await llm.invoke(
    [
      new ToolMessage({
        additional_kwargs: { type: "computer_call_output" },
        tool_call_id: computerCall.call_id,
        content: [
          {
            type: "computer_screenshot",
            image_url: `data:image/png;base64,${screenshot}`,
          },
        ],
      }),
    ],
    { previous_response_id: message.response_metadata["id"] }
  );
}

```

#### Code interpreter

ChatOpenAI를 사용하면 built-in [code interpreter tool](https://platform.openai.com/docs/guides/tools-code-interpreter)을 사용하여 샌드박스 코드 생성 및 실행을 지원할 수 있습니다.

```typescript
import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "o4-mini",
  useResponsesApi: true,
});

const llmWithTools = llm.bindToools([
  {
    type: "code_interpreter",
    // Creates a new container
    container: { type: "auto" }
  },
]);

const response = await llmWithTools.invoke(
  "Write and run code to answer the question: what is 3^3?"
);
```

위 명령은 새 [container](https://platform.openai.com/docs/guides/tools-code-interpreter#containers)를 생성합니다. 기존 container ID를 지정하여 호출 간에 container를 재사용할 수 있습니다.

```typescript
const tool_outputs: Record<string, any>[] = response.additional_kwargs.tool_outputs
const container_id = tool_outputs[0].container_id

const llmWithTools = llm.bindTools([
  {
    type: "code_interpreter",
    // Re-uses container from the last call
    container: container_id,
  },
]);
```

#### Remote MCP

ChatOpenAI는 MCP server에 대한 model 생성 호출이 OpenAI server에서 발생하도록 하는 built-in [remote MCP tool](https://platform.openai.com/docs/guides/tools-remote-mcp)을 지원합니다.

```typescript
import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
    model: "o4-mini",
    useResponsesApi: true,
});

const llmWithMcp = llm.bindTools([
    {
        type: "mcp",
        server_label: "deepwiki",
        server_url: "https://mcp.deepwiki.com/mcp",
        require_approval: "never"
    }
]);

const response = await llmWithMcp.invoke(
    "What transport protocols does the 2025-03-26 version of the MCP spec (modelcontextprotocol/modelcontextprotocol) support?"
);
```

<Note>
**MCP Approvals**

지시를 받으면 OpenAI는 remote MCP server를 호출하기 전에 승인을 요청합니다.

위 명령에서는 model에 승인을 요구하지 않도록 지시했습니다. 항상 승인을 요청하거나 특정 tool에 대해 항상 승인을 요청하도록 구성할 수도 있습니다:

```typescript
...
const llmWithMcp = llm.bindTools([
  {
    type: "mcp",
    server_label: "deepwiki",
    server_url: "https://mcp.deepwiki.com/mcp",
    require_approval: {
      always: {
        tool_names: ["read_wiki_structure"],
      },
    },
  },
]);
const response = await llmWithMcp.invoke(
    "What transport protocols does the 2025-03-26 version of the MCP spec (modelcontextprotocol/modelcontextprotocol) support?"
);
```

이 구성을 사용하면 response에 `mcp_approval_request`로 입력된 tool output이 포함될 수 있습니다. approval request에 대한 승인을 제출하려면 후속 message의 content block으로 구조화할 수 있습니다:

```typescript
const approvals = [];
if (Array.isArray(response.additional_kwargs.tool_outputs)) {
  for (const content of response.additional_kwargs.tool_outputs) {
    if (content.type === "mcp_approval_request") {
      approvals.push({
        type: "mcp_approval_response",
        approval_request_id: content.id,
        approve: true,
      });
    }
  }
}

const nextResponse = await model.invoke(
  [
    response,
    new HumanMessage({ content: approvals }),
  ],
);
```

</Note>

#### Image Generation

ChatOpenAI를 사용하면 built-in [image generation tool](https://platform.openai.com/docs/guides/tools-image-generation)을 사용하여 responses API를 통한 다중 턴 대화의 일부로 이미지를 생성할 수 있습니다.

```typescript
import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4.1",
  useResponsesApi: true,
});

const llmWithImageGeneration = llm.bindTools([
  {
    type: "image_generation",
    quality: "low",
  }
]);

const response = await llmWithImageGeneration.invoke(
  "Draw a random short word in green font."
)
```

### Reasoning models


<Warning>
**호환성**: 아래 내용은 `@langchain/openai>=0.4.0`에 적용됩니다.
</Warning>


`o1`과 같은 reasoning model을 사용할 때 `withStructuredOutput`의 기본 method는 OpenAI의 built-in structured output method입니다(`withStructuredOutput`에 option으로 `method: "jsonSchema"`를 전달하는 것과 동일). JSON schema는 대부분 다른 model과 동일하게 작동하지만 한 가지 중요한 주의사항이 있습니다: schema를 정의할 때 `z.optional()`은 존중되지 않으며 대신 `z.nullable()`을 사용해야 합니다.

다음은 예제입니다:

```typescript
import * as z from "zod";
import { ChatOpenAI } from "@langchain/openai";

// Will not work
const reasoningModelSchemaOptional = z.object({
  color: z.optional(z.string()).describe("A color mentioned in the input"),
});

const reasoningModelOptionalSchema = new ChatOpenAI({
  model: "o1",
}).withStructuredOutput(reasoningModelSchemaOptional, {
  name: "extract_color",
});

await reasoningModelOptionalSchema.invoke([{
  role: "user",
  content: `I am 6'5" tall and love fruit.`
}]);
```

```output
{ color: 'No color mentioned' }
```

다음은 `z.nullable()`을 사용한 예제입니다:

```typescript
import * as z from "zod";
import { ChatOpenAI } from "@langchain/openai";

// Will not work
const reasoningModelSchemaNullable = z.object({
  color: z.nullable(z.string()).describe("A color mentioned in the input"),
});

const reasoningModelNullableSchema = new ChatOpenAI({
  model: "o1",
}).withStructuredOutput(reasoningModelSchemaNullable, {
  name: "extract_color",
});

await reasoningModelNullableSchema.invoke([{
  role: "user",
  content: `I am 6'5" tall and love fruit.`
}]);
```

```output
{ color: null }
```

## Prompt caching

최신 OpenAI model은 비용을 절감하기 위해 긴 context가 필요한 use-case를 위해 입력이 특정 크기(작성 시점에 1024 token) 이상인 경우 [prompt의 일부를 자동으로 캐시](https://openai.com/index/api-prompt-caching/)합니다.

**참고:** 주어진 query에 대해 캐시된 token 수는 아직 `AIMessage.usage_metadata`에서 표준화되지 않았으며 대신 `AIMessage.response_metadata` field에 포함되어 있습니다.

다음은 예제입니다

```typescript
// @lc-docs-hide-cell

const CACHED_TEXT = `## Components

LangChain provides standard, extendable interfaces and external integrations for various components useful for building with LLMs.
Some components LangChain implements, some components we rely on third-party integrations for, and others are a mix.

### Chat models

<span data-heading-keywords="chat model,chat models"></span>

Language models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain text).
These are generally newer models (older models are generally \`LLMs\`, see below).
Chat models support the assignment of distinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system messages.

Although the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string as input.
This gives them the same interface as LLMs (and simpler to use).
When a string is passed in as input, it will be converted to a \`HumanMessage\` under the hood before being passed to the underlying model.

LangChain does not host any Chat Models, rather we rely on third party integrations.

We have some standardized parameters when constructing ChatModels:

- \`model\`: the name of the model

Chat Models also accept other parameters that are specific to that integration.

<Warning>
**Some chat models have been fine-tuned for **tool calling** and provide a dedicated API for it.**

Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling.
Please see the [tool calling section](/oss/javascript/langchain/tools) for more information.
</Warning>

For specifics on how to use chat models, see the [relevant how-to guides here](/oss/javascript/langchain/models).

#### Multimodality

Some chat models are multimodal, accepting images, audio and even video as inputs.
These are still less common, meaning model providers haven't standardized on the "best" way to define the API.
Multimodal outputs are even less common. As such, we've kept our multimodal abstractions fairly light weight
and plan to further solidify the multimodal APIs and interaction patterns as the field matures.

In LangChain, most chat models that support multimodal inputs also accept those values in OpenAI's content blocks format.
So far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the native, model-specific representations.

For specifics on how to use multimodal models, see the [relevant how-to guides here](/oss/javascript/how-to/#multimodal).

### LLMs

<span data-heading-keywords="llm,llms"></span>

<Warning>
**Pure text-in/text-out LLMs tend to be older or lower-level. Many popular models are best used as [chat completion models](/oss/javascript/langchain/models),**

even for non-chat use cases.

You are probably looking for [the section above instead](/oss/javascript/langchain/models).
</Warning>

Language models that takes a string as input and returns a string.
These are traditionally older models (newer models generally are [Chat Models](/oss/javascript/langchain/models), see above).

Although the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as input.
This gives them the same interface as [Chat Models](/oss/javascript/langchain/models).
When messages are passed in as input, they will be formatted into a string under the hood before being passed to the underlying model.

LangChain does not host any LLMs, rather we rely on third party integrations.

For specifics on how to use LLMs, see the [relevant how-to guides here](/oss/javascript/langchain/models).

### Message types

Some language models take an array of messages as input and return a message.
There are a few different types of messages.
All messages have a \`role\`, \`content\`, and \`response_metadata\` property.

The \`role\` describes WHO is saying the message.
LangChain has different message classes for different roles.

The \`content\` property describes the content of the message.
This can be a few different things:

- A string (most models deal this type of content)
- A List of objects (this is used for multi-modal input, where the object contains information about that input type and that input location)

#### HumanMessage

This represents a message from the user.

#### AIMessage

This represents a message from the model. In addition to the \`content\` property, these messages also have:

**\`response_metadata\`**

The \`response_metadata\` property contains additional metadata about the response. The data here is often specific to each model provider.
This is where information like log-probs and token usage may be stored.

**\`tool_calls\`**

These represent a decision from an language model to call a tool. They are included as part of an \`AIMessage\` output.
They can be accessed from there with the \`.tool_calls\` property.

This property returns a list of \`ToolCall\`s. A \`ToolCall\` is an object with the following arguments:

- \`name\`: The name of the tool that should be called.
- \`args\`: The arguments to that tool.
- \`id\`: The id of that tool call.

#### SystemMessage

This represents a system message, which tells the model how to behave. Not every model provider supports this.

#### ToolMessage

This represents the result of a tool call. In addition to \`role\` and \`content\`, this message has:

- a \`tool_call_id\` field which conveys the id of the call to the tool that was called to produce this result.
- an \`artifact\` field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.

#### (Legacy) FunctionMessage

This is a legacy message type, corresponding to OpenAI's legacy function-calling API. \`ToolMessage\` should be used instead to correspond to the updated tool-calling API.

This represents the result of a function call. In addition to \`role\` and \`content\`, this message has a \`name\` parameter which conveys the name of the function that was called to produce this result.

### Prompt templates

<span data-heading-keywords="prompt,prompttemplate,chatprompttemplate"></span>

Prompt templates help to translate user input and parameters into instructions for a language model.
This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.

Prompt Templates take as input an object, where each key represents a variable in the prompt template to fill in.

Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or an array of messages.
The reason this PromptValue exists is to make it easy to switch between strings and messages.

There are a few different types of prompt templates:

#### String PromptTemplates

These prompt templates are used to format a single string, and generally are used for simpler inputs.
For example, a common way to construct and use a PromptTemplate is as follows:

\`\`\`typescript
import { PromptTemplate } from "@langchain/core/prompts";

const promptTemplate = PromptTemplate.fromTemplate(
  "Tell me a joke about {topic}"
);

await promptTemplate.invoke({ topic: "cats" });
\`\`\`

#### ChatPromptTemplates

These prompt templates are used to format an array of messages. These "templates" consist of an array of templates themselves.
For example, a common way to construct and use a ChatPromptTemplate is as follows:

\`\`\`typescript
import { ChatPromptTemplate } from "@langchain/core/prompts";

const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant"],
  ["user", "Tell me a joke about {topic}"],
]);

await promptTemplate.invoke({ topic: "cats" });
\`\`\`

In the above example, this ChatPromptTemplate will construct two messages when called.
The first is a system message, that has no variables to format.
The second is a HumanMessage, and will be formatted by the \`topic\` variable the user passes in.

#### MessagesPlaceholder

<span data-heading-keywords="messagesplaceholder"></span>

This prompt template is responsible for adding an array of messages in a particular place.
In the above ChatPromptTemplate, we saw how we could format two messages, each one a string.
But what if we wanted the user to pass in an array of messages that we would slot into a particular spot?
This is how you use MessagesPlaceholder.

\`\`\`typescript
import {
  ChatPromptTemplate,
  MessagesPlaceholder,
} from "@langchain/core/prompts";
import { HumanMessage } from "@langchain/core/messages";

const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant"],
  new MessagesPlaceholder("msgs"),
]);

promptTemplate.invoke({ msgs: [new HumanMessage({ content: "hi!" })] });
\`\`\`

This will produce an array of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.
If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).
This is useful for letting an array of messages be slotted into a particular spot.

An alternative way to accomplish the same thing without using the \`MessagesPlaceholder\` class explicitly is:

\`\`\`typescript
const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant"],
  ["placeholder", "{msgs}"], // <-- This is the changed part
]);
\`\`\`

For specifics on how to use prompt templates, see the [relevant how-to guides here](/oss/javascript/how-to/#prompt-templates).

### Example Selectors

One common prompting technique for achieving better performance is to include examples as part of the prompt.
This gives the language model concrete examples of how it should behave.
Sometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.
Example Selectors are classes responsible for selecting and then formatting examples into prompts.

For specifics on how to use example selectors, see the [relevant how-to guides here](/oss/javascript/how-to/#example-selectors).

### Output parsers

<span data-heading-keywords="output parser"></span>

<Note>
**The information here refers to parsers that take a text output from a model try to parse it into a more structured representation.**

More and more models are supporting function (or tool) calling, which handles this automatically.
It is recommended to use function/tool calling rather than output parsing.
See documentation for that [here](/oss/javascript/langchain/tools).

</Note>

Responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.
Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.

There are two main methods an output parser must implement:

- "Get format instructions": A method which returns a string containing instructions for how the output of a language model should be formatted.
- "Parse": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.

And then one optional one:

- "Parse with prompt": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.

Output parsers accept a string or \`BaseMessage\` as input and can return an arbitrary type.

LangChain has many different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:

**Name**: The name of the output parser

**Supports Streaming**: Whether the output parser supports streaming.

**Input Type**: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific arguments.

**Output Type**: The output type of the object returned by the parser.

**Description**: Our commentary on this output parser and when to use it.

The current date is ${new Date().toISOString()}`;

// Noop statement to hide output
void 0;
```

```typescript
import { ChatOpenAI } from "@langchain/openai";

const modelWithCaching = new ChatOpenAI({
  model: "gpt-4o-mini-2024-07-18",
});

// CACHED_TEXT is some string longer than 1024 tokens
const LONG_TEXT = `You are a pirate. Always respond in pirate dialect.

Use the following as context when answering questions:

${CACHED_TEXT}`;

const longMessages = [
  {
    role: "system",
    content: LONG_TEXT,
  },
  {
    role: "user",
    content: "What types of messages are supported in LangChain?",
  },
];

const originalRes = await modelWithCaching.invoke(longMessages);

console.log("USAGE:", originalRes.response_metadata.usage);
```

```output
USAGE: {
  prompt_tokens: 2624,
  completion_tokens: 263,
  total_tokens: 2887,
  prompt_tokens_details: { cached_tokens: 0 },
  completion_tokens_details: { reasoning_tokens: 0 }
}
```

```typescript
const resWitCaching = await modelWithCaching.invoke(longMessages);

console.log("USAGE:", resWitCaching.response_metadata.usage);
```

```output
USAGE: {
  prompt_tokens: 2624,
  completion_tokens: 272,
  total_tokens: 2896,
  prompt_tokens_details: { cached_tokens: 2432 },
  completion_tokens_details: { reasoning_tokens: 0 }
}
```

## Predicted output

일부 OpenAI model(`gpt-4o` 및 `gpt-4o-mini` 시리즈 등)은 [Predicted Outputs](https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs)를 지원하여 대기 시간을 줄이기 위해 LLM의 예상 출력 중 알려진 부분을 미리 전달할 수 있습니다. 이는 model 출력의 작은 부분만 변경되는 텍스트 또는 코드 편집과 같은 경우에 유용합니다.

다음은 예제입니다:

```typescript
import { ChatOpenAI } from "@langchain/openai";

const modelWithPredictions = new ChatOpenAI({
  model: "gpt-4o-mini",
});

const codeSample = `
/// <summary>
/// Represents a user with a first name, last name, and username.
/// </summary>
public class User
{
/// <summary>
/// Gets or sets the user's first name.
/// </summary>
public string FirstName { get; set; }

/// <summary>
/// Gets or sets the user's last name.
/// </summary>
public string LastName { get; set; }

/// <summary>
/// Gets or sets the user's username.
/// </summary>
public string Username { get; set; }
}
`;

// Can also be attached ahead of time
// using `model.bind({ prediction: {...} })`;
await modelWithPredictions.invoke(
  [
    {
      role: "user",
      content:
        "Replace the Username property with an Email property. Respond only with code, and with no markdown formatting.",
    },
    {
      role: "user",
      content: codeSample,
    },
  ],
  {
    prediction: {
      type: "content",
      content: codeSample,
    },
  }
);
```

```output
AIMessage {
  "id": "chatcmpl-AQLyQKnazr7lEV7ejLTo1UqhzHDBl",
  "content": "/// <summary>\n/// Represents a user with a first name, last name, and email.\n/// </summary>\npublic class User\n{\n/// <summary>\n/// Gets or sets the user's first name.\n/// </summary>\npublic string FirstName { get; set; }\n\n/// <summary>\n/// Gets or sets the user's last name.\n/// </summary>\npublic string LastName { get; set; }\n\n/// <summary>\n/// Gets or sets the user's email.\n/// </summary>\npublic string Email { get; set; }\n}",
  "additional_kwargs": {},
  "response_metadata": {
    "tokenUsage": {
      "promptTokens": 148,
      "completionTokens": 217,
      "totalTokens": 365
    },
    "finish_reason": "stop",
    "usage": {
      "prompt_tokens": 148,
      "completion_tokens": 217,
      "total_tokens": 365,
      "prompt_tokens_details": {
        "cached_tokens": 0
      },
      "completion_tokens_details": {
        "reasoning_tokens": 0,
        "accepted_prediction_tokens": 36,
        "rejected_prediction_tokens": 116
      }
    },
    "system_fingerprint": "fp_0ba0d124f1"
  },
  "tool_calls": [],
  "invalid_tool_calls": [],
  "usage_metadata": {
    "output_tokens": 217,
    "input_tokens": 148,
    "total_tokens": 365,
    "input_token_details": {
      "cache_read": 0
    },
    "output_token_details": {
      "reasoning": 0
    }
  }
}
```

현재 prediction은 추가 token으로 청구되며 이 감소된 대기 시간과 교환하여 사용량과 비용이 증가합니다.

## Audio output

일부 OpenAI model(`gpt-4o-audio-preview` 등)은 audio output 생성을 지원합니다. 다음 예제는 해당 기능을 사용하는 방법을 보여줍니다:

```typescript
import { ChatOpenAI } from "@langchain/openai";

const modelWithAudioOutput = new ChatOpenAI({
  model: "gpt-4o-audio-preview",
  // You may also pass these fields to `.bind` as a call argument.
  modalities: ["text", "audio"], // Specifies that the model should output audio.
  audio: {
    voice: "alloy",
    format: "wav",
  },
});

const audioOutputResult = await modelWithAudioOutput.invoke("Tell me a joke about cats.");
const castAudioContent = audioOutputResult.additional_kwargs.audio as Record<string, any>;

console.log({
  ...castAudioContent,
  data: castAudioContent.data.slice(0, 100) // Sliced for brevity
})
```

```output
{
  id: 'audio_67129e9466f48190be70372922464162',
  data: 'UklGRgZ4BABXQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGHA',
  expires_at: 1729277092,
  transcript: "Why did the cat sit on the computer's keyboard? Because it wanted to keep an eye on the mouse!"
}
```

audio data가 `data` field 내에 반환되는 것을 볼 수 있습니다. `expires_at` date field도 제공됩니다. 이 field는 다중 턴 대화에서 사용하기 위해 server에서 audio response에 더 이상 액세스할 수 없는 날짜를 나타냅니다.

### Streaming Audio Output

OpenAI는 streaming audio output도 지원합니다. 다음은 예제입니다:

```typescript
import { AIMessageChunk } from "@langchain/core/messages";
import { concat } from "@langchain/core/utils/stream"
import { ChatOpenAI } from "@langchain/openai";

const modelWithStreamingAudioOutput = new ChatOpenAI({
  model: "gpt-4o-audio-preview",
  modalities: ["text", "audio"],
  audio: {
    voice: "alloy",
    format: "pcm16", // Format must be `pcm16` for streaming
  },
});

const audioOutputStream = await modelWithStreamingAudioOutput.stream("Tell me a joke about cats.");
let finalAudioOutputMsg: AIMessageChunk | undefined;
for await (const chunk of audioOutputStream) {
  finalAudioOutputMsg = finalAudioOutputMsg ? concat(finalAudioOutputMsg, chunk) : chunk;
}
const castStreamedAudioContent = finalAudioOutputMsg?.additional_kwargs.audio as Record<string, any>;

console.log({
  ...castStreamedAudioContent,
  data: castStreamedAudioContent.data.slice(0, 100) // Sliced for brevity
})
```

```output
{
  id: 'audio_67129e976ce081908103ba4947399a3eaudio_67129e976ce081908103ba4947399a3e',
  transcript: 'Why was the cat sitting on the computer? Because it wanted to keep an eye on the mouse!',
  index: 0,
  data: 'CgAGAAIADAAAAA0AAwAJAAcACQAJAAQABQABAAgABQAPAAAACAADAAUAAwD8/wUA+f8MAPv/CAD7/wUA///8/wUA/f8DAPj/AgD6',
  expires_at: 1729277096
}
```

### Audio input

이러한 model은 audio를 input으로 전달하는 것도 지원합니다. 이를 위해 아래와 같이 `input_audio` field를 지정해야 합니다:

```typescript
import { HumanMessage } from "@langchain/core/messages";

const userInput = new HumanMessage({
  content: [{
    type: "input_audio",
    input_audio: {
      data: castAudioContent.data, // Re-use the base64 data from the first example
      format: "wav",
    },
  }]
})

// Re-use the same model instance
const userInputAudioRes = await modelWithAudioOutput.invoke([userInput]);

console.log((userInputAudioRes.additional_kwargs.audio as Record<string, any>).transcript);
```

```output
That's a great joke! It's always fun to imagine why cats do the funny things they do. Keeping an eye on the "mouse" is a creatively punny way to describe it!
```

## API reference

모든 ChatOpenAI 기능과 구성에 대한 자세한 문서는 [API reference](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html)를 참조하세요.

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/chat/openai.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
