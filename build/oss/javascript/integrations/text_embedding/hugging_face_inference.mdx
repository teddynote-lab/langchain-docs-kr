---
title: HuggingFace Inference
---

이 Embeddings integration은 HuggingFace Inference API를 사용하여 주어진 텍스트에 대한 embedding을 생성하며, 기본적으로 `BAAI/bge-base-en-v1.5` model을 사용합니다. 다른 model을 사용하려면 constructor에 다른 model 이름을 전달할 수 있습니다.

## Setup

먼저 [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) package와 필요한 peer dependency를 설치해야 합니다:

<Tip>
LangChain package 설치에 대한 일반적인 지침은 [이 섹션](/oss/javascript/langchain/install)을 참조하세요.
</Tip>

```bash npm
npm install @langchain/community @langchain/core @huggingface/inference@4
```
## Usage

```typescript
import { HuggingFaceInferenceEmbeddings } from "@langchain/community/embeddings/hf";

const embeddings = new HuggingFaceInferenceEmbeddings({
  apiKey: "YOUR-API-KEY", // Defaults to process.env.HUGGINGFACEHUB_API_KEY
  model: "MODEL-NAME", // Defaults to `BAAI/bge-base-en-v1.5` if not provided
  provider: "MODEL-PROVIDER", // Falls back to auto selection mechanism within Hugging Face's inference API if not provided
});
```

> **Note:**
> `model`을 제공하지 않으면 경고가 기록되고 기본 model인 `BAAI/bge-base-en-v1.5`가 사용됩니다.
> `provider`를 제공하지 않으면 Hugging Face는 기본적으로 `auto` 선택을 사용하며, 이는 https://hf.co/settings/inference-providers의 설정에 따라 model에 사용 가능한 첫 번째 provider를 선택합니다.

> **Hint:**
> `hf-inference`는 Hugging Face에서 직접 호스팅하는 model의 provider 이름입니다.

## Related

- Embedding model [개념 가이드](/oss/javascript/integrations/text_embedding)
- Embedding model [how-to 가이드](/oss/javascript/integrations/text_embedding)

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/text_embedding/hugging_face_inference.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
