---
title: Upstash Ratelimit Callback
---

이 가이드에서는 `UpstashRatelimitHandler`를 사용하여 요청 수 또는 토큰 수를 기반으로 rate limiting을 추가하는 방법을 다룹니다. 이 handler는 [Upstash의 ratelimit 라이브러리](https://github.com/upstash/ratelimit-js/)를 사용하며, 이는 [Upstash Redis](https://upstash.com/docs/redis/overall/getstarted)를 활용합니다.

Upstash Ratelimit은 `limit` 메서드가 호출될 때마다 Upstash Redis에 HTTP 요청을 보내는 방식으로 작동합니다. 사용자의 남은 토큰/요청이 확인되고 업데이트됩니다. 남은 토큰을 기반으로 LLM 호출이나 vector store 쿼리와 같은 비용이 많이 드는 작업의 실행을 중단할 수 있습니다:

```tsx
const response = await ratelimit.limit();
if (response.success) {
  execute_costly_operation();
}
```

`UpstashRatelimitHandler`를 사용하면 몇 분 안에 이 ratelimit 로직을 chain에 통합할 수 있습니다.

## Setup

먼저 [Upstash Console](https://console.upstash.com/login)로 이동하여 redis 데이터베이스를 생성해야 합니다([문서 참조](https://upstash.com/docs/redis/overall/getstarted)). 데이터베이스를 생성한 후 환경 변수를 설정해야 합니다:

```
UPSTASH_REDIS_REST_URL="****"
UPSTASH_REDIS_REST_TOKEN="****"
```

다음으로 Upstash Ratelimit과 `@langchain/community`를 설치해야 합니다:

<Tip>
LangChain 패키지 설치에 대한 일반적인 지침은 [이 섹션](/oss/javascript/langchain/install)을 참조하세요.
</Tip>

```bash npm
npm install @upstash/ratelimit @langchain/community @langchain/core
```
이제 chain에 rate limiting을 추가할 준비가 되었습니다!

## 요청당 Ratelimiting

사용자가 분당 10번 chain을 호출할 수 있도록 허용하고 싶다고 가정해 봅시다. 이를 달성하는 것은 다음과 같이 간단합니다:

```tsx
const UPSTASH_REDIS_REST_URL = "****";
const UPSTASH_REDIS_REST_TOKEN = "****";

import {
  UpstashRatelimitHandler,
  UpstashRatelimitError,
} from "@langchain/community/callbacks/handlers/upstash_ratelimit";
import { RunnableLambda } from "@langchain/core/runnables";
import { Ratelimit } from "@upstash/ratelimit";
import { Redis } from "@upstash/redis";

// create ratelimit
const ratelimit = new Ratelimit({
  redis: new Redis({
    url: UPSTASH_REDIS_REST_URL,
    token: UPSTASH_REDIS_REST_TOKEN,
  }),
  // 10 requests per window, where window size is 60 seconds:
  limiter: Ratelimit.fixedWindow(10, "60 s"),
});

// create handler
const user_id = "user_id"; // should be a method which gets the user id
const handler = new UpstashRatelimitHandler(user_id, {
  requestRatelimit: ratelimit,
});

// create mock chain
const chain = new RunnableLambda({ func: (str: string): string => str });

try {
  const response = await chain.invoke("hello world", {
    callbacks: [handler],
  });
  console.log(response);
} catch (err) {
  if (err instanceof UpstashRatelimitError) {
    console.log("Handling ratelimit.");
  }
}
```
chain을 정의할 때 handler를 전달하는 대신 `invoke` 메서드에 handler를 전달한다는 점에 유의하세요.

`FixedWindow` 이외의 rate limiting 알고리즘에 대해서는 [upstash-ratelimit 문서](https://upstash.com/docs/oss/sdks/ts/ratelimit/algorithms)를 참조하세요.

파이프라인의 단계를 실행하기 전에 ratelimit은 사용자가 요청 제한을 초과했는지 확인합니다. 초과한 경우 `UpstashRatelimitError`가 발생합니다.

## 토큰당 Ratelimiting

또 다른 옵션은 다음을 기반으로 chain 호출을 rate limit하는 것입니다:

1. prompt의 토큰 수
2. prompt와 LLM completion의 토큰 수

이는 chain에 LLM이 있는 경우에만 작동합니다. 또 다른 요구 사항은 사용 중인 LLM이 `LLMOutput`에 토큰 사용량을 반환해야 한다는 것입니다. 반환되는 토큰 사용량 dictionary의 형식은 LLM에 따라 다릅니다. LLM에 따라 handler를 구성하는 방법에 대해 알아보려면 아래 Configuration 섹션의 끝부분을 참조하세요.

### 작동 방식

handler는 LLM을 호출하기 전에 남은 토큰을 가져옵니다. 남은 토큰이 0보다 크면 LLM이 호출됩니다. 그렇지 않으면 `UpstashRatelimitError`가 발생합니다.

LLM이 호출된 후 토큰 사용량 정보는 사용자의 남은 토큰에서 차감됩니다. chain의 이 단계에서는 오류가 발생하지 않습니다.

### Configuration

첫 번째 구성의 경우 다음과 같이 handler를 초기화하면 됩니다:

```tsx
const user_id = "user_id"; // should be a method which gets the user id
const handler = new UpstashRatelimitHandler(user_id, {
  requestRatelimit: ratelimit,
});
```
두 번째 구성의 경우 handler를 초기화하는 방법은 다음과 같습니다:

```tsx
const user_id = "user_id"; // should be a method which gets the user id
const handler = new UpstashRatelimitHandler(user_id, {
  tokenRatelimit: ratelimit,
});
```
`request_ratelimit`과 `token_ratelimit` 파라미터를 모두 전달하여 요청과 토큰을 동시에 기반으로 ratelimiting을 적용할 수도 있습니다.

토큰 사용량이 올바르게 작동하려면 LangChain.js의 LLM 단계가 다음 형식으로 토큰 사용량 필드를 반환해야 합니다:

```json
{
  "tokenUsage": {
    "totalTokens": 123,
    "promptTokens": 456,
    "otherFields: "..."
  },
  "otherFields: "..."
}
```
그러나 LangChain.js의 모든 LLM이 이 형식을 준수하는 것은 아닙니다. LLM이 다른 키로 동일한 값을 반환하는 경우 `llmOutputTokenUsageField`, `llmOutputTotalTokenField` 및 `llmOutputPromptTokenField` 파라미터를 handler에 전달하여 사용할 수 있습니다:

```tsx
const handler = new UpstashRatelimitHandler(
  user_id,
  {
    requestRatelimit: ratelimit
    llmOutputTokenUsageField: "usage",
    llmOutputTotalTokenField: "total",
    llmOutputPromptTokenField: "prompt"
  }
)
```
다음은 LLM을 활용하는 chain의 예시입니다:

```tsx
const UPSTASH_REDIS_REST_URL = "****";
const UPSTASH_REDIS_REST_TOKEN = "****";
const OPENAI_API_KEY = "****";

import {
  UpstashRatelimitHandler,
  UpstashRatelimitError,
} from "@langchain/community/callbacks/handlers/upstash_ratelimit";
import { RunnableLambda, RunnableSequence } from "@langchain/core/runnables";
import { OpenAI } from "@langchain/openai";
import { Ratelimit } from "@upstash/ratelimit";
import { Redis } from "@upstash/redis";

// create ratelimit
const ratelimit = new Ratelimit({
  redis: new Redis({
    url: UPSTASH_REDIS_REST_URL,
    token: UPSTASH_REDIS_REST_TOKEN,
  }),
  // 500 tokens per window, where window size is 60 seconds:
  limiter: Ratelimit.fixedWindow(500, "60 s"),
});

// create handler
const user_id = "user_id"; // should be a method which gets the user id
const handler = new UpstashRatelimitHandler(user_id, {
  tokenRatelimit: ratelimit,
});

// create mock chain
const asStr = new RunnableLambda({ func: (str: string): string => str });
const model = new OpenAI({
  apiKey: OPENAI_API_KEY,
});
const chain = RunnableSequence.from([asStr, model]);

// invoke chain with handler:
try {
  const response = await chain.invoke("hello world", {
    callbacks: [handler],
  });
  console.log(response);
} catch (err) {
  if (err instanceof UpstashRatelimitError) {
    console.log("Handling ratelimit.");
  }
}
```

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/callbacks/upstash_ratelimit_callback.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
