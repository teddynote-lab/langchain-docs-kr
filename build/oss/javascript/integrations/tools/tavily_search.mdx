---
title: Tavily Search
---

[Tavily](https://tavily.com/)는 AI 에이전트(LLM)를 위해 특별히 구축된 검색 엔진으로, 실시간으로 정확하고 사실적인 결과를 빠르게 제공합니다. Tavily는 두 가지 주요 엔드포인트를 제공하며, 그 중 하나인 Search는 LLM과 RAG에 맞춤화된 검색 결과를 제공합니다.

이 가이드는 Tavily [tool](/oss/javascript/integrations/tools/) 시작하기에 대한 간단한 개요를 제공합니다. Tavily tool의 전체 분석은 [API reference](https://v03.api.js.langchain.com/modules/_langchain_tavily.html)에서 더 자세한 문서를 확인할 수 있습니다.

## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/tools/tavily_search/) | Version |
| :--- | :--- | :---: | :---: |
| [TavilySearch](https://api.js.langchain.com/classes/_langchain_tavily.TavilySearch.html) | [`@langchain/tavily`](https://www.npmjs.com/package/@langchain/tavily) | ✅ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/tavily?style=flat-square&label=%20&) |

## Setup

이 integration은 `@langchain/tavily` 패키지에 포함되어 있으며, 아래와 같이 설치할 수 있습니다:

<CodeGroup>
```bash npm
npm install @langchain/tavily @langchain/core
```
```bash yarn
yarn add @langchain/tavily @langchain/core
```
```bash pnpm
pnpm add @langchain/tavily @langchain/core
```
</CodeGroup>

### Credentials

[여기](https://app.tavily.com)에서 API key를 설정하고 `TAVILY_API_KEY`라는 이름의 환경 변수로 설정하세요.

```typescript
process.env.TAVILY_API_KEY = "YOUR_API_KEY"
```

최고 수준의 관찰성을 위해 [LangSmith](https://smith.langchain.com/)를 설정하는 것도 도움이 됩니다(필수는 아님):

```typescript
process.env.LANGSMITH_TRACING="true"
process.env.LANGSMITH_API_KEY="your-api-key"
```

## Instantiation

다음과 같이 `TavilySearch` tool의 인스턴스를 import하고 인스턴스화할 수 있습니다:

```typescript
import { TavilySearch } from "@langchain/tavily";

const tool = new TavilySearch({
  maxResults: 5,
  topic: "general",
  // includeAnswer: false,
  // includeRawContent: false,
  // includeImages: false,
  // includeImageDescriptions: false,
  // searchDepth: "basic",
  // timeRange: "day",
  // includeDomains: [],
  // excludeDomains: [],
});
```

## Invocation

### [Invoke directly with args](/oss/javascript/langchain/tools)

Tavily search tool은 호출 시 다음 인자를 받습니다:

* `query` (필수): 자연어 검색 쿼리

* 다음 인자들도 호출 시 설정할 수 있습니다: `includeImages`, `searchDepth`, `timeRange`, `includeDomains`, `excludeDomains`, `includeImages`.

* 신뢰성과 성능상의 이유로, 응답 크기에 영향을 주는 특정 매개변수는 호출 중에 수정할 수 없습니다: `includeAnswer`와 `includeRawContent`. 이러한 제한은 예상치 못한 context window 문제를 방지하고 일관된 결과를 보장합니다.

```typescript
await tool.invoke({
  query: "what is the current weather in SF?"
});
```

### [Invoke with ToolCall](/oss/javascript/langchain/tools)

모델이 생성한 `ToolCall`로 tool을 호출할 수도 있으며, 이 경우 @[`ToolMessage`]가 반환됩니다:

```typescript
// This is usually generated by a model, but we'll create a tool call directly for demo purposes.
const modelGeneratedToolCall = {
  args: {
    query: "what is the current weather in SF?"
  },
  id: "1",
  name: tool.name,
  type: "tool_call",
}

await tool.invoke(modelGeneratedToolCall)
```

## Chaining

먼저 [tool-calling model](/oss/javascript/langchain/tools/)에 tool을 바인딩한 다음 호출하여 chain에서 tool을 사용할 수 있습니다:

```typescript
// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai"

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
})
```

```typescript
import { HumanMessage } from "@langchain/core/messages";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableLambda } from "@langchain/core/runnables";

const prompt = ChatPromptTemplate.fromMessages(
  [
    ["system", "You are a helpful assistant."],
    ["placeholder", "{messages}"],
  ]
)

const llmWithTools = llm.bindTools([tool]);

const chain = prompt.pipe(llmWithTools);

const toolChain = RunnableLambda.from(
  async (userInput: string, config) => {
    const humanMessage = new HumanMessage(userInput,);
    const aiMsg = await chain.invoke({
      messages: [new HumanMessage(userInput)],
    }, config);
    const toolMsgs = await tool.batch(aiMsg.tool_calls, config);
    return chain.invoke({
      messages: [humanMessage, aiMsg, ...toolMsgs],
    }, config);
  }
);

const toolChainResult = await toolChain.invoke("what is the current weather in sf?");
```

```typescript
const { tool_calls, content } = toolChainResult;

console.log("AIMessage", JSON.stringify({
  tool_calls,
  content,
}, null, 2));
```

## Agents

에이전트에서 LangChain tool을 사용하는 방법에 대한 가이드는 [LangGraph.js](https://langchain-ai.github.io/langgraphjs/how-tos/#tool-calling) 문서를 참조하세요.

## API reference

모든 Tavily Search API 기능 및 구성에 대한 자세한 문서는 API reference를 참조하세요:

[docs.tavily.com/documentation/api-reference/endpoint/search](https://docs.tavily.com/documentation/api-reference/endpoint/search)

## Related

[Tool docs](/oss/javascript/langchain/tools)

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/tools/tavily_search.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
