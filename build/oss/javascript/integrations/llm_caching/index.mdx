---
title: Model cache
---

[LLM 호출 캐싱](/oss/javascript/langchain/models#caching)은 테스트, 비용 절감 및 속도 향상에 유용할 수 있습니다.

다음은 다양한 전략을 가진 여러 cache를 사용하여 개별 LLM 호출 결과를 캐싱할 수 있는 통합 목록입니다.

<Columns cols={3}>
  <Card
    title="Azure Cosmos DB NoSQL Semantic Cache"
    icon="link"
    href="/oss/javascript/integrations/llm_caching/azure_cosmosdb_nosql"
    arrow="true"
    cta="가이드 보기"
  >
  </Card>
</Columns>

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/llm_caching/index.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
