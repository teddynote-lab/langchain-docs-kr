---
title: Streaming
---



LangGraph는 실시간 업데이트를 제공하기 위한 streaming 시스템을 구현합니다. Streaming은 LLM 기반 애플리케이션의 응답성을 향상시키는 데 매우 중요합니다. 완전한 응답이 준비되기 전에도 출력을 점진적으로 표시함으로써, streaming은 특히 LLM의 지연 시간을 처리할 때 사용자 경험(UX)을 크게 개선합니다.

LangGraph streaming으로 가능한 것들:

* <Icon icon="share-nodes" size={16} /> [**Graph state stream**](#stream-graph-state) — `updates`와 `values` 모드로 state 업데이트 / 값을 가져옵니다.
* <Icon icon="square-poll-horizontal" size={16} /> [**Subgraph 출력 stream**](#stream-subgraph-outputs) — 부모 graph와 중첩된 subgraph의 출력을 모두 포함합니다.
* <Icon icon="square-binary" size={16} /> [**LLM token stream**](#messages) — node, subgraph 또는 tool 내부 어디에서든 token stream을 캡처합니다.
* <Icon icon="table" size={16} /> [**사용자 정의 데이터 stream**](#stream-custom-data) — tool function에서 직접 사용자 정의 업데이트 또는 진행 신호를 전송합니다.
* <Icon icon="layer-plus" size={16} /> [**여러 streaming 모드 사용**](#stream-multiple-modes) — `values` (전체 state), `updates` (state 델타), `messages` (LLM token + metadata), `custom` (임의의 사용자 데이터), 또는 `debug` (상세한 trace) 중에서 선택합니다.

## 지원되는 stream 모드



다음 stream 모드 중 하나 이상을 리스트로 [`stream`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.CompiledStateGraph.html#stream) 메서드에 전달하세요:


| 모드       | 설명                                                                                                                                                                         |
| ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `values`   | graph의 각 단계 후 state의 전체 값을 stream합니다.                                                                                                                   |
| `updates`  | graph의 각 단계 후 state에 대한 업데이트를 stream합니다. 동일한 단계에서 여러 업데이트가 발생하면(예: 여러 node가 실행됨), 해당 업데이트는 개별적으로 stream됩니다. |
| `custom`   | graph node 내부에서 사용자 정의 데이터를 stream합니다.                                                                                                                                   |
| `messages` | LLM이 호출되는 모든 graph node에서 2-tuple (LLM token, metadata)을 stream합니다.                                                                                                |
| `debug`    | graph 실행 전반에 걸쳐 가능한 한 많은 정보를 stream합니다.                                                                                                      |

## 기본 사용 예제



LangGraph graph는 [`stream`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Pregel.html#stream) 메서드를 노출하여 stream된 출력을 iterator로 생성합니다.

```typescript
for await (const chunk of await graph.stream(inputs, {
  streamMode: "updates",
})) {
  console.log(chunk);
}
```


<Accordion title="확장 예제: streaming updates">


  ```typescript
  import { StateGraph, START, END } from "@langchain/langgraph";
  import * as z from "zod";

  const State = z.object({
    topic: z.string(),
    joke: z.string(),
  });

  const graph = new StateGraph(State)
    .addNode("refineTopic", (state) => {
      return { topic: state.topic + " and cats" };
    })
    .addNode("generateJoke", (state) => {
      return { joke: `This is a joke about ${state.topic}` };
    })
    .addEdge(START, "refineTopic")
    .addEdge("refineTopic", "generateJoke")
    .addEdge("generateJoke", END)
    .compile();

  for await (const chunk of await graph.stream(
    { topic: "ice cream" },
    // Set streamMode: "updates" to stream only the updates to the graph state after each node
    // Other stream modes are also available. See supported stream modes for details
    { streamMode: "updates" }
  )) {
    console.log(chunk);
  }
  ```


  ```output
  {'refineTopic': {'topic': 'ice cream and cats'}}
  {'generateJoke': {'joke': 'This is a joke about ice cream and cats'}}
  ```
</Accordion>

## 여러 모드 stream



`streamMode` 매개변수에 배열을 전달하여 여러 모드를 동시에 stream할 수 있습니다.

stream된 출력은 `[mode, chunk]` tuple이 되며, 여기서 `mode`는 stream 모드의 이름이고 `chunk`는 해당 모드에서 stream된 데이터입니다.

```typescript
for await (const [mode, chunk] of await graph.stream(inputs, {
  streamMode: ["updates", "custom"],
})) {
  console.log(chunk);
}
```


## Graph state stream

stream 모드 `updates`와 `values`를 사용하여 graph가 실행될 때 state를 stream합니다.

* `updates`는 graph의 각 단계 후 state에 대한 **업데이트**를 stream합니다.
* `values`는 graph의 각 단계 후 state의 **전체 값**을 stream합니다.



```typescript
import { StateGraph, START, END } from "@langchain/langgraph";
import * as z from "zod";

const State = z.object({
  topic: z.string(),
  joke: z.string(),
});

const graph = new StateGraph(State)
  .addNode("refineTopic", (state) => {
    return { topic: state.topic + " and cats" };
  })
  .addNode("generateJoke", (state) => {
    return { joke: `This is a joke about ${state.topic}` };
  })
  .addEdge(START, "refineTopic")
  .addEdge("refineTopic", "generateJoke")
  .addEdge("generateJoke", END)
  .compile();
```


<Tabs>
    <Tab title="updates">
    각 단계 후 node에서 반환된 **state 업데이트**만 stream하려면 이것을 사용하세요. stream된 출력에는 node의 이름과 업데이트가 포함됩니다.



    ```typescript
    for await (const chunk of await graph.stream(
      { topic: "ice cream" },
      { streamMode: "updates" }
    )) {
      console.log(chunk);
    }
    ```

    </Tab>
    <Tab title="values">
    각 단계 후 graph의 **전체 state**를 stream하려면 이것을 사용하세요.



    ```typescript
    for await (const chunk of await graph.stream(
      { topic: "ice cream" },
      { streamMode: "values" }
    )) {
      console.log(chunk);
    }
    ```

    </Tab>
</Tabs>

## Subgraph 출력 stream



stream된 출력에 [subgraph](/oss/javascript/langgraph/use-subgraphs)의 출력을 포함하려면, 부모 graph의 `.stream()` 메서드에서 `subgraphs: true`를 설정할 수 있습니다. 이렇게 하면 부모 graph와 모든 subgraph의 출력이 stream됩니다.

출력은 tuple `[namespace, data]`로 stream되며, 여기서 `namespace`는 subgraph가 호출되는 node의 경로를 가진 tuple입니다. 예: `["parent_node:<task_id>", "child_node:<task_id>"]`.

```typescript
for await (const chunk of await graph.stream(
  { foo: "foo" },
  {
    // Set subgraphs: true to stream outputs from subgraphs
    subgraphs: true,
    streamMode: "updates",
  }
)) {
  console.log(chunk);
}
```


<Accordion title="확장 예제: subgraph에서 streaming">


  ```typescript
  import { StateGraph, START } from "@langchain/langgraph";
  import * as z from "zod";

  // Define subgraph
  const SubgraphState = z.object({
    foo: z.string(), // note that this key is shared with the parent graph state
    bar: z.string(),
  });

  const subgraphBuilder = new StateGraph(SubgraphState)
    .addNode("subgraphNode1", (state) => {
      return { bar: "bar" };
    })
    .addNode("subgraphNode2", (state) => {
      return { foo: state.foo + state.bar };
    })
    .addEdge(START, "subgraphNode1")
    .addEdge("subgraphNode1", "subgraphNode2");
  const subgraph = subgraphBuilder.compile();

  // Define parent graph
  const ParentState = z.object({
    foo: z.string(),
  });

  const builder = new StateGraph(ParentState)
    .addNode("node1", (state) => {
      return { foo: "hi! " + state.foo };
    })
    .addNode("node2", subgraph)
    .addEdge(START, "node1")
    .addEdge("node1", "node2");
  const graph = builder.compile();

  for await (const chunk of await graph.stream(
    { foo: "foo" },
    {
      streamMode: "updates",
      // Set subgraphs: true to stream outputs from subgraphs
      subgraphs: true,
    }
  )) {
    console.log(chunk);
  }
  ```




  ```
  [[], {'node1': {'foo': 'hi! foo'}}]
  [['node2:dfddc4ba-c3c5-6887-5012-a243b5b377c2'], {'subgraphNode1': {'bar': 'bar'}}]
  [['node2:dfddc4ba-c3c5-6887-5012-a243b5b377c2'], {'subgraphNode2': {'foo': 'hi! foobar'}}]
  [[], {'node2': {'foo': 'hi! foobar'}}]
  ```


  **참고**: node 업데이트뿐만 아니라 어떤 graph(또는 subgraph)에서 stream하고 있는지 알려주는 namespace도 받고 있습니다.
</Accordion>

<a id="debug"></a>
### Debugging

`debug` streaming 모드를 사용하여 graph 실행 전반에 걸쳐 가능한 한 많은 정보를 stream합니다. stream된 출력에는 node의 이름과 전체 state가 포함됩니다.



```typescript
for await (const chunk of await graph.stream(
  { topic: "ice cream" },
  { streamMode: "debug" }
)) {
  console.log(chunk);
}
```


<a id="messages"></a>
## LLM token

`messages` streaming 모드를 사용하여 graph의 모든 부분(node, tool, subgraph 또는 task 포함)에서 Large Language Model (LLM) 출력을 **token 단위로** stream합니다.



[`messages` 모드](#supported-stream-modes)에서 stream된 출력은 tuple `[message_chunk, metadata]`이며, 여기서:

* `message_chunk`: LLM의 token 또는 message segment입니다.
* `metadata`: graph node 및 LLM 호출에 대한 세부 정보를 포함하는 dictionary입니다.

> LLM이 LangChain integration으로 제공되지 않는 경우, 대신 `custom` 모드를 사용하여 출력을 stream할 수 있습니다. 자세한 내용은 [모든 LLM과 함께 사용](#use-with-any-llm)을 참조하세요.

```typescript
import { ChatOpenAI } from "@langchain/openai";
import { StateGraph, START } from "@langchain/langgraph";
import * as z from "zod";

const MyState = z.object({
  topic: z.string(),
  joke: z.string().default(""),
});

const model = new ChatOpenAI({ model: "gpt-4o-mini" });

const callModel = async (state: z.infer<typeof MyState>) => {
  // Call the LLM to generate a joke about a topic
  // Note that message events are emitted even when the LLM is run using .invoke rather than .stream
  const modelResponse = await model.invoke([
    { role: "user", content: `Generate a joke about ${state.topic}` },
  ]);
  return { joke: modelResponse.content };
};

const graph = new StateGraph(MyState)
  .addNode("callModel", callModel)
  .addEdge(START, "callModel")
  .compile();

// The "messages" stream mode returns an iterator of tuples [messageChunk, metadata]
// where messageChunk is the token streamed by the LLM and metadata is a dictionary
// with information about the graph node where the LLM was called and other information
for await (const [messageChunk, metadata] of await graph.stream(
  { topic: "ice cream" },
  { streamMode: "messages" }
)) {
  if (messageChunk.content) {
    console.log(messageChunk.content + "|");
  }
}
```


#### LLM 호출별로 필터링

LLM 호출과 `tags`를 연결하여 LLM 호출별로 stream된 token을 필터링할 수 있습니다.



```typescript
import { ChatOpenAI } from "@langchain/openai";

// model1 is tagged with "joke"
const model1 = new ChatOpenAI({
  model: "gpt-4o-mini",
  tags: ['joke']
});
// model2 is tagged with "poem"
const model2 = new ChatOpenAI({
  model: "gpt-4o-mini",
  tags: ['poem']
});

const graph = // ... define a graph that uses these LLMs

// The streamMode is set to "messages" to stream LLM tokens
// The metadata contains information about the LLM invocation, including the tags
for await (const [msg, metadata] of await graph.stream(
  { topic: "cats" },
  { streamMode: "messages" }
)) {
  // Filter the streamed tokens by the tags field in the metadata to only include
  // the tokens from the LLM invocation with the "joke" tag
  if (metadata.tags?.includes("joke")) {
    console.log(msg.content + "|");
  }
}
```


<Accordion title="확장 예제: tag로 필터링">


  ```typescript
  import { ChatOpenAI } from "@langchain/openai";
  import { StateGraph, START } from "@langchain/langgraph";
  import * as z from "zod";

  // The jokeModel is tagged with "joke"
  const jokeModel = new ChatOpenAI({
    model: "gpt-4o-mini",
    tags: ["joke"]
  });
  // The poemModel is tagged with "poem"
  const poemModel = new ChatOpenAI({
    model: "gpt-4o-mini",
    tags: ["poem"]
  });

  const State = z.object({
    topic: z.string(),
    joke: z.string(),
    poem: z.string(),
  });

  const graph = new StateGraph(State)
    .addNode("callModel", (state) => {
      const topic = state.topic;
      console.log("Writing joke...");

      const jokeResponse = await jokeModel.invoke([
        { role: "user", content: `Write a joke about ${topic}` }
      ]);

      console.log("\n\nWriting poem...");
      const poemResponse = await poemModel.invoke([
        { role: "user", content: `Write a short poem about ${topic}` }
      ]);

      return {
        joke: jokeResponse.content,
        poem: poemResponse.content
      };
    })
    .addEdge(START, "callModel")
    .compile();

  // The streamMode is set to "messages" to stream LLM tokens
  // The metadata contains information about the LLM invocation, including the tags
  for await (const [msg, metadata] of await graph.stream(
    { topic: "cats" },
    { streamMode: "messages" }
  )) {
    // Filter the streamed tokens by the tags field in the metadata to only include
    // the tokens from the LLM invocation with the "joke" tag
    if (metadata.tags?.includes("joke")) {
      console.log(msg.content + "|");
    }
  }
  ```

</Accordion>

#### Node별로 필터링

특정 node에서만 token을 stream하려면, `stream_mode="messages"`를 사용하고 stream된 metadata의 `langgraph_node` 필드로 출력을 필터링하세요:



```typescript
// The "messages" stream mode returns a tuple of [messageChunk, metadata]
// where messageChunk is the token streamed by the LLM and metadata is a dictionary
// with information about the graph node where the LLM was called and other information
for await (const [msg, metadata] of await graph.stream(
  inputs,
  { streamMode: "messages" }
)) {
  // Filter the streamed tokens by the langgraph_node field in the metadata
  // to only include the tokens from the specified node
  if (msg.content && metadata.langgraph_node === "some_node_name") {
    // ...
  }
}
```


<Accordion title="확장 예제: 특정 node에서 LLM token streaming">


  ```typescript
  import { ChatOpenAI } from "@langchain/openai";
  import { StateGraph, START } from "@langchain/langgraph";
  import * as z from "zod";

  const model = new ChatOpenAI({ model: "gpt-4o-mini" });

  const State = z.object({
    topic: z.string(),
    joke: z.string(),
    poem: z.string(),
  });

  const graph = new StateGraph(State)
    .addNode("writeJoke", async (state) => {
      const topic = state.topic;
      const jokeResponse = await model.invoke([
        { role: "user", content: `Write a joke about ${topic}` }
      ]);
      return { joke: jokeResponse.content };
    })
    .addNode("writePoem", async (state) => {
      const topic = state.topic;
      const poemResponse = await model.invoke([
        { role: "user", content: `Write a short poem about ${topic}` }
      ]);
      return { poem: poemResponse.content };
    })
    // write both the joke and the poem concurrently
    .addEdge(START, "writeJoke")
    .addEdge(START, "writePoem")
    .compile();

  // The "messages" stream mode returns a tuple of [messageChunk, metadata]
  // where messageChunk is the token streamed by the LLM and metadata is a dictionary
  // with information about the graph node where the LLM was called and other information
  for await (const [msg, metadata] of await graph.stream(
    { topic: "cats" },
    { streamMode: "messages" }
  )) {
    // Filter the streamed tokens by the langgraph_node field in the metadata
    // to only include the tokens from the writePoem node
    if (msg.content && metadata.langgraph_node === "writePoem") {
      console.log(msg.content + "|");
    }
  }
  ```

</Accordion>

## 사용자 정의 데이터 stream



LangGraph node 또는 tool 내부에서 **사용자 정의 데이터**를 전송하려면 다음 단계를 따르세요:

1. `LangGraphRunnableConfig`의 `writer` 매개변수를 사용하여 사용자 정의 데이터를 emit합니다.
2. `.stream()`을 호출할 때 `streamMode: "custom"`을 설정하여 stream에서 사용자 정의 데이터를 가져옵니다. 여러 모드를 결합할 수 있지만(예: `["updates", "custom"]`), 최소한 하나는 `"custom"`이어야 합니다.

<Tabs>
    <Tab title="node">
    ```typescript
    import { StateGraph, START, LangGraphRunnableConfig } from "@langchain/langgraph";
    import * as z from "zod";

    const State = z.object({
      query: z.string(),
      answer: z.string(),
    });

    const graph = new StateGraph(State)
      .addNode("node", async (state, config) => {
        // Use the writer to emit a custom key-value pair (e.g., progress update)
        config.writer({ custom_key: "Generating custom data inside node" });
        return { answer: "some data" };
      })
      .addEdge(START, "node")
      .compile();

    const inputs = { query: "example" };

    // Set streamMode: "custom" to receive the custom data in the stream
    for await (const chunk of await graph.stream(inputs, { streamMode: "custom" })) {
      console.log(chunk);
    }
    ```
    </Tab>
    <Tab title="tool">
    ```typescript
    import { tool } from "@langchain/core/tools";
    import { LangGraphRunnableConfig } from "@langchain/langgraph";
    import * as z from "zod";

    const queryDatabase = tool(
      async (input, config: LangGraphRunnableConfig) => {
        // Use the writer to emit a custom key-value pair (e.g., progress update)
        config.writer({ data: "Retrieved 0/100 records", type: "progress" });
        // perform query
        // Emit another custom key-value pair
        config.writer({ data: "Retrieved 100/100 records", type: "progress" });
        return "some-answer";
      },
      {
        name: "query_database",
        description: "Query the database.",
        schema: z.object({
          query: z.string().describe("The query to execute."),
        }),
      }
    );

    const graph = // ... define a graph that uses this tool

    // Set streamMode: "custom" to receive the custom data in the stream
    for await (const chunk of await graph.stream(inputs, { streamMode: "custom" })) {
      console.log(chunk);
    }
    ```
    </Tab>
</Tabs>


## 모든 LLM과 함께 사용



`streamMode: "custom"`을 사용하여 **모든 LLM API**에서 데이터를 stream할 수 있습니다 — 해당 API가 LangChain chat model interface를 구현하지 **않더라도** 가능합니다.

이를 통해 자체 streaming interface를 제공하는 raw LLM client 또는 외부 서비스를 통합할 수 있어, LangGraph를 사용자 정의 설정에 매우 유연하게 만듭니다.

```typescript
import { LangGraphRunnableConfig } from "@langchain/langgraph";

const callArbitraryModel = async (
  state: any,
  config: LangGraphRunnableConfig
) => {
  // Example node that calls an arbitrary model and streams the output
  // Assume you have a streaming client that yields chunks
  // Generate LLM tokens using your custom streaming client
  for await (const chunk of yourCustomStreamingClient(state.topic)) {
    // Use the writer to send custom data to the stream
    config.writer({ custom_llm_chunk: chunk });
  }
  return { result: "completed" };
};

const graph = new StateGraph(State)
  .addNode("callArbitraryModel", callArbitraryModel)
  // Add other nodes and edges as needed
  .compile();

// Set streamMode: "custom" to receive the custom data in the stream
for await (const chunk of await graph.stream(
  { topic: "cats" },
  { streamMode: "custom" }
)) {
  // The chunk will contain the custom data streamed from the llm
  console.log(chunk);
}
```


<Accordion title="확장 예제: 임의의 chat model streaming">


  ```typescript
  import { StateGraph, START, MessagesZodMeta, LangGraphRunnableConfig } from "@langchain/langgraph";
  import { BaseMessage } from "@langchain/core/messages";
  import { registry } from "@langchain/langgraph/zod";
  import * as z from "zod";
  import OpenAI from "openai";

  const openaiClient = new OpenAI();
  const modelName = "gpt-4o-mini";

  async function* streamTokens(modelName: string, messages: any[]) {
    const response = await openaiClient.chat.completions.create({
      messages,
      model: modelName,
      stream: true,
    });

    let role: string | null = null;
    for await (const chunk of response) {
      const delta = chunk.choices[0]?.delta;

      if (delta?.role) {
        role = delta.role;
      }

      if (delta?.content) {
        yield { role, content: delta.content };
      }
    }
  }

  // this is our tool
  const getItems = tool(
    async (input, config: LangGraphRunnableConfig) => {
      let response = "";
      for await (const msgChunk of streamTokens(
        modelName,
        [
          {
            role: "user",
            content: `Can you tell me what kind of items i might find in the following place: '${input.place}'. List at least 3 such items separating them by a comma. And include a brief description of each item.`,
          },
        ]
      )) {
        response += msgChunk.content;
        config.writer?.(msgChunk);
      }
      return response;
    },
    {
      name: "get_items",
      description: "Use this tool to list items one might find in a place you're asked about.",
      schema: z.object({
        place: z.string().describe("The place to look up items for."),
      }),
    }
  );

  const State = z.object({
    messages: z
      .array(z.custom<BaseMessage>())
      .register(registry, MessagesZodMeta),
  });

  const graph = new StateGraph(State)
    // this is the tool-calling graph node
    .addNode("callTool", async (state) => {
      const aiMessage = state.messages.at(-1);
      const toolCall = aiMessage.tool_calls?.at(-1);

      const functionName = toolCall?.function?.name;
      if (functionName !== "get_items") {
        throw new Error(`Tool ${functionName} not supported`);
      }

      const functionArguments = toolCall?.function?.arguments;
      const args = JSON.parse(functionArguments);

      const functionResponse = await getItems.invoke(args);
      const toolMessage = {
        tool_call_id: toolCall.id,
        role: "tool",
        name: functionName,
        content: functionResponse,
      };
      return { messages: [toolMessage] };
    })
    .addEdge(START, "callTool")
    .compile();
  ```

  tool call을 포함하는 [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html)로 graph를 호출해 봅시다:

  ```typescript
  const inputs = {
    messages: [
      {
        content: null,
        role: "assistant",
        tool_calls: [
          {
            id: "1",
            function: {
              arguments: '{"place":"bedroom"}',
              name: "get_items",
            },
            type: "function",
          }
        ],
      }
    ]
  };

  for await (const chunk of await graph.stream(
    inputs,
    { streamMode: "custom" }
  )) {
    console.log(chunk.content + "|");
  }
  ```

</Accordion>

## 특정 chat model에 대해 streaming 비활성화

애플리케이션이 streaming을 지원하는 model과 지원하지 않는 model을 혼합하는 경우, streaming을 지원하지 않는 model에 대해 명시적으로 streaming을 비활성화해야 할 수 있습니다.



model을 초기화할 때 `streaming: false`를 설정하세요.

```typescript
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  model: "o1-preview",
  // Set streaming: false to disable streaming for the chat model
  streaming: false,
});
```

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/streaming.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
