---
title: Functional API 사용하기
sidebarTitle: Functional API 사용하기
---



[**Functional API**](/oss/javascript/langgraph/functional-api)를 사용하면 기존 코드를 최소한으로 변경하면서 LangGraph의 핵심 기능인 [persistence](/oss/javascript/langgraph/persistence), [memory](/oss/javascript/langgraph/add-memory), [human-in-the-loop](/oss/javascript/langgraph/interrupts), [streaming](/oss/javascript/langgraph/streaming)을 애플리케이션에 추가할 수 있습니다.

<Tip>
Functional API에 대한 개념적 정보는 [Functional API](/oss/javascript/langgraph/functional-api)를 참조하세요.
</Tip>

## 간단한 워크플로우 만들기

`entrypoint`를 정의할 때, 입력은 함수의 첫 번째 인자로 제한됩니다. 여러 입력을 전달하려면 dictionary를 사용할 수 있습니다.



```typescript
const checkpointer = new MemorySaver();

const myWorkflow = entrypoint(
  { checkpointer, name: "myWorkflow" },
  async (inputs: { value: number; anotherValue: number }) => {
    const value = inputs.value;
    const anotherValue = inputs.anotherValue;
    // ...
  }
);

await myWorkflow.invoke({ value: 1, anotherValue: 2 });
```


<Accordion title="확장 예제: 간단한 워크플로우">


  ```typescript
  import { v4 as uuidv4 } from "uuid";
  import { entrypoint, task, MemorySaver } from "@langchain/langgraph";

  // Task that checks if a number is even
  const isEven = task("isEven", async (number: number) => {
    return number % 2 === 0;
  });

  // Task that formats a message
  const formatMessage = task("formatMessage", async (isEven: boolean) => {
    return isEven ? "The number is even." : "The number is odd.";
  });

  // Create a checkpointer for persistence
  const checkpointer = new MemorySaver();

  const workflow = entrypoint(
    { checkpointer, name: "workflow" },
    async (inputs: { number: number }) => {
      // Simple workflow to classify a number
      const even = await isEven(inputs.number);
      return await formatMessage(even);
    }
  );

  // Run the workflow with a unique thread ID
  const config = { configurable: { thread_id: uuidv4() } };
  const result = await workflow.invoke({ number: 7 }, config);
  console.log(result);
  ```

</Accordion>

<Accordion title="확장 예제: LLM으로 에세이 작성하기">
  이 예제는 `@task`와 `@entrypoint` decorator를 구문적으로 사용하는 방법을 보여줍니다.
  checkpointer가 제공되면 워크플로우 결과가 checkpointer에 저장됩니다.



  ```typescript
  import { v4 as uuidv4 } from "uuid";
  import { ChatOpenAI } from "@langchain/openai";
  import { entrypoint, task, MemorySaver } from "@langchain/langgraph";

  const model = new ChatOpenAI({ model: "gpt-3.5-turbo" });

  // Task: generate essay using an LLM
  const composeEssay = task("composeEssay", async (topic: string) => {
    // Generate an essay about the given topic
    const response = await model.invoke([
      { role: "system", content: "You are a helpful assistant that writes essays." },
      { role: "user", content: `Write an essay about ${topic}.` }
    ]);
    return response.content as string;
  });

  // Create a checkpointer for persistence
  const checkpointer = new MemorySaver();

  const workflow = entrypoint(
    { checkpointer, name: "workflow" },
    async (topic: string) => {
      // Simple workflow that generates an essay with an LLM
      return await composeEssay(topic);
    }
  );

  // Execute the workflow
  const config = { configurable: { thread_id: uuidv4() } };
  const result = await workflow.invoke("the history of flight", config);
  console.log(result);
  ```

</Accordion>

## 병렬 실행

Task들을 동시에 호출하고 결과를 기다림으로써 병렬로 실행할 수 있습니다. 이는 IO 바운드 작업(예: LLM API 호출)의 성능을 향상시키는 데 유용합니다.



```typescript
const addOne = task("addOne", async (number: number) => {
  return number + 1;
});

const graph = entrypoint(
  { checkpointer, name: "graph" },
  async (numbers: number[]) => {
    return await Promise.all(numbers.map(addOne));
  }
);
```


<Accordion title="확장 예제: 병렬 LLM 호출">
  이 예제는 `@task`를 사용하여 여러 LLM 호출을 병렬로 실행하는 방법을 보여줍니다. 각 호출은 다른 주제에 대한 단락을 생성하고, 결과는 단일 텍스트 출력으로 결합됩니다.



  ```typescript
  import { v4 as uuidv4 } from "uuid";
  import { ChatOpenAI } from "@langchain/openai";
  import { entrypoint, task, MemorySaver } from "@langchain/langgraph";

  // Initialize the LLM model
  const model = new ChatOpenAI({ model: "gpt-3.5-turbo" });

  // Task that generates a paragraph about a given topic
  const generateParagraph = task("generateParagraph", async (topic: string) => {
    const response = await model.invoke([
      { role: "system", content: "You are a helpful assistant that writes educational paragraphs." },
      { role: "user", content: `Write a paragraph about ${topic}.` }
    ]);
    return response.content as string;
  });

  // Create a checkpointer for persistence
  const checkpointer = new MemorySaver();

  const workflow = entrypoint(
    { checkpointer, name: "workflow" },
    async (topics: string[]) => {
      // Generates multiple paragraphs in parallel and combines them
      const paragraphs = await Promise.all(topics.map(generateParagraph));
      return paragraphs.join("\n\n");
    }
  );

  // Run the workflow
  const config = { configurable: { thread_id: uuidv4() } };
  const result = await workflow.invoke(["quantum computing", "climate change", "history of aviation"], config);
  console.log(result);
  ```


  이 예제는 LangGraph의 동시성 모델을 사용하여 특히 LLM completion과 같은 I/O 작업이 포함된 경우 실행 시간을 개선합니다.
</Accordion>

## Graph 호출하기

**Functional API**와 [**Graph API**](/oss/javascript/langgraph/graph-api)는 동일한 기본 runtime을 공유하므로 동일한 애플리케이션에서 함께 사용할 수 있습니다.



```typescript
import { entrypoint } from "@langchain/langgraph";
import { StateGraph } from "@langchain/langgraph";

const builder = new StateGraph(/* ... */);
// ...
const someGraph = builder.compile();

const someWorkflow = entrypoint(
  { name: "someWorkflow" },
  async (someInput: Record<string, any>) => {
    // Call a graph defined using the graph API
    const result1 = await someGraph.invoke(/* ... */);
    // Call another graph defined using the graph API
    const result2 = await anotherGraph.invoke(/* ... */);
    return {
      result1,
      result2,
    };
  }
);
```


<Accordion title="확장 예제: functional API에서 간단한 graph 호출하기">


  ```typescript
  import { v4 as uuidv4 } from "uuid";
  import { entrypoint, MemorySaver } from "@langchain/langgraph";
  import { StateGraph } from "@langchain/langgraph";
  import * as z from "zod";

  // Define the shared state type
  const State = z.object({
    foo: z.number(),
  });

  // Build the graph using the Graph API
  const builder = new StateGraph(State)
    .addNode("double", (state) => {
      return { foo: state.foo * 2 };
    })
    .addEdge("__start__", "double");
  const graph = builder.compile();

  // Define the functional API workflow
  const checkpointer = new MemorySaver();

  const workflow = entrypoint(
    { checkpointer, name: "workflow" },
    async (x: number) => {
      const result = await graph.invoke({ foo: x });
      return { bar: result.foo };
    }
  );

  // Execute the workflow
  const config = { configurable: { thread_id: uuidv4() } };
  console.log(await workflow.invoke(5, config)); // Output: { bar: 10 }
  ```

</Accordion>

## 다른 entrypoint 호출하기

**entrypoint** 또는 **task** 내에서 다른 **entrypoint**를 호출할 수 있습니다.



```typescript
// Will automatically use the checkpointer from the parent entrypoint
const someOtherWorkflow = entrypoint(
  { name: "someOtherWorkflow" },
  async (inputs: { value: number }) => {
    return inputs.value;
  }
);

const myWorkflow = entrypoint(
  { checkpointer, name: "myWorkflow" },
  async (inputs: { value: number }) => {
    const value = await someOtherWorkflow.invoke({ value: 1 });
    return value;
  }
);
```


<Accordion title="확장 예제: 다른 entrypoint 호출하기">


  ```typescript
  import { v4 as uuidv4 } from "uuid";
  import { entrypoint, MemorySaver } from "@langchain/langgraph";

  // Initialize a checkpointer
  const checkpointer = new MemorySaver();

  // A reusable sub-workflow that multiplies a number
  const multiply = entrypoint(
    { name: "multiply" },
    async (inputs: { a: number; b: number }) => {
      return inputs.a * inputs.b;
    }
  );

  // Main workflow that invokes the sub-workflow
  const main = entrypoint(
    { checkpointer, name: "main" },
    async (inputs: { x: number; y: number }) => {
      const result = await multiply.invoke({ a: inputs.x, b: inputs.y });
      return { product: result };
    }
  );

  // Execute the main workflow
  const config = { configurable: { thread_id: uuidv4() } };
  console.log(await main.invoke({ x: 6, y: 7 }, config)); // Output: { product: 42 }
  ```

</Accordion>

## Streaming

**Functional API**는 **Graph API**와 동일한 streaming 메커니즘을 사용합니다. 자세한 내용은 [**streaming 가이드**](/oss/javascript/langgraph/streaming) 섹션을 참조하세요.

업데이트와 사용자 정의 데이터를 모두 스트리밍하는 streaming API 사용 예제입니다.



```typescript
import {
  entrypoint,
  MemorySaver,
  LangGraphRunnableConfig,
} from "@langchain/langgraph";

const checkpointer = new MemorySaver();

const main = entrypoint(
  { checkpointer, name: "main" },
  async (
    inputs: { x: number },
    config: LangGraphRunnableConfig
  ): Promise<number> => {
    config.writer?.("Started processing");   // [!code highlight]
    const result = inputs.x * 2;
    config.writer?.(`Result is ${result}`);   // [!code highlight]
    return result;
  }
);

const config = { configurable: { thread_id: "abc" } };

  // [!code highlight]
for await (const [mode, chunk] of await main.stream(
  { x: 5 },
  { streamMode: ["custom", "updates"], ...config }   // [!code highlight]
)) {
  console.log(`${mode}: ${JSON.stringify(chunk)}`);
}
```

1. 계산이 시작되기 전에 사용자 정의 데이터를 emit합니다.
2. 결과를 계산한 후 다른 사용자 정의 메시지를 emit합니다.
3. `.stream()`을 사용하여 스트리밍된 출력을 처리합니다.
4. 사용할 streaming mode를 지정합니다.

```
updates: {"addOne": 2}
updates: {"addTwo": 3}
custom: "hello"
custom: "world"
updates: {"main": 5}
```


## Retry policy



```typescript
import {
  MemorySaver,
  entrypoint,
  task,
  RetryPolicy,
} from "@langchain/langgraph";

// This variable is just used for demonstration purposes to simulate a network failure.
// It's not something you will have in your actual code.
let attempts = 0;

// Let's configure the RetryPolicy to retry on ValueError.
// The default RetryPolicy is optimized for retrying specific network errors.
const retryPolicy: RetryPolicy = { retryOn: (error) => error instanceof Error };

const getInfo = task(
  {
    name: "getInfo",
    retry: retryPolicy,
  },
  () => {
    attempts += 1;

    if (attempts < 2) {
      throw new Error("Failure");
    }
    return "OK";
  }
);

const checkpointer = new MemorySaver();

const main = entrypoint(
  { checkpointer, name: "main" },
  async (inputs: Record<string, any>) => {
    return await getInfo();
  }
);

const config = {
  configurable: {
    thread_id: "1",
  },
};

await main.invoke({ any_input: "foobar" }, config);
```

```
'OK'
```


## Task 캐싱하기



```typescript
import {
  InMemoryCache,
  entrypoint,
  task,
  CachePolicy,
} from "@langchain/langgraph";

const slowAdd = task(
  {
    name: "slowAdd",
    cache: { ttl: 120 },   // [!code highlight]
  },
  async (x: number) => {
    await new Promise((resolve) => setTimeout(resolve, 1000));
    return x * 2;
  }
);

const main = entrypoint(
  { cache: new InMemoryCache(), name: "main" },
  async (inputs: { x: number }) => {
    const result1 = await slowAdd(inputs.x);
    const result2 = await slowAdd(inputs.x);
    return { result1, result2 };
  }
);

for await (const chunk of await main.stream(
  { x: 5 },
  { streamMode: "updates" }
)) {
  console.log(chunk);
}

//> { slowAdd: 10 }
//> { slowAdd: 10, '__metadata__': { cached: true } }
//> { main: { result1: 10, result2: 10 } }
```

1. `ttl`은 초 단위로 지정됩니다. 이 시간이 지나면 캐시가 무효화됩니다.


## 오류 후 재개하기



```typescript
import { entrypoint, task, MemorySaver } from "@langchain/langgraph";

// This variable is just used for demonstration purposes to simulate a network failure.
// It's not something you will have in your actual code.
let attempts = 0;

const getInfo = task("getInfo", async () => {
  /**
   * Simulates a task that fails once before succeeding.
   * Throws an exception on the first attempt, then returns "OK" on subsequent tries.
   */
  attempts += 1;

  if (attempts < 2) {
    throw new Error("Failure"); // Simulate a failure on the first attempt
  }
  return "OK";
});

// Initialize an in-memory checkpointer for persistence
const checkpointer = new MemorySaver();

const slowTask = task("slowTask", async () => {
  /**
   * Simulates a slow-running task by introducing a 1-second delay.
   */
  await new Promise((resolve) => setTimeout(resolve, 1000));
  return "Ran slow task.";
});

const main = entrypoint(
  { checkpointer, name: "main" },
  async (inputs: Record<string, any>) => {
    /**
     * Main workflow function that runs the slowTask and getInfo tasks sequentially.
     *
     * Parameters:
     * - inputs: Record<string, any> containing workflow input values.
     *
     * The workflow first executes `slowTask` and then attempts to execute `getInfo`,
     * which will fail on the first invocation.
     */
    const slowTaskResult = await slowTask(); // Blocking call to slowTask
    await getInfo(); // Exception will be raised here on the first attempt
    return slowTaskResult;
  }
);

// Workflow execution configuration with a unique thread identifier
const config = {
  configurable: {
    thread_id: "1", // Unique identifier to track workflow execution
  },
};

// This invocation will take ~1 second due to the slowTask execution
try {
  // First invocation will raise an exception due to the `getInfo` task failing
  await main.invoke({ any_input: "foobar" }, config);
} catch (err) {
  // Handle the failure gracefully
}
```

실행을 재개할 때, `slowTask`의 결과가 이미 checkpoint에 저장되어 있으므로 다시 실행할 필요가 없습니다.

```typescript
await main.invoke(null, config);
```

```
'Ran slow task.'
```


## Human-in-the-loop

Functional API는 [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) 함수와 `Command` primitive를 사용하여 [human-in-the-loop](/oss/javascript/langgraph/interrupts) 워크플로우를 지원합니다.

### 기본 human-in-the-loop 워크플로우

세 개의 [task](/oss/javascript/langgraph/functional-api#task)를 생성합니다:

1. `"bar"`를 추가합니다.
2. 사용자 입력을 위해 일시 중지합니다. 재개할 때 사용자 입력을 추가합니다.
3. `"qux"`를 추가합니다.



```typescript
import { entrypoint, task, interrupt, Command } from "@langchain/langgraph";

const step1 = task("step1", async (inputQuery: string) => {
  // Append bar
  return `${inputQuery} bar`;
});

const humanFeedback = task("humanFeedback", async (inputQuery: string) => {
  // Append user input
  const feedback = interrupt(`Please provide feedback: ${inputQuery}`);
  return `${inputQuery} ${feedback}`;
});

const step3 = task("step3", async (inputQuery: string) => {
  // Append qux
  return `${inputQuery} qux`;
});
```


이제 이러한 task들을 [entrypoint](/oss/javascript/langgraph/functional-api#entrypoint)에서 구성할 수 있습니다:



```typescript
import { MemorySaver } from "@langchain/langgraph";

const checkpointer = new MemorySaver();

const graph = entrypoint(
  { checkpointer, name: "graph" },
  async (inputQuery: string) => {
    const result1 = await step1(inputQuery);
    const result2 = await humanFeedback(result1);
    const result3 = await step3(result2);

    return result3;
  }
);
```


[interrupt()](/oss/javascript/langgraph/interrupts#pause-using-interrupt)는 task 내에서 호출되어 사용자가 이전 task의 출력을 검토하고 편집할 수 있도록 합니다. 이전 task의 결과(이 경우 `step_1`)는 저장되므로 [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) 이후에 다시 실행되지 않습니다.

쿼리 문자열을 전송해 보겠습니다:



```typescript
const config = { configurable: { thread_id: "1" } };

for await (const event of await graph.stream("foo", config)) {
  console.log(event);
  console.log("\n");
}
```


`step_1` 이후 [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html)로 일시 중지되었습니다. interrupt는 실행을 재개하기 위한 지침을 제공합니다. 재개하려면 `human_feedback` task가 예상하는 데이터를 포함하는 [`Command`](/oss/javascript/langgraph/interrupts#resuming-interrupts)를 발행합니다.



```typescript
// Continue execution
for await (const event of await graph.stream(
  new Command({ resume: "baz" }),
  config
)) {
  console.log(event);
  console.log("\n");
}
```


재개 후, 실행은 나머지 단계를 진행하고 예상대로 종료됩니다.

### Tool 호출 검토하기

실행 전에 tool 호출을 검토하려면 [`interrupt`](/oss/javascript/langgraph/interrupts#pause-using-interrupt)를 호출하는 `review_tool_call` 함수를 추가합니다. 이 함수가 호출되면 재개 명령을 발행할 때까지 실행이 일시 중지됩니다.

tool 호출이 주어지면 함수는 사용자 검토를 위해 [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html)합니다. 이 시점에서 다음 중 하나를 수행할 수 있습니다:

* tool 호출 수락
* tool 호출 수정 및 계속
* 사용자 정의 tool 메시지 생성(예: 모델에 tool 호출을 다시 포맷하도록 지시)



```typescript
import { ToolCall } from "@langchain/core/messages/tool";
import { ToolMessage } from "@langchain/core/messages";

function reviewToolCall(toolCall: ToolCall): ToolCall | ToolMessage {
  // Review a tool call, returning a validated version
  const humanReview = interrupt({
    question: "Is this correct?",
    tool_call: toolCall,
  });

  const reviewAction = humanReview.action;
  const reviewData = humanReview.data;

  if (reviewAction === "continue") {
    return toolCall;
  } else if (reviewAction === "update") {
    const updatedToolCall = { ...toolCall, args: reviewData };
    return updatedToolCall;
  } else if (reviewAction === "feedback") {
    return new ToolMessage({
      content: reviewData,
      name: toolCall.name,
      tool_call_id: toolCall.id,
    });
  }

  throw new Error(`Unknown review action: ${reviewAction}`);
}
```


이제 생성된 tool 호출을 검토하도록 [entrypoint](/oss/javascript/langgraph/functional-api#entrypoint)를 업데이트할 수 있습니다. tool 호출이 수락되거나 수정되면 이전과 동일한 방식으로 실행합니다. 그렇지 않으면 사용자가 제공한 @[`ToolMessage`]를 추가하기만 하면 됩니다. 이전 task의 결과(이 경우 초기 모델 호출)는 저장되므로 [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) 이후에 다시 실행되지 않습니다.



```typescript
import {
  MemorySaver,
  entrypoint,
  interrupt,
  Command,
  addMessages,
} from "@langchain/langgraph";
import { ToolMessage, AIMessage, BaseMessage } from "@langchain/core/messages";

const checkpointer = new MemorySaver();

const agent = entrypoint(
  { checkpointer, name: "agent" },
  async (
    messages: BaseMessage[],
    previous?: BaseMessage[]
  ): Promise<BaseMessage> => {
    if (previous !== undefined) {
      messages = addMessages(previous, messages);
    }

    let modelResponse = await callModel(messages);
    while (true) {
      if (!modelResponse.tool_calls?.length) {
        break;
      }

      // Review tool calls
      const toolResults: ToolMessage[] = [];
      const toolCalls: ToolCall[] = [];

      for (let i = 0; i < modelResponse.tool_calls.length; i++) {
        const review = reviewToolCall(modelResponse.tool_calls[i]);
        if (review instanceof ToolMessage) {
          toolResults.push(review);
        } else {
          // is a validated tool call
          toolCalls.push(review);
          if (review !== modelResponse.tool_calls[i]) {
            modelResponse.tool_calls[i] = review; // update message
          }
        }
      }

      // Execute remaining tool calls
      const remainingToolResults = await Promise.all(
        toolCalls.map((toolCall) => callTool(toolCall))
      );

      // Append to message list
      messages = addMessages(messages, [
        modelResponse,
        ...toolResults,
        ...remainingToolResults,
      ]);

      // Call model again
      modelResponse = await callModel(messages);
    }

    // Generate final response
    messages = addMessages(messages, modelResponse);
    return entrypoint.final({ value: modelResponse, save: messages });
  }
);
```


## 단기 메모리

단기 메모리를 사용하면 동일한 **thread id**의 서로 다른 **invocation**에서 정보를 저장할 수 있습니다. 자세한 내용은 [short-term memory](/oss/javascript/langgraph/functional-api#short-term-memory)를 참조하세요.

### Checkpoint 관리하기

checkpointer에 저장된 정보를 보고 삭제할 수 있습니다.

<a id="checkpoint"></a>
#### Thread 상태 보기



```typescript
const config = {
  configurable: {
    thread_id: "1",  // [!code highlight]
    // optionally provide an ID for a specific checkpoint,
    // otherwise the latest checkpoint is shown
    // checkpoint_id: "1f029ca3-1f5b-6704-8004-820c16b69a5a" [!code highlight]
  },
};
await graph.getState(config);  // [!code highlight]
```

```
StateSnapshot {
  values: {
    messages: [
      HumanMessage { content: "hi! I'm bob" },
      AIMessage { content: "Hi Bob! How are you doing today?" },
      HumanMessage { content: "what's my name?" },
      AIMessage { content: "Your name is Bob." }
    ]
  },
  next: [],
  config: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1f5b-6704-8004-820c16b69a5a' } },
  metadata: {
    source: 'loop',
    writes: { call_model: { messages: AIMessage { content: "Your name is Bob." } } },
    step: 4,
    parents: {},
    thread_id: '1'
  },
  createdAt: '2025-05-05T16:01:24.680462+00:00',
  parentConfig: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1790-6b0a-8003-baf965b6a38f' } },
  tasks: [],
  interrupts: []
}
```


<a id="checkpoints"></a>
#### Thread 히스토리 보기



```typescript
const config = {
  configurable: {
    thread_id: "1",  // [!code highlight]
  },
};
const history = [];  // [!code highlight]
for await (const state of graph.getStateHistory(config)) {
  history.push(state);
}
```

```
[
  StateSnapshot {
    values: {
      messages: [
        HumanMessage { content: "hi! I'm bob" },
        AIMessage { content: "Hi Bob! How are you doing today? Is there anything I can help you with?" },
        HumanMessage { content: "what's my name?" },
        AIMessage { content: "Your name is Bob." }
      ]
    },
    next: [],
    config: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1f5b-6704-8004-820c16b69a5a' } },
    metadata: { source: 'loop', writes: { call_model: { messages: AIMessage { content: "Your name is Bob." } } }, step: 4, parents: {}, thread_id: '1' },
    createdAt: '2025-05-05T16:01:24.680462+00:00',
    parentConfig: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1790-6b0a-8003-baf965b6a38f' } },
    tasks: [],
    interrupts: []
  },
  // ... more state snapshots
]
```


### 반환 값과 저장 값 분리하기

`entrypoint.final`을 사용하여 호출자에게 반환되는 값과 checkpoint에 저장되는 값을 분리합니다. 이는 다음과 같은 경우에 유용합니다:

* 계산된 결과(예: 요약 또는 상태)를 반환하지만 다음 invocation에서 사용할 다른 내부 값을 저장하려는 경우.
* 다음 실행 시 previous 매개변수에 전달되는 내용을 제어해야 하는 경우.



```typescript
import { entrypoint, MemorySaver } from "@langchain/langgraph";

const checkpointer = new MemorySaver();

const accumulate = entrypoint(
  { checkpointer, name: "accumulate" },
  async (n: number, previous?: number) => {
    const prev = previous || 0;
    const total = prev + n;
    // Return the *previous* value to the caller but save the *new* total to the checkpoint.
    return entrypoint.final({ value: prev, save: total });
  }
);

const config = { configurable: { thread_id: "my-thread" } };

console.log(await accumulate.invoke(1, config)); // 0
console.log(await accumulate.invoke(2, config)); // 1
console.log(await accumulate.invoke(3, config)); // 3
```


### Chatbot 예제

Functional API와 @[`InMemorySaver`] checkpointer를 사용한 간단한 chatbot 예제입니다.

봇은 이전 대화를 기억하고 중단한 지점부터 계속할 수 있습니다.



```typescript
import { BaseMessage } from "@langchain/core/messages";
import {
  addMessages,
  entrypoint,
  task,
  MemorySaver,
} from "@langchain/langgraph";
import { ChatAnthropic } from "@langchain/anthropic";

const model = new ChatAnthropic({ model: "claude-sonnet-4-5" });

const callModel = task(
  "callModel",
  async (messages: BaseMessage[]): Promise<BaseMessage> => {
    const response = await model.invoke(messages);
    return response;
  }
);

const checkpointer = new MemorySaver();

const workflow = entrypoint(
  { checkpointer, name: "workflow" },
  async (
    inputs: BaseMessage[],
    previous?: BaseMessage[]
  ): Promise<BaseMessage> => {
    let messages = inputs;
    if (previous) {
      messages = addMessages(previous, inputs);
    }

    const response = await callModel(messages);
    return entrypoint.final({
      value: response,
      save: addMessages(messages, response),
    });
  }
);

const config = { configurable: { thread_id: "1" } };
const inputMessage = { role: "user", content: "hi! I'm bob" };

for await (const chunk of await workflow.stream([inputMessage], {
  ...config,
  streamMode: "values",
})) {
  console.log(chunk.content);
}

const inputMessage2 = { role: "user", content: "what's my name?" };
for await (const chunk of await workflow.stream([inputMessage2], {
  ...config,
  streamMode: "values",
})) {
  console.log(chunk.content);
}
```


## 장기 메모리

[long-term memory](/oss/javascript/concepts/memory#long-term-memory)를 사용하면 서로 다른 **thread id**에서 정보를 저장할 수 있습니다. 이는 한 대화에서 특정 사용자에 대한 정보를 학습하고 다른 대화에서 사용하는 데 유용할 수 있습니다.

## 워크플로우

* Functional API를 사용하여 워크플로우를 구축하는 방법에 대한 더 많은 예제는 [Workflows and agent](/oss/javascript/langgraph/workflows-agents) 가이드를 참조하세요.

## 다른 라이브러리와 통합하기

* [functional API를 사용하여 다른 프레임워크에 LangGraph 기능 추가하기](/langsmith/autogen-integration): 기본적으로 제공하지 않는 다른 agent 프레임워크에 persistence, memory, streaming과 같은 LangGraph 기능을 추가합니다.

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-functional-api.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
