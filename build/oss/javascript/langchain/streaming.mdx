---
title: Streaming
---



LangChain은 실시간 업데이트를 제공하기 위한 streaming 시스템을 구현합니다.

Streaming은 LLM 기반 애플리케이션의 응답성을 향상시키는 데 매우 중요합니다. 완전한 응답이 준비되기 전에도 출력을 점진적으로 표시함으로써, streaming은 특히 LLM의 지연 시간을 처리할 때 사용자 경험(UX)을 크게 개선합니다.

## Overview

LangChain의 streaming 시스템을 사용하면 agent 실행의 실시간 피드백을 애플리케이션에 전달할 수 있습니다.

LangChain streaming으로 가능한 것들:

* <Icon icon="brain" size={16} /> [**Stream agent progress**](#agent-progress) — 각 agent 단계 후 상태 업데이트를 받습니다.
* <Icon icon="square-binary" size={16} /> [**Stream LLM tokens**](#llm-tokens) — 생성되는 language model token을 streaming합니다.
* <Icon icon="table" size={16} /> [**Stream custom updates**](#custom-updates) — 사용자 정의 신호를 전송합니다 (예: `"Fetched 10/100 records"`).
* <Icon icon="layer-plus" size={16} /> [**Stream multiple modes**](#stream-multiple-modes) — `updates` (agent progress), `messages` (LLM tokens + metadata), 또는 `custom` (임의의 사용자 데이터) 중에서 선택합니다.

## Agent progress



Agent progress를 streaming하려면 `streamMode: "updates"`와 함께 [`stream`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.CompiledStateGraph.html#stream) 메서드를 사용하세요. 이는 모든 agent 단계 후에 이벤트를 전송합니다.


예를 들어, tool을 한 번 호출하는 agent가 있다면 다음과 같은 업데이트를 볼 수 있습니다:

* **LLM node**: tool call 요청이 포함된 [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html)
* **Tool node**: 실행 결과가 포함된 @[`ToolMessage`]
* **LLM node**: 최종 AI 응답



```typescript
import z from "zod";
import { createAgent, tool } from "langchain";

const getWeather = tool(
    async ({ city }) => {
        return `The weather in ${city} is always sunny!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string(),
        }),
    }
);

const agent = createAgent({
    model: "openai:gpt-5-nano",
    tools: [getWeather],
});

for await (const chunk of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: "updates" }
)) {
    const [step, content] = Object.entries(chunk)[0];
    console.log(`step: ${step}`);
    console.log(`content: ${JSON.stringify(content, null, 2)}`);
}
/**
 * step: model
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         // ...
 *         "tool_calls": [
 *           {
 *             "name": "get_weather",
 *             "args": {
 *               "city": "San Francisco"
 *             },
 *             "type": "tool_call",
 *             "id": "call_0qLS2Jp3MCmaKJ5MAYtr4jJd"
 *           }
 *         ],
 *         // ...
 *       }
 *     }
 *   ]
 * }
 * step: tools
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         "content": "The weather in San Francisco is always sunny!",
 *         "name": "get_weather",
 *         // ...
 *       }
 *     }
 *   ]
 * }
 * step: model
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         "content": "The latest update says: The weather in San Francisco is always sunny!\n\nIf you'd like real-time details (current temperature, humidity, wind, and today's forecast), I can pull the latest data for you. Want me to fetch that?",
 *         // ...
 *       }
 *     }
 *   ]
 * }
 */
```


## LLM tokens



LLM에서 생성되는 token을 streaming하려면 `streamMode: "messages"`를 사용하세요:

```typescript
import z from "zod";
import { createAgent, tool } from "langchain";

const getWeather = tool(
    async ({ city }) => {
        return `The weather in ${city} is always sunny!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string(),
        }),
    }
);

const agent = createAgent({
    model: "openai:gpt-4o-mini",
    tools: [getWeather],
});

for await (const [token, metadata] of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: "messages" }
)) {
    console.log(`node: ${metadata.langgraph_node}`);
    console.log(`content: ${JSON.stringify(token.contentBlocks, null, 2)}`);
}
```


## Custom updates



Tool이 실행될 때 업데이트를 streaming하려면 configuration의 `writer` parameter를 사용할 수 있습니다.

```typescript
import z from "zod";
import { tool, createAgent } from "langchain";
import { LangGraphRunnableConfig } from "@langchain/langgraph";

const getWeather = tool(
    async (input, config: LangGraphRunnableConfig) => {
        // Stream any arbitrary data
        config.writer?.(`Looking up data for city: ${input.city}`);
        // ... fetch city data
        config.writer?.(`Acquired data for city: ${input.city}`);
        return `It's always sunny in ${input.city}!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string().describe("The city to get weather for."),
        }),
    }
);

const agent = createAgent({
    model: "openai:gpt-4o-mini",
    tools: [getWeather],
});

for await (const chunk of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: "custom" }
)) {
    console.log(chunk);
}
```

```shell title="Output"
Looking up data for city: San Francisco
Acquired data for city: San Francisco
```

<Note>
    Tool에 `writer` parameter를 추가하면 writer function을 제공하지 않고는 LangGraph 실행 컨텍스트 외부에서 tool을 호출할 수 없습니다.
</Note>


## Stream multiple modes



streamMode를 array로 전달하여 여러 streaming mode를 지정할 수 있습니다: `streamMode: ["updates", "messages", "custom"]`:

```typescript
import z from "zod";
import { tool, createAgent } from "langchain";
import { LangGraphRunnableConfig } from "@langchain/langgraph";

const getWeather = tool(
    async (input, config: LangGraphRunnableConfig) => {
        // Stream any arbitrary data
        config.writer?.(`Looking up data for city: ${input.city}`);
        // ... fetch city data
        config.writer?.(`Acquired data for city: ${input.city}`);
        return `It's always sunny in ${input.city}!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string().describe("The city to get weather for."),
        }),
    }
);

const agent = createAgent({
    model: "openai:gpt-4o-mini",
    tools: [getWeather],
});

for await (const [streamMode, chunk] of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: ["updates", "messages", "custom"] }
)) {
    console.log(`${streamMode}: ${JSON.stringify(chunk, null, 2)}`);
}
```


## Disable streaming

일부 애플리케이션에서는 특정 model에 대해 개별 token의 streaming을 비활성화해야 할 수 있습니다.

이는 [multi-agent](/oss/javascript/langchain/multi-agent) 시스템에서 어떤 agent가 출력을 streaming할지 제어하는 데 유용합니다.

Streaming을 비활성화하는 방법을 알아보려면 [Models](/oss/javascript/langchain/models#disable-streaming) 가이드를 참조하세요.

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/streaming.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
