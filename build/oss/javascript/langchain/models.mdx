---
title: Models
---

import ChatModelTabsPy from '/snippets/chat-model-tabs.mdx';
import ChatModelTabsJS from '/snippets/chat-model-tabs-js.mdx';

[LLM](https://en.wikipedia.org/wiki/Large_language_model)은 인간처럼 텍스트를 해석하고 생성할 수 있는 강력한 AI 도구입니다. 각 작업에 대한 전문적인 훈련 없이도 콘텐츠 작성, 언어 번역, 요약, 질문 답변 등 다양한 작업을 수행할 수 있을 만큼 다재다능합니다.

텍스트 생성 외에도 많은 model들이 다음을 지원합니다:

* <Icon icon="hammer" size={16} /> [Tool calling](#tool-calling) - 외부 tool(데이터베이스 쿼리나 API 호출 등)을 호출하고 그 결과를 응답에 사용합니다.
* <Icon icon="shapes" size={16} /> [Structured output](#structured-outputs) - model의 응답이 정의된 형식을 따르도록 제약합니다.
* <Icon icon="image" size={16} /> [Multimodality](#multimodal) - 텍스트 외에 이미지, 오디오, 비디오와 같은 데이터를 처리하고 반환합니다.
* <Icon icon="brain" size={16} /> [Reasoning](#reasoning) - model이 결론에 도달하기 위해 다단계 추론을 수행합니다.

Model은 [agent](/oss/javascript/langchain/agents)의 추론 엔진입니다. agent의 의사결정 프로세스를 주도하여 어떤 tool을 호출할지, 결과를 어떻게 해석할지, 언제 최종 답변을 제공할지를 결정합니다.

선택한 model의 품질과 기능은 agent의 신뢰성과 성능에 직접적인 영향을 미칩니다. 서로 다른 model들은 서로 다른 작업에 뛰어납니다 - 일부는 복잡한 지시사항을 따르는 데 더 좋고, 다른 일부는 구조화된 추론에 더 좋으며, 일부는 더 많은 정보를 처리하기 위한 더 큰 context window를 지원합니다.

LangChain의 표준 model interface는 다양한 provider 통합에 대한 액세스를 제공하므로, 사용 사례에 가장 적합한 model을 쉽게 실험하고 전환할 수 있습니다.

<Info>
    provider별 통합 정보 및 기능은 provider의 [통합 페이지](/oss/javascript/integrations/providers/overview)를 참조하세요.
</Info>

## 기본 사용법

Model은 두 가지 방식으로 활용할 수 있습니다:

1. **Agent와 함께** - [Agent](/oss/javascript/langchain/agents#model)를 생성할 때 model을 동적으로 지정할 수 있습니다.
2. **독립적으로** - Agent 프레임워크 없이 텍스트 생성, 분류 또는 추출과 같은 작업을 위해 model을 직접 호출할 수 있습니다(agent 루프 외부).

동일한 model interface가 두 컨텍스트 모두에서 작동하므로, 간단하게 시작하여 필요에 따라 더 복잡한 agent 기반 워크플로우로 확장할 수 있는 유연성을 제공합니다.

### Model 초기화


LangChain에서 독립적인 model을 시작하는 가장 쉬운 방법은 `initChatModel`을 사용하여 선택한 [provider](/oss/javascript/integrations/providers/overview)에서 초기화하는 것입니다(아래 예제):

<ChatModelTabsJS />
```typescript
const response = await model.invoke("Why do parrots talk?");
```
model [parameter](#parameters)를 전달하는 방법을 포함한 자세한 내용은 [`initChatModel`](https://v03.api.js.langchain.com/functions/langchain.chat_models_universal.initChatModel.html)을 참조하세요.



### 주요 메서드

<Card title="Invoke" href="#invoke" icon="paper-plane" arrow="true" horizontal>
    Model은 message를 입력으로 받아 완전한 응답을 생성한 후 message를 출력합니다.
</Card>
<Card title="Stream" href="#stream" icon="tower-broadcast" arrow="true" horizontal>
    Model을 호출하되, 출력이 실시간으로 생성되는 동안 스트리밍합니다.
</Card>
<Card title="Batch" href="#batch" icon="grip" arrow="true" horizontal>
    더 효율적인 처리를 위해 여러 요청을 batch로 model에 전송합니다.
</Card>

<Info>
    Chat model 외에도 LangChain은 embedding model 및 vector store와 같은 다른 인접 기술에 대한 지원을 제공합니다. 자세한 내용은 [통합 페이지](/oss/javascript/integrations/providers/overview)를 참조하세요.
</Info>

## Parameter

Chat model은 동작을 구성하는 데 사용할 수 있는 parameter를 받습니다. 지원되는 전체 parameter 세트는 model 및 provider에 따라 다르지만 표준 parameter는 다음과 같습니다:

<ParamField body="model" type="string" required>
    Provider와 함께 사용하려는 특정 model의 이름 또는 식별자입니다.
</ParamField>


<ParamField body="apiKey" type="string">
    Model의 provider로 인증하는 데 필요한 키입니다. 일반적으로 model에 대한 액세스를 신청할 때 발급됩니다. 종종 <Tooltip tip="프로그램 외부에서 값이 설정되는 변수로, 일반적으로 운영 체제 또는 마이크로서비스에 내장된 기능을 통해 설정됩니다.">환경 변수</Tooltip>를 설정하여 액세스합니다.
</ParamField>


<ParamField body="temperature" type="number">
    Model 출력의 무작위성을 제어합니다. 높은 값은 응답을 더 창의적으로 만들고, 낮은 값은 더 결정론적으로 만듭니다.
</ParamField>

<ParamField body="timeout" type="number">
    요청을 취소하기 전에 model의 응답을 기다리는 최대 시간(초)입니다.
</ParamField>


<ParamField body="maxTokens" type="number">
    응답의 총 <Tooltip tip="Model이 읽고 생성하는 기본 단위입니다. Provider마다 다르게 정의할 수 있지만 일반적으로 단어의 전체 또는 일부를 나타낼 수 있습니다.">token</Tooltip> 수를 제한하여 출력 길이를 효과적으로 제어합니다.
</ParamField>

<ParamField body="maxRetries" type="number">
    네트워크 타임아웃이나 rate limit과 같은 문제로 인해 요청이 실패할 경우 시스템이 요청을 재전송하는 최대 시도 횟수입니다.
</ParamField>



`initChatModel`을 사용하여 이러한 parameter를 인라인 parameter로 전달합니다:

```typescript Initialize using model parameters
const model = await initChatModel(
    "anthropic:claude-sonnet-4-5",
    { temperature: 0.7, timeout: 30, max_tokens: 1000 }
)
```


<Info>
    각 chat model 통합에는 provider별 기능을 제어하는 데 사용되는 추가 parameter가 있을 수 있습니다. 예를 들어 @[`ChatOpenAI`]에는 OpenAI Responses 또는 Completions API 사용 여부를 지정하는 `use_responses_api`가 있습니다.

    특정 chat model이 지원하는 모든 parameter를 찾으려면 [chat model 통합](/oss/javascript/integrations/chat) 페이지를 참조하세요.
</Info>

---

## 호출

출력을 생성하려면 chat model을 호출해야 합니다. 각각 다른 사용 사례에 적합한 세 가지 주요 호출 메서드가 있습니다.

### Invoke

Model을 호출하는 가장 간단한 방법은 단일 message 또는 message 목록과 함께 [`invoke()`](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#invoke)를 사용하는 것입니다.



```typescript Single message
const response = await model.invoke("Why do parrots have colorful feathers?");
console.log(response);
```


대화 기록을 나타내기 위해 message 목록을 model에 제공할 수 있습니다. 각 message에는 model이 대화에서 누가 message를 보냈는지 나타내는 데 사용하는 역할이 있습니다. 역할, 유형 및 내용에 대한 자세한 내용은 [message](/oss/javascript/langchain/messages) 가이드를 참조하세요.



```typescript Object format
const conversation = [
  { role: "system", content: "You are a helpful assistant that translates English to French." },
  { role: "user", content: "Translate: I love programming." },
  { role: "assistant", content: "J'adore la programmation." },
  { role: "user", content: "Translate: I love building applications." },
];

const response = await model.invoke(conversation);
console.log(response);  // AIMessage("J'adore créer des applications.")
```
```typescript Message objects
import { HumanMessage, AIMessage, SystemMessage } from "langchain";

const conversation = [
  new SystemMessage("You are a helpful assistant that translates English to French."),
  new HumanMessage("Translate: I love programming."),
  new AIMessage("J'adore la programmation."),
  new HumanMessage("Translate: I love building applications."),
];

const response = await model.invoke(conversation);
console.log(response);  // AIMessage("J'adore créer des applications.")
```


### Stream

대부분의 model은 생성되는 동안 출력 내용을 스트리밍할 수 있습니다. 출력을 점진적으로 표시함으로써 스트리밍은 특히 긴 응답의 경우 사용자 경험을 크게 향상시킵니다.

[`stream()`](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#stream)을 호출하면 생성되는 출력 chunk를 생성하는 <Tooltip tip="컬렉션의 각 항목에 순서대로 점진적으로 액세스를 제공하는 객체입니다.">iterator</Tooltip>가 반환됩니다. 루프를 사용하여 각 chunk를 실시간으로 처리할 수 있습니다:


<CodeGroup>
    ```typescript Basic text streaming
    const stream = await model.stream("Why do parrots have colorful feathers?");
    for await (const chunk of stream) {
      console.log(chunk.text)
    }
    ```

    ```typescript Stream tool calls, reasoning, and other content
    const stream = await model.stream("What color is the sky?");
    for await (const chunk of stream) {
      for (const block of chunk.contentBlocks) {
        if (block.type === "reasoning") {
          console.log(`Reasoning: ${block.reasoning}`);
        } else if (block.type === "tool_call_chunk") {
          console.log(`Tool call chunk: ${block}`);
        } else if (block.type === "text") {
          console.log(block.text);
        } else {
          ...
        }
      }
    }
    ```
</CodeGroup>


Model이 전체 응답 생성을 완료한 후 단일 [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html)를 반환하는 [`invoke()`](#invoke)와 달리, `stream()`은 각각 출력 텍스트의 일부를 포함하는 여러 [`AIMessageChunk`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessageChunk.html) 객체를 반환합니다. 중요한 점은 스트림의 각 chunk가 합산을 통해 전체 message로 수집되도록 설계되었다는 것입니다:



```typescript Construct AIMessage
let full: AIMessageChunk | null = null;
for await (const chunk of stream) {
  full = full ? full.concat(chunk) : chunk;
  console.log(full.text);
}

// The
// The sky
// The sky is
// The sky is typically
// The sky is typically blue
// ...

console.log(full.contentBlocks);
// [{"type": "text", "text": "The sky is typically blue..."}]
```


결과 message는 [`invoke()`](#invoke)로 생성된 message와 동일하게 처리할 수 있습니다 - 예를 들어 message 기록에 집계하여 대화 컨텍스트로 model에 다시 전달할 수 있습니다.

<Warning>
    스트리밍은 프로그램의 모든 단계가 chunk 스트림을 처리하는 방법을 알고 있는 경우에만 작동합니다. 예를 들어 스트리밍을 지원하지 않는 애플리케이션은 처리하기 전에 전체 출력을 메모리에 저장해야 하는 애플리케이션입니다.
</Warning>

<Accordion title="고급 스트리밍 주제">
    <Accordion title='"자동 스트리밍" chat model'>
        LangChain은 스트리밍 메서드를 명시적으로 호출하지 않는 경우에도 특정 경우에 자동으로 스트리밍 모드를 활성화하여 chat model에서의 스트리밍을 단순화합니다. 이는 비스트리밍 invoke 메서드를 사용하지만 chat model의 중간 결과를 포함하여 전체 애플리케이션을 스트리밍하려는 경우 특히 유용합니다.

        예를 들어 [LangGraph agent](/oss/javascript/langchain/agents)에서는 node 내에서 `model.invoke()`를 호출할 수 있지만, 스트리밍 모드에서 실행 중인 경우 LangChain이 자동으로 스트리밍으로 위임합니다.

        #### 작동 방식

        Chat model을 `invoke()`할 때, 전체 애플리케이션을 스트리밍하려고 한다는 것을 감지하면 LangChain이 자동으로 내부 스트리밍 모드로 전환합니다. 호출 결과는 invoke를 사용하는 코드에 관한 한 동일하지만, chat model이 스트리밍되는 동안 LangChain은 LangChain의 callback 시스템에서 @[`on_llm_new_token`] 이벤트를 호출합니다.


        Callback 이벤트를 통해 LangGraph `stream()` 및 [`streamEvents()`](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#streamEvents)가 chat model의 출력을 실시간으로 표시할 수 있습니다.

    </Accordion>
    <Accordion title="스트리밍 이벤트">


        LangChain chat model은 [`streamEvents()`][BaseChatModel.streamEvents]를 사용하여 의미론적 이벤트를 스트리밍할 수도 있습니다.

        이를 통해 이벤트 유형 및 기타 메타데이터를 기반으로 필터링을 단순화하고 백그라운드에서 전체 message를 집계합니다. 예제는 아래를 참조하세요.

        ```typescript
        const stream = await model.streamEvents("Hello");
        for await (const event of stream) {
            if (event.event === "on_chat_model_start") {
                console.log(`Input: ${event.data.input}`);
            }
            if (event.event === "on_chat_model_stream") {
                console.log(`Token: ${event.data.chunk.text}`);
            }
            if (event.event === "on_chat_model_end") {
                console.log(`Full message: ${event.data.output.text}`);
            }
        }
        ```
        ```txt
        Input: Hello
        Token: Hi
        Token:  there
        Token: !
        Token:  How
        Token:  can
        Token:  I
        ...
        Full message: Hi there! How can I help today?
        ```

        이벤트 유형 및 기타 세부 정보는 [`streamEvents()`](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#streamEvents) 참조를 확인하세요.

    </Accordion>
</Accordion>

### Batch

독립적인 요청 모음을 model에 batch 처리하면 처리를 병렬로 수행할 수 있으므로 성능이 크게 향상되고 비용이 절감될 수 있습니다:



```typescript Batch
const responses = await model.batch([
  "Why do parrots have colorful feathers?",
  "How do airplanes fly?",
  "What is quantum computing?",
  "Why do parrots have colorful feathers?",
  "How do airplanes fly?",
  "What is quantum computing?",
]);
for (const response of responses) {
  console.log(response);
}
```

<Tip>
    `batch()`를 사용하여 많은 수의 입력을 처리할 때 최대 병렬 호출 수를 제어할 수 있습니다. 이는 [`RunnableConfig`](https://v03.api.js.langchain.com/interfaces/_langchain_core.runnables.RunnableConfig.html) dictionary에서 `maxConcurrency` 속성을 설정하여 수행할 수 있습니다.

    ```typescript Batch with max concurrency
    model.batch(
      listOfInputs,
      {
        maxConcurrency: 5,  // Limit to 5 parallel calls
      }
    )
    ```

    지원되는 속성의 전체 목록은 [`RunnableConfig`](https://v03.api.js.langchain.com/interfaces/_langchain_core.runnables.RunnableConfig.html) 참조를 확인하세요.
</Tip>

Batch 처리에 대한 자세한 내용은 [참조](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#batch)를 확인하세요.


---

## Tool calling

Model은 데이터베이스에서 데이터를 가져오거나, 웹을 검색하거나, 코드를 실행하는 등의 작업을 수행하는 tool을 호출하도록 요청할 수 있습니다. Tool은 다음의 쌍입니다:

1. Tool의 이름, 설명 및/또는 인수 정의(종종 JSON schema)를 포함한 schema
2. 실행할 function 또는 <Tooltip tip="실행을 일시 중단하고 나중에 재개할 수 있는 메서드">coroutine</Tooltip>

<Note>
    "function calling"이라는 용어를 들을 수 있습니다. 이를 "tool calling"과 같은 의미로 사용합니다.
</Note>



정의한 tool을 model이 사용할 수 있도록 하려면 [`bindTools()`](https://v03.api.js.langchain.com/classes/langchain.chat_models_universal.ConfigurableModel.html#bindTools)를 사용하여 바인딩해야 합니다. 후속 호출에서 model은 필요에 따라 바인딩된 tool 중 하나를 선택하여 호출할 수 있습니다.


일부 model provider는 model 또는 호출 parameter를 통해 활성화할 수 있는 내장 tool을 제공합니다(예: [`ChatOpenAI`](/oss/javascript/integrations/chat/openai), [`ChatAnthropic`](/oss/javascript/integrations/chat/anthropic)). 자세한 내용은 해당 [provider 참조](/oss/javascript/integrations/providers/overview)를 확인하세요.

<Tip>
    Tool 생성에 대한 세부 정보 및 기타 옵션은 [tool 가이드](/oss/javascript/langchain/tools)를 참조하세요.
</Tip>



```typescript Binding user tools
import { tool } from "langchain";
import * as z from "zod";
import { ChatOpenAI } from "@langchain/openai";

const getWeather = tool(
  (input) => `It's sunny in ${input.location}.`,
  {
    name: "get_weather",
    description: "Get the weather at a location.",
    schema: z.object({
      location: z.string().describe("The location to get the weather for"),
    }),
  },
);

const model = new ChatOpenAI({ model: "gpt-4o" });
const modelWithTools = model.bindTools([getWeather]);  // [!code highlight]

const response = await modelWithTools.invoke("What's the weather like in Boston?");
const toolCalls = response.tool_calls || [];
for (const tool_call of toolCalls) {
  // View tool calls made by the model
  console.log(`Tool: ${tool_call.name}`);
  console.log(`Args: ${tool_call.args}`);
}
```


사용자 정의 tool을 바인딩할 때 model의 응답에는 tool 실행 **요청**이 포함됩니다. [Agent](/oss/javascript/langchain/agents)와 별도로 model을 사용할 때는 요청된 작업을 수행하고 결과를 model에 반환하여 후속 추론에 사용하는 것은 사용자의 몫입니다. [Agent](/oss/javascript/langchain/agents)를 사용할 때는 agent 루프가 tool 실행 루프를 처리합니다.

아래에서는 tool calling을 사용할 수 있는 몇 가지 일반적인 방법을 보여줍니다.

<AccordionGroup>
    <Accordion title="Tool 실행 루프" icon="arrow-rotate-right">
        Model이 tool call을 반환하면 tool을 실행하고 결과를 model에 다시 전달해야 합니다. 이렇게 하면 model이 tool 결과를 사용하여 최종 응답을 생성할 수 있는 대화 루프가 생성됩니다. LangChain에는 이 오케스트레이션을 처리하는 [agent](/oss/javascript/langchain/agents) 추상화가 포함되어 있습니다.

        다음은 이를 수행하는 방법의 간단한 예입니다:


        ```typescript Tool execution loop
        // Bind (potentially multiple) tools to the model
        const modelWithTools = model.bindTools([get_weather])

        // Step 1: Model generates tool calls
        const messages = [{"role": "user", "content": "What's the weather in Boston?"}]
        const ai_msg = await modelWithTools.invoke(messages)
        messages.push(ai_msg)

        // Step 2: Execute tools and collect results
        for (const tool_call of ai_msg.tool_calls) {
            // Execute the tool with the generated arguments
            const tool_result = await get_weather.invoke(tool_call)
            messages.push(tool_result)
        }

        // Step 3: Pass results back to model for final response
        const final_response = await modelWithTools.invoke(messages)
        console.log(final_response.text)
        // "The current weather in Boston is 72°F and sunny."
        ```



        Tool이 반환하는 각 @[`ToolMessage`]에는 원래 tool call과 일치하는 `tool_call_id`가 포함되어 있어 model이 결과를 요청과 연관시키는 데 도움이 됩니다.
    </Accordion>
    <Accordion title="Tool call 강제" icon="asterisk">
        기본적으로 model은 사용자의 입력을 기반으로 사용할 바인딩된 tool을 자유롭게 선택할 수 있습니다. 그러나 특정 tool을 선택하도록 강제하거나 주어진 목록에서 **임의의** tool을 사용하도록 보장할 수 있습니다:


        <CodeGroup>
            ```typescript Force use of any tool
            const modelWithTools = model.bindTools([tool_1], { toolChoice: "any" })
            ```
            ```typescript Force use of specific tools
            const modelWithTools = model.bindTools([tool_1], { toolChoice: "tool_1" })
            ```
        </CodeGroup>

    </Accordion>
    <Accordion title="병렬 tool call" icon="layer-group">
        많은 model이 적절한 경우 여러 tool을 병렬로 호출하는 것을 지원합니다. 이를 통해 model이 서로 다른 소스에서 동시에 정보를 수집할 수 있습니다.


        ```typescript Parallel tool calls
        const modelWithTools = model.bind_tools([get_weather])

        const response = await modelWithTools.invoke(
            "What's the weather in Boston and Tokyo?"
        )


        // The model may generate multiple tool calls
        console.log(response.tool_calls)
        // [
        //   { name: 'get_weather', args: { location: 'Boston' }, id: 'call_1' },
        //   { name: 'get_time', args: { location: 'Tokyo' }, id: 'call_2' }
        // ]


        // Execute all tools (can be done in parallel with async)
        const results = []
        for (const tool_call of response.tool_calls || []) {
            if (tool_call.name === 'get_weather') {
                const result = await get_weather.invoke(tool_call)
                results.push(result)
            }
        }
        ```



        Model은 요청된 작업의 독립성을 기반으로 병렬 실행이 적절한 시기를 지능적으로 결정합니다.

        <Tip>
        Tool calling을 지원하는 대부분의 model은 기본적으로 병렬 tool call을 활성화합니다. 일부([OpenAI](/oss/javascript/integrations/chat/openai) 및 [Anthropic](/oss/javascript/integrations/chat/anthropic) 포함)는 이 기능을 비활성화할 수 있습니다. 이렇게 하려면 `parallel_tool_calls=False`를 설정하세요:
        ```python
        model.bind_tools([get_weather], parallel_tool_calls=False)
        ```
        </Tip>
    </Accordion>
    <Accordion title="Tool call 스트리밍" icon="rss">
        응답을 스트리밍할 때 tool call은 @[`ToolCallChunk`]를 통해 점진적으로 구축됩니다. 이를 통해 완전한 응답을 기다리지 않고 생성되는 동안 tool call을 볼 수 있습니다.


        ```typescript Streaming tool calls
        const stream = await modelWithTools.stream(
            "What's the weather in Boston and Tokyo?"
        )
        for await (const chunk of stream) {
            // Tool call chunks arrive progressively
            if (chunk.tool_call_chunks) {
                for (const tool_chunk of chunk.tool_call_chunks) {
                console.log(`Tool: ${tool_chunk.get('name', '')}`)
                console.log(`Args: ${tool_chunk.get('args', '')}`)
                }
            }
        }

        // Output:
        // Tool: get_weather
        // Args:
        // Tool:
        // Args: {"loc
        // Tool:
        // Args: ation": "BOS"}
        // Tool: get_time
        // Args:
        // Tool:
        // Args: {"timezone": "Tokyo"}
        ```

        Chunk를 누적하여 완전한 tool call을 구축할 수 있습니다:

        ```typescript Accumulate tool calls
        let full: AIMessageChunk | null = null
        const stream = await modelWithTools.stream("What's the weather in Boston?")
        for await (const chunk of stream) {
            full = full ? full.concat(chunk) : chunk
            console.log(full.contentBlocks)
        }
        ```


    </Accordion>
</AccordionGroup>

---

## Structured output

Model은 주어진 schema와 일치하는 형식으로 응답을 제공하도록 요청할 수 있습니다. 이는 출력을 쉽게 구문 분석하고 후속 처리에 사용할 수 있도록 하는 데 유용합니다. LangChain은 구조화된 출력을 적용하기 위한 여러 schema 유형과 메서드를 지원합니다.



<Tabs>
    <Tab title="Zod">
        [Zod schema](https://zod.dev/)는 출력 schema를 정의하는 선호되는 방법입니다. Zod schema가 제공되면 model 출력도 zod의 parse 메서드를 사용하여 schema에 대해 검증됩니다.

        ```typescript
        import * as z from "zod";

        const Movie = z.object({
          title: z.string().describe("The title of the movie"),
          year: z.number().describe("The year the movie was released"),
          director: z.string().describe("The director of the movie"),
          rating: z.number().describe("The movie's rating out of 10"),
        });

        const modelWithStructure = model.withStructuredOutput(Movie);

        const response = await modelWithStructure.invoke("Provide details about the movie Inception");
        console.log(response);
        // {
        //   title: "Inception",
        //   year: 2010,
        //   director: "Christopher Nolan",
        //   rating: 8.8,
        // }
        ```
    </Tab>
    <Tab title="JSON Schema">
        최대한의 제어 또는 상호 운용성을 위해 원시 JSON Schema를 제공할 수 있습니다.

        ```typescript
        const jsonSchema = {
          "title": "Movie",
          "description": "A movie with details",
          "type": "object",
          "properties": {
            "title": {
              "type": "string",
              "description": "The title of the movie",
            },
            "year": {
              "type": "integer",
              "description": "The year the movie was released",
            },
            "director": {
              "type": "string",
              "description": "The director of the movie",
            },
            "rating": {
              "type": "number",
              "description": "The movie's rating out of 10",
            },
          },
          "required": ["title", "year", "director", "rating"],
        }

        const modelWithStructure = model.withStructuredOutput(
          jsonSchema,
          { method: "jsonSchema" },
        )

        const response = await modelWithStructure.invoke("Provide details about the movie Inception")
        console.log(response)  // {'title': 'Inception', 'year': 2010, ...}
        ```
    </Tab>
</Tabs>




<Note>
    **구조화된 출력에 대한 주요 고려 사항:**

    - **Method parameter**: 일부 provider는 다양한 메서드(`'jsonSchema'`, `'functionCalling'`, `'jsonMode'`)를 지원합니다
    - **Include raw**: @[`includeRaw: true`][BaseChatModel.with_structured_output(include_raw)]를 사용하여 구문 분석된 출력과 원시 [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html)를 모두 가져옵니다
    - **Validation**: Zod model은 자동 검증을 제공하는 반면 JSON Schema는 수동 검증이 필요합니다
</Note>


<Accordion title="예제: 구문 분석된 구조와 함께 Message 출력">

[Token 수](#token-usage)와 같은 응답 메타데이터에 액세스하기 위해 구문 분석된 표현과 함께 원시 [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html) 객체를 반환하는 것이 유용할 수 있습니다. 이렇게 하려면 @[`with_structured_output`][BaseChatModel.with_structured_output]을 호출할 때 @[`include_raw=True`][BaseChatModel.with_structured_output(include_raw)]를 설정하세요:



    ```typescript
    import * as z from "zod";

    const Movie = z.object({
      title: z.string().describe("The title of the movie"),
      year: z.number().describe("The year the movie was released"),
      director: z.string().describe("The director of the movie"),
      rating: z.number().describe("The movie's rating out of 10"),
      title: z.string().describe("The title of the movie"),
      year: z.number().describe("The year the movie was released"),
      director: z.string().describe("The director of the movie"),  // [!code highlight]
      rating: z.number().describe("The movie's rating out of 10"),
    });

    const modelWithStructure = model.withStructuredOutput(Movie, { includeRaw: true });

    const response = await modelWithStructure.invoke("Provide details about the movie Inception");
    console.log(response);
    // {
    //   raw: AIMessage { ... },
    //   parsed: { title: "Inception", ... }
    // }
    ```

</Accordion>
<Accordion title="예제: 중첩 구조">
    Schema는 중첩될 수 있습니다:


    ```typescript
    import * as z from "zod";

    const Actor = z.object({
      name: str
      role: z.string(),
    });

    const MovieDetails = z.object({
      title: z.string(),
      year: z.number(),
      cast: z.array(Actor),
      genres: z.array(z.string()),
      budget: z.number().nullable().describe("Budget in millions USD"),
    });

    const modelWithStructure = model.withStructuredOutput(MovieDetails);
    ```

</Accordion>

---

## 지원되는 model

LangChain은 OpenAI, Anthropic, Google, Azure, AWS Bedrock 등을 포함한 모든 주요 model provider를 지원합니다. 각 provider는 다양한 기능을 가진 다양한 model을 제공합니다. LangChain에서 지원되는 model의 전체 목록은 [통합 페이지](/oss/javascript/integrations/providers/overview)를 참조하세요.

---

## 고급 주제

### Multimodal

특정 model은 이미지, 오디오, 비디오와 같은 비텍스트 데이터를 처리하고 반환할 수 있습니다. [Content block](/oss/javascript/langchain/messages#message-content)을 제공하여 비텍스트 데이터를 model에 전달할 수 있습니다.

<Tip>
    기본 multimodal 기능을 갖춘 모든 LangChain chat model은 다음을 지원합니다:

    1. 교차 provider 표준 형식의 데이터([message 가이드](/oss/javascript/langchain/messages) 참조)
    2. OpenAI [chat completion](https://platform.openai.com/docs/api-reference/chat) 형식
    3. 해당 특정 provider의 기본 형식(예: Anthropic model은 Anthropic 기본 형식을 허용)
</Tip>

자세한 내용은 message 가이드의 [multimodal 섹션](/oss/javascript/langchain/messages#multimodal)을 참조하세요.

<Tooltip tip="모든 LLM이 동등하게 만들어지지는 않습니다!" cta="참조 보기" href="https://models.dev/">일부 model</Tooltip>은 응답의 일부로 multimodal 데이터를 반환할 수 있습니다. 그렇게 호출되면 결과 [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html)에는 multimodal 유형의 content block이 포함됩니다.



```typescript Multimodal output
const response = await model.invoke("Create a picture of a cat");
console.log(response.contentBlocks);
// [
//   { type: "text", text: "Here's a picture of a cat" },
//   { type: "image", data: "...", mimeType: "image/jpeg" },
// ]
```


특정 provider에 대한 자세한 내용은 [통합 페이지](/oss/javascript/integrations/providers/overview)를 참조하세요.

### Reasoning

최신 model은 결론에 도달하기 위해 다단계 추론을 수행할 수 있습니다. 여기에는 복잡한 문제를 더 작고 관리하기 쉬운 단계로 나누는 것이 포함됩니다.

**기본 model이 지원하는 경우,** 이 추론 프로세스를 표시하여 model이 최종 답변에 어떻게 도달했는지 더 잘 이해할 수 있습니다.



<CodeGroup>
    ```typescript Stream reasoning output
    const stream = model.stream("Why do parrots have colorful feathers?");
    for await (const chunk of stream) {
        const reasoningSteps = chunk.contentBlocks.filter(b => b.type === "reasoning");
        console.log(reasoningSteps.length > 0 ? reasoningSteps : chunk.text);
    }
    ```

    ```typescript Complete reasoning output
    const response = await model.invoke("Why do parrots have colorful feathers?");
    const reasoningSteps = response.contentBlocks.filter(b => b.type === "reasoning");
    console.log(reasoningSteps.map(step => step.reasoning).join(" "));
    ```
</CodeGroup>


Model에 따라 추론에 투입해야 하는 노력 수준을 지정할 수 있는 경우가 있습니다. 마찬가지로 model이 추론을 완전히 끄도록 요청할 수 있습니다. 이는 추론의 범주형 "계층"(예: `'low'` 또는 `'high'`) 또는 정수 token 예산의 형태를 취할 수 있습니다.

자세한 내용은 해당 chat model의 [통합 페이지](/oss/javascript/integrations/providers/overview) 또는 [참조](https://reference.langchain.com/python/integrations/)를 참조하세요.


### 로컬 model

LangChain은 자체 하드웨어에서 로컬로 model을 실행하는 것을 지원합니다. 이는 데이터 프라이버시가 중요하거나, 사용자 정의 model을 호출하려는 경우, 또는 클라우드 기반 model을 사용할 때 발생하는 비용을 피하려는 시나리오에 유용합니다.

[Ollama](/oss/javascript/integrations/chat/ollama)는 로컬에서 model을 실행하는 가장 쉬운 방법 중 하나입니다. [통합 페이지](/oss/javascript/integrations/providers/overview)에서 로컬 통합의 전체 목록을 참조하세요.

### Prompt caching

많은 provider가 동일한 token의 반복 처리에 대한 지연 시간과 비용을 줄이기 위해 prompt caching 기능을 제공합니다. 이러한 기능은 **암시적** 또는 **명시적**일 수 있습니다:

- **암시적 prompt caching:** 요청이 cache에 도달하면 provider가 자동으로 비용 절감을 전달합니다. 예: [OpenAI](/oss/javascript/integrations/chat/openai) 및 [Gemini](/oss/javascript/integrations/chat/google_generative_ai) (Gemini 2.5 이상).
- **명시적 caching:** provider가 더 큰 제어 또는 비용 절감을 보장하기 위해 cache 지점을 수동으로 표시할 수 있습니다. 예: @[`ChatOpenAI`] (`prompt_cache_key`를 통해), Anthropic의 [`AnthropicPromptCachingMiddleware`](/oss/javascript/integrations/chat/anthropic#prompt-caching) 및 [`cache_control`](https://docs.langchain.com/oss/python/integrations/chat/anthropic#prompt-caching) 옵션, [AWS Bedrock](/oss/javascript/integrations/chat/bedrock#prompt-caching), [Gemini](https://python.langchain.com/api_reference/google_genai/chat_models/langchain_google_genai.chat_models.ChatGoogleGenerativeAI.html).

<Warning>
    Prompt caching은 종종 최소 입력 token 임계값 이상에서만 활성화됩니다. 자세한 내용은 [provider 페이지](/oss/javascript/integrations/chat)를 참조하세요.
</Warning>

Cache 사용은 model 응답의 [사용 메타데이터](/oss/javascript/langchain/messages#token-usage)에 반영됩니다.

### 서버 측 tool 사용

일부 provider는 서버 측 [tool-calling](#tool-calling) 루프를 지원합니다: model이 웹 검색, 코드 인터프리터 및 기타 tool과 상호 작용하고 단일 대화 턴에서 결과를 분석할 수 있습니다.

Model이 서버 측에서 tool을 호출하면 응답 message의 내용에 tool의 호출 및 결과를 나타내는 내용이 포함됩니다. 응답의 [content block](/oss/javascript/langchain/messages#standard-content-blocks)에 액세스하면 provider에 구애받지 않는 형식으로 서버 측 tool call 및 결과가 반환됩니다:


```typescript
import { initChatModel } from "langchain";

const model = await initChatModel("openai:gpt-4.1-mini");
const modelWithTools = model.bindTools([{ type: "web_search" }])

const message = await modelWithTools.invoke("What was a positive news story from today?");
console.log(message.contentBlocks);
```

이는 단일 대화 턴을 나타냅니다. 클라이언트 측 [tool-calling](#tool-calling)에서처럼 전달해야 하는 관련 [ToolMessage](/oss/javascript/langchain/messages#tool-message) 객체가 없습니다.

사용 가능한 tool 및 사용 세부 정보는 해당 provider의 [통합 페이지](/oss/javascript/integrations/chat)를 참조하세요.



### Base URL 또는 proxy

많은 chat model 통합의 경우 API 요청에 대한 base URL을 구성할 수 있으므로 OpenAI 호환 API가 있는 model provider를 사용하거나 proxy 서버를 사용할 수 있습니다.

<Accordion title="Base URL" icon="link">


    많은 model provider가 OpenAI 호환 API를 제공합니다(예: [Together AI](https://www.together.ai/), [vLLM](https://github.com/vllm-project/vllm)). 적절한 `base_url` parameter를 지정하여 이러한 provider와 함께 `initChatModel`을 사용할 수 있습니다:

    ```python
    model = initChatModel(
        "MODEL_NAME",
        {
            modelProvider: "openai",
            baseUrl: "BASE_URL",
            apiKey: "YOUR_API_KEY",
        }
    )
    ```


    <Note>
        직접 chat model 클래스 인스턴스화를 사용할 때 parameter 이름은 provider에 따라 다를 수 있습니다. 자세한 내용은 해당 [참조](/oss/javascript/integrations/providers/overview)를 확인하세요.
    </Note>
</Accordion>




### Log probability

특정 model은 model을 초기화할 때 `logprobs` parameter를 설정하여 주어진 token의 가능성을 나타내는 token 수준 log probability를 반환하도록 구성할 수 있습니다:



```typescript
const model = new ChatOpenAI({
    model: "gpt-4o",
    logprobs: true,
});

const responseMessage = await model.invoke("Why do parrots talk?");

responseMessage.response_metadata.logprobs.content.slice(0, 5);
```


### Token 사용량

많은 model provider가 호출 응답의 일부로 token 사용 정보를 반환합니다. 사용 가능한 경우 이 정보는 해당 model이 생성한 [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html) 객체에 포함됩니다. 자세한 내용은 [message](/oss/javascript/langchain/messages) 가이드를 참조하세요.

<Note>
    일부 provider API, 특히 OpenAI 및 Azure OpenAI chat completion은 스트리밍 컨텍스트에서 token 사용 데이터를 수신하기 위해 사용자가 옵트인해야 합니다. 자세한 내용은 통합 가이드의 [스트리밍 사용 메타데이터](/oss/javascript/integrations/chat/openai#streaming-usage-metadata) 섹션을 참조하세요.
</Note>



### 호출 구성



Model을 호출할 때 [`RunnableConfig`](https://v03.api.js.langchain.com/interfaces/_langchain_core.runnables.RunnableConfig.html) 객체를 사용하여 `config` parameter를 통해 추가 구성을 전달할 수 있습니다. 이는 실행 동작, callback 및 메타데이터 추적에 대한 런타임 제어를 제공합니다.


일반적인 구성 옵션은 다음과 같습니다:



```typescript Invocation with config
const response = await model.invoke(
    "Tell me a joke",
    {
        runName: "joke_generation",      // Custom name for this run
        tags: ["humor", "demo"],          // Tags for categorization
        metadata: {"user_id": "123"},     // Custom metadata
        callbacks: [my_callback_handler], // Callback handlers
    }
)
```


이러한 구성 값은 다음과 같은 경우에 특히 유용합니다:
- [LangSmith](https://docs.smith.langchain.com/) tracing으로 디버깅
- 사용자 정의 로깅 또는 모니터링 구현
- 프로덕션에서 리소스 사용 제어
- 복잡한 pipeline 전체에서 호출 추적



<Accordion title="주요 구성 속성">
    <ParamField body="runName" type="string">
        로그 및 trace에서 이 특정 호출을 식별합니다. 하위 호출에 상속되지 않습니다.
    </ParamField>

    <ParamField body="tags" type="string[]">
        디버깅 도구에서 필터링 및 구성을 위해 모든 하위 호출에 상속되는 레이블입니다.
    </ParamField>

    <ParamField body="metadata" type="object">
        추가 컨텍스트를 추적하기 위한 사용자 정의 키-값 쌍으로, 모든 하위 호출에 상속됩니다.
    </ParamField>

    <ParamField body="maxConcurrency" type="number">
        `batch()`를 사용할 때 최대 병렬 호출 수를 제어합니다.
    </ParamField>

    <ParamField body="callbacks" type="CallbackHandler[]">
        실행 중 이벤트를 모니터링하고 응답하기 위한 handler입니다.
    </ParamField>

    <ParamField body="recursion_limit" type="number">
        복잡한 pipeline에서 무한 루프를 방지하기 위한 chain의 최대 재귀 깊이입니다.
    </ParamField>
</Accordion>


<Tip>
    지원되는 모든 속성은 [`RunnableConfig`](https://v03.api.js.langchain.com/interfaces/_langchain_core.runnables.RunnableConfig.html) 참조를 확인하세요.
</Tip>

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/models.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
