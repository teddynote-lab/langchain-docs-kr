---
title: LangGraph로 사고하기
description: 고객 지원 이메일 에이전트를 개별 단계로 분해하여 LangGraph로 에이전트를 구축하는 사고 방식을 배워보세요
---



LangGraph는 여러분이 구축하는 에이전트에 대한 사고 방식을 바꿀 수 있습니다. LangGraph로 에이전트를 구축할 때, 먼저 **노드**라고 하는 개별 단계로 분해합니다. 그런 다음 각 노드에 대한 다양한 결정과 전환을 설명합니다. 마지막으로 각 노드가 읽고 쓸 수 있는 공유 **상태**를 통해 노드들을 연결합니다. 이 튜토리얼에서는 LangGraph로 고객 지원 이메일 에이전트를 구축하는 사고 과정을 안내합니다.

## 자동화하려는 프로세스부터 시작하기

고객 지원 이메일을 처리하는 AI 에이전트를 구축해야 한다고 가정해 봅시다. 제품 팀이 다음과 같은 요구사항을 제시했습니다:

에이전트는 다음을 수행해야 합니다:

- 수신된 고객 이메일 읽기
- 긴급도와 주제별로 분류
- 질문에 답변하기 위해 관련 문서 검색
- 적절한 응답 초안 작성
- 복잡한 문제를 담당자에게 에스컬레이션
- 필요시 후속 조치 예약

처리해야 할 예시 시나리오:

1. 간단한 제품 질문: "비밀번호를 재설정하려면 어떻게 해야 하나요?"
2. 버그 리포트: "PDF 형식을 선택하면 내보내기 기능이 충돌합니다"
3. 긴급 결제 문제: "구독료가 두 번 청구되었습니다!"
4. 기능 요청: "모바일 앱에 다크 모드를 추가할 수 있나요?"
5. 복잡한 기술 문제: "API 통합이 504 오류와 함께 간헐적으로 실패합니다"

LangGraph에서 에이전트를 구현하려면 일반적으로 동일한 다섯 단계를 따릅니다.

## 1단계: 워크플로우를 개별 단계로 매핑하기

프로세스의 개별 단계를 식별하는 것부터 시작합니다. 각 단계는 **노드**(특정 작업을 수행하는 함수)가 됩니다. 그런 다음 이러한 단계들이 서로 어떻게 연결되는지 스케치합니다.

```mermaid
flowchart TD
    A[START] --> B[Read Email]
    B --> C[Classify Intent]

    C -.-> D[Doc Search]
    C -.-> E[Bug Track]
    C -.-> F[Human Review]

    D --> G[Draft Reply]
    E --> G
    F --> G

    G -.-> H[Human Review]
    G -.-> I[Send Reply]

    H --> J[END]
    I --> J[END]
```

화살표는 가능한 경로를 보여주지만, 어떤 경로를 선택할지에 대한 실제 결정은 각 노드 내부에서 발생합니다.

이제 워크플로우의 구성 요소를 식별했으므로 각 노드가 수행해야 하는 작업을 이해해 봅시다:

- Read Email: 이메일 내용 추출 및 파싱
- Classify Intent: LLM을 사용하여 긴급도와 주제를 분류한 다음 적절한 작업으로 라우팅
- Doc Search: 관련 정보를 위해 지식 베이스 쿼리
- Bug Track: 추적 시스템에서 이슈 생성 또는 업데이트
- Draft Reply: 적절한 응답 생성
- Human Review: 승인 또는 처리를 위해 담당자에게 에스컬레이션
- Send Reply: 이메일 응답 발송

<Tip>
일부 노드는 다음에 갈 위치에 대한 결정을 내리고(Classify Intent, Draft Reply, Human Review), 다른 노드는 항상 동일한 다음 단계로 진행합니다(Read Email은 항상 Classify Intent로, Doc Search는 항상 Draft Reply로 이동).
</Tip>

## 2단계: 각 단계가 수행해야 하는 작업 식별하기

그래프의 각 노드에 대해 어떤 유형의 작업을 나타내는지, 제대로 작동하기 위해 어떤 컨텍스트가 필요한지 결정합니다.

<CardGroup cols={2}>
    <Card title="LLM 단계" icon="brain" href="#llm-steps">
        텍스트를 이해, 분석, 생성하거나 추론 결정을 내려야 할 때 사용
    </Card>
    <Card title="데이터 단계" icon="database" href="#data-steps">
        외부 소스에서 정보를 검색해야 할 때 사용
    </Card>
    <Card title="액션 단계" icon="bolt" href="#action-steps">
        외부 작업을 수행해야 할 때 사용
    </Card>
    <Card title="사용자 입력 단계" icon="user" href="#user-input-steps">
        사람의 개입이 필요할 때 사용
    </Card>
</CardGroup>

### LLM 단계

단계에서 텍스트를 이해, 분석, 생성하거나 추론 결정을 내려야 할 때:

<AccordionGroup>
    <Accordion title="Classify Intent 노드">
        - 정적 컨텍스트(프롬프트): 분류 카테고리, 긴급도 정의, 응답 형식
        - 동적 컨텍스트(상태에서): 이메일 내용, 발신자 정보
        - 원하는 결과: 라우팅을 결정하는 구조화된 분류
    </Accordion>

    <Accordion title="Draft Reply 노드">
        - 정적 컨텍스트(프롬프트): 톤 가이드라인, 회사 정책, 응답 템플릿
        - 동적 컨텍스트(상태에서): 분류 결과, 검색 결과, 고객 이력
        - 원하는 결과: 검토 준비가 완료된 전문적인 이메일 응답
    </Accordion>
</AccordionGroup>

### 데이터 단계

단계에서 외부 소스로부터 정보를 검색해야 할 때:

<AccordionGroup>
    <Accordion title="Document Search 노드">
        - 매개변수: 의도와 주제로부터 구축된 쿼리
        - 재시도 전략: 예, 일시적 실패에 대한 지수 백오프 사용
        - 캐싱: API 호출을 줄이기 위해 일반적인 쿼리를 캐시할 수 있음
    </Accordion>

    <Accordion title="Customer History Lookup">
        - 매개변수: 상태에서 가져온 고객 이메일 또는 ID
        - 재시도 전략: 예, 하지만 사용 불가능한 경우 기본 정보로 폴백
        - 캐싱: 예, 신선도와 성능의 균형을 위해 time-to-live 사용
    </Accordion>
</AccordionGroup>

### 액션 단계

단계에서 외부 작업을 수행해야 할 때:

<AccordionGroup>
    <Accordion title="Send Reply 노드">
        - 실행 시점: 승인 후(사람 또는 자동)
        - 재시도 전략: 예, 네트워크 문제에 대한 지수 백오프 사용
        - 캐시하지 않아야 함: 각 발송은 고유한 작업
    </Accordion>

    <Accordion title="Bug Track 노드">
        - 실행 시점: 의도가 "bug"일 때 항상
        - 재시도 전략: 예, 버그 리포트를 잃지 않는 것이 중요
        - 반환값: 응답에 포함할 티켓 ID
    </Accordion>
</AccordionGroup>

### 사용자 입력 단계

단계에서 사람의 개입이 필요할 때:

<AccordionGroup>
    <Accordion title="Human Review 노드">
        - 결정을 위한 컨텍스트: 원본 이메일, 응답 초안, 긴급도, 분류
        - 예상 입력 형식: 승인 boolean과 선택적으로 편집된 응답
        - 트리거 시점: 높은 긴급도, 복잡한 문제 또는 품질 우려
    </Accordion>
</AccordionGroup>

## 3단계: 상태 설계하기

상태는 에이전트의 모든 노드가 접근할 수 있는 공유 [메모리](/oss/python/concepts/memory)입니다. 에이전트가 프로세스를 진행하면서 학습하고 결정하는 모든 것을 추적하는 데 사용하는 노트북으로 생각하세요.

### 상태에 무엇이 속하나요?

각 데이터 조각에 대해 다음 질문을 스스로에게 해보세요:

<CardGroup cols={2}>
    <Card title="상태에 포함" icon="check">
        단계 간에 지속되어야 하나요? 그렇다면 상태에 포함됩니다.
    </Card>

    <Card title="저장하지 않음" icon="code">
        다른 데이터로부터 파생할 수 있나요? 그렇다면 상태에 저장하는 대신 필요할 때 계산합니다.
    </Card>
</CardGroup>

이메일 에이전트의 경우 다음을 추적해야 합니다:

- 원본 이메일과 발신자 정보(재구성할 수 없음)
- 분류 결과(여러 다운스트림 노드에서 필요)
- 검색 결과와 고객 데이터(다시 가져오는 데 비용이 많이 듦)
- 응답 초안(검토를 거치는 동안 지속되어야 함)
- 실행 메타데이터(디버깅 및 복구용)

### 상태는 원시 데이터로 유지하고, 프롬프트는 필요할 때 포맷하기

<Tip>
핵심 원칙: 상태는 원시 데이터를 저장해야 하며, 포맷된 텍스트를 저장하지 않습니다. 필요할 때 노드 내부에서 프롬프트를 포맷합니다.
</Tip>

이러한 분리는 다음을 의미합니다:

- 서로 다른 노드가 필요에 따라 동일한 데이터를 다르게 포맷할 수 있음
- 상태 스키마를 수정하지 않고 프롬프트 템플릿을 변경할 수 있음
- 디버깅이 더 명확함 - 각 노드가 받은 데이터를 정확히 볼 수 있음
- 기존 상태를 손상시키지 않고 에이전트를 발전시킬 수 있음

상태를 정의해 봅시다:

```python
from typing import TypedDict, Literal

# Define the structure for email classification
class EmailClassification(TypedDict):
    intent: Literal["question", "bug", "billing", "feature", "complex"]
    urgency: Literal["low", "medium", "high", "critical"]
    topic: str
    summary: str

class EmailAgentState(TypedDict):
    # Raw email data
    email_content: str
    sender_email: str
    email_id: str

    # Classification result
    classification: EmailClassification | None

    # Raw search/API results
    search_results: list[str] | None  # List of raw document chunks
    customer_history: dict | None  # Raw customer data from CRM

    # Generated content
    draft_response: str | None
    messages: list[str] | None
```





상태에는 원시 데이터만 포함되어 있으며 프롬프트 템플릿, 포맷된 문자열, 지시사항은 없습니다. 분류 출력은 LLM에서 직접 가져온 단일 딕셔너리로 저장됩니다.

## 4단계: 노드 구축하기

이제 각 단계를 함수로 구현합니다. LangGraph의 노드는 현재 상태를 받아 업데이트를 반환하는 Python 함수일 뿐입니다.





### 오류를 적절하게 처리하기

서로 다른 오류는 서로 다른 처리 전략이 필요합니다:

| 오류 유형 | 수정 주체 | 전략 | 사용 시점 |
|------------|--------------|----------|-------------|
| 일시적 오류(네트워크 문제, 속도 제한) | 시스템(자동) | 재시도 정책 | 재시도 시 일반적으로 해결되는 일시적 실패 |
| LLM 복구 가능 오류(도구 실패, 파싱 문제) | LLM | 상태에 오류를 저장하고 루프백 | LLM이 오류를 보고 접근 방식을 조정할 수 있음 |
| 사용자 수정 가능 오류(누락된 정보, 불명확한 지시사항) | 사람 | `interrupt()`로 일시 중지 | 진행하려면 사용자 입력이 필요 |
| 예상치 못한 오류 | 개발자 | 버블업 허용 | 디버깅이 필요한 알 수 없는 문제 |

<Tabs>
    <Tab title="일시적 오류" icon="rotate">
        네트워크 문제와 속도 제한을 자동으로 재시도하는 재시도 정책을 추가합니다:

    ```python
    from langgraph.types import RetryPolicy

    workflow.add_node(
        "search_documentation",
        search_documentation,
        retry_policy=RetryPolicy(max_attempts=3, initial_interval=1.0)
    )
    ```





    </Tab>

    <Tab title="LLM 복구 가능" icon="brain">
        LLM이 무엇이 잘못되었는지 보고 다시 시도할 수 있도록 상태에 오류를 저장하고 루프백합니다:

    ```python
    from langgraph.types import Command


    def execute_tool(state: State) -> Command[Literal["agent", "execute_tool"]]:
        try:
            result = run_tool(state['tool_call'])
            return Command(update={"tool_result": result}, goto="agent")
        except ToolError as e:
            # Let the LLM see what went wrong and try again
            return Command(
                update={"tool_result": f"Tool error: {str(e)}"},
                goto="agent"
            )
    ```





    </Tab>

    <Tab title="사용자 수정 가능" icon="user">
        필요할 때(계정 ID, 주문 번호 또는 명확화 등) 사용자로부터 정보를 수집하기 위해 일시 중지합니다:

    ```python
    from langgraph.types import Command


    def lookup_customer_history(state: State) -> Command[Literal["draft_response"]]:
        if not state.get('customer_id'):
            user_input = interrupt({
                "message": "Customer ID needed",
                "request": "Please provide the customer's account ID to look up their subscription history"
            })
            return Command(
                update={"customer_id": user_input['customer_id']},
                goto="lookup_customer_history"
            )
        # Now proceed with the lookup
        customer_data = fetch_customer_history(state['customer_id'])
        return Command(update={"customer_history": customer_data}, goto="draft_response")
    ```





    </Tab>

    <Tab title="예상치 못한" icon="triangle-exclamation">
        디버깅을 위해 버블업하도록 합니다. 처리할 수 없는 것은 캐치하지 마세요:

    ```python
    def send_reply(state: EmailAgentState):
        try:
            email_service.send(state["draft_response"])
        except Exception:
            raise  # Surface unexpected errors
    ```





    </Tab>
</Tabs>


### 이메일 에이전트 노드 구현하기

각 노드를 간단한 함수로 구현합니다. 기억하세요: 노드는 상태를 받아 작업을 수행하고 업데이트를 반환합니다.

<AccordionGroup>
    <Accordion title="읽기 및 분류 노드" icon="brain">

    ```python
    from typing import Literal
    from langgraph.graph import StateGraph, START, END
    from langgraph.types import interrupt, Command, RetryPolicy
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import HumanMessage

    llm = ChatOpenAI(model="gpt-4")

    def read_email(state: EmailAgentState) -> dict:
        """Extract and parse email content"""
        # In production, this would connect to your email service
        return {
            "messages": [HumanMessage(content=f"Processing email: {state['email_content']}")]
        }

    def classify_intent(state: EmailAgentState) -> Command[Literal["search_documentation", "human_review", "draft_response", "bug_tracking"]]:
        """Use LLM to classify email intent and urgency, then route accordingly"""

        # Create structured LLM that returns EmailClassification dict
        structured_llm = llm.with_structured_output(EmailClassification)

        # Format the prompt on-demand, not stored in state
        classification_prompt = f"""
        Analyze this customer email and classify it:

        Email: {state['email_content']}
        From: {state['sender_email']}

        Provide classification including intent, urgency, topic, and summary.
        """

        # Get structured response directly as dict
        classification = structured_llm.invoke(classification_prompt)

        # Determine next node based on classification
        if classification['intent'] == 'billing' or classification['urgency'] == 'critical':
            goto = "human_review"
        elif classification['intent'] in ['question', 'feature']:
            goto = "search_documentation"
        elif classification['intent'] == 'bug':
            goto = "bug_tracking"
        else:
            goto = "draft_response"

        # Store classification as a single dict in state
        return Command(
            update={"classification": classification},
            goto=goto
        )
    ```





    </Accordion>

    <Accordion title="검색 및 추적 노드" icon="database">

    ```python
    def search_documentation(state: EmailAgentState) -> Command[Literal["draft_response"]]:
        """Search knowledge base for relevant information"""

        # Build search query from classification
        classification = state.get('classification', {})
        query = f"{classification.get('intent', '')} {classification.get('topic', '')}"

        try:
            # Implement your search logic here
            # Store raw search results, not formatted text
            search_results = [
                "Reset password via Settings > Security > Change Password",
                "Password must be at least 12 characters",
                "Include uppercase, lowercase, numbers, and symbols"
            ]
        except SearchAPIError as e:
            # For recoverable search errors, store error and continue
            search_results = [f"Search temporarily unavailable: {str(e)}"]

        return Command(
            update={"search_results": search_results},  # Store raw results or error
            goto="draft_response"
        )

    def bug_tracking(state: EmailAgentState) -> Command[Literal["draft_response"]]:
        """Create or update bug tracking ticket"""

        # Create ticket in your bug tracking system
        ticket_id = "BUG-12345"  # Would be created via API

        return Command(
            update={
                "search_results": [f"Bug ticket {ticket_id} created"],
                "current_step": "bug_tracked"
            },
            goto="draft_response"
        )
    ```





    </Accordion>

    <Accordion title="응답 노드" icon="pen-to-square">

    ```python
    def draft_response(state: EmailAgentState) -> Command[Literal["human_review", "send_reply"]]:
        """Generate response using context and route based on quality"""

        classification = state.get('classification', {})

        # Format context from raw state data on-demand
        context_sections = []

        if state.get('search_results'):
            # Format search results for the prompt
            formatted_docs = "\n".join([f"- {doc}" for doc in state['search_results']])
            context_sections.append(f"Relevant documentation:\n{formatted_docs}")

        if state.get('customer_history'):
            # Format customer data for the prompt
            context_sections.append(f"Customer tier: {state['customer_history'].get('tier', 'standard')}")

        # Build the prompt with formatted context
        draft_prompt = f"""
        Draft a response to this customer email:
        {state['email_content']}

        Email intent: {classification.get('intent', 'unknown')}
        Urgency level: {classification.get('urgency', 'medium')}

        {chr(10).join(context_sections)}

        Guidelines:
        - Be professional and helpful
        - Address their specific concern
        - Use the provided documentation when relevant
        """

        response = llm.invoke(draft_prompt)

        # Determine if human review needed based on urgency and intent
        needs_review = (
            classification.get('urgency') in ['high', 'critical'] or
            classification.get('intent') == 'complex'
        )

        # Route to appropriate next node
        goto = "human_review" if needs_review else "send_reply"

        return Command(
            update={"draft_response": response.content},  # Store only the raw response
            goto=goto
        )

    def human_review(state: EmailAgentState) -> Command[Literal["send_reply", END]]:
        """Pause for human review using interrupt and route based on decision"""

        classification = state.get('classification', {})
        
        # interrupt() must come first - any code before it will re-run on resume
        human_decision = interrupt({
            "email_id": state.get('email_id',''),
            "original_email": state.get('email_content',''),
            "draft_response": state.get('draft_response',''),
            "urgency": classification.get('urgency'),
            "intent": classification.get('intent'),
            "action": "Please review and approve/edit this response"
        })

        # Now process the human's decision
        if human_decision.get("approved"):
            return Command(
                update={"draft_response": human_decision.get("edited_response", state.get('draft_response',''))},
                goto="send_reply"
            )
        else:
            # Rejection means human will handle directly
            return Command(update={}, goto=END)

    def send_reply(state: EmailAgentState) -> dict:
        """Send the email response"""
        # Integrate with email service
        print(f"Sending reply: {state['draft_response'][:100]}...")
        return {}
    ```





    </Accordion>
</AccordionGroup>

## 5단계: 연결하기

이제 노드들을 작동하는 그래프로 연결합니다. 노드가 자체 라우팅 결정을 처리하므로 몇 가지 필수 엣지만 필요합니다.

`interrupt()`를 사용한 [human-in-the-loop](/oss/python/langgraph/interrupts)를 활성화하려면 실행 간 상태를 저장하기 위해 [checkpointer](/oss/python/langgraph/persistence)로 컴파일해야 합니다:

<Accordion title="그래프 컴파일 코드" icon="diagram-project" defaultOpen={true}>

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import RetryPolicy

# Create the graph
workflow = StateGraph(EmailAgentState)

# Add nodes with appropriate error handling
workflow.add_node("read_email", read_email)
workflow.add_node("classify_intent", classify_intent)

# Add retry policy for nodes that might have transient failures
workflow.add_node(
    "search_documentation",
    search_documentation,
    retry_policy=RetryPolicy(max_attempts=3)
)
workflow.add_node("bug_tracking", bug_tracking)
workflow.add_node("draft_response", draft_response)
workflow.add_node("human_review", human_review)
workflow.add_node("send_reply", send_reply)

# Add only the essential edges
workflow.add_edge(START, "read_email")
workflow.add_edge("read_email", "classify_intent")
workflow.add_edge("send_reply", END)

# Compile with checkpointer for persistence, in case run graph with Local_Server --> Please compile without checkpointer
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```





</Accordion>

그래프 구조는 최소한입니다. 라우팅이 [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) 객체를 통해 노드 내부에서 발생하기 때문입니다. 각 노드는 `Command[Literal["node1", "node2"]]`와 같은 타입 힌트를 사용하여 갈 수 있는 위치를 선언하므로 흐름이 명시적이고 추적 가능합니다.




### 에이전트 사용해보기

사람의 검토가 필요한 긴급 결제 문제로 에이전트를 실행해 봅시다:

<Accordion title="에이전트 테스트" icon="flask">

```python
# Test with an urgent billing issue
initial_state = {
    "email_content": "I was charged twice for my subscription! This is urgent!",
    "sender_email": "customer@example.com",
    "email_id": "email_123",
    "messages": []
}

# Run with a thread_id for persistence
config = {"configurable": {"thread_id": "customer_123"}}
result = app.invoke(initial_state, config)
# The graph will pause at human_review
print(f"Draft ready for review: {result['draft_response'][:100]}...")

# When ready, provide human input to resume
from langgraph.types import Command

human_response = Command(
    resume={
        "approved": True,
        "edited_response": "We sincerely apologize for the double charge. I've initiated an immediate refund..."
    }
)

# Resume execution
final_result = app.invoke(human_response, config)
print(f"Email sent successfully!")
```





</Accordion>

그래프는 `interrupt()`에 도달하면 일시 중지되고, 모든 것을 checkpointer에 저장하고 대기합니다. 며칠 후에도 재개할 수 있으며, 중단된 지점에서 정확히 이어집니다. `thread_id`는 이 대화의 모든 상태가 함께 보존되도록 보장합니다.

## 요약 및 다음 단계

### 핵심 인사이트

이 이메일 에이전트를 구축하면서 LangGraph 사고 방식을 보여주었습니다:

<CardGroup cols={2}>
    <Card title="개별 단계로 분해" icon="sitemap" href="#step-1-map-out-your-workflow-as-discrete-steps">
        각 노드는 한 가지를 잘 수행합니다. 이러한 분해는 스트리밍 진행 업데이트, 일시 중지 및 재개가 가능한 내구성 있는 실행, 단계 간 상태를 검사할 수 있어 명확한 디버깅을 가능하게 합니다.
    </Card>

    <Card title="상태는 공유 메모리" icon="database" href="#step-3-design-your-state">
        포맷된 텍스트가 아닌 원시 데이터를 저장합니다. 이를 통해 서로 다른 노드가 동일한 정보를 다른 방식으로 사용할 수 있습니다.
    </Card>

    <Card title="노드는 함수" icon="code" href="#step-4-build-your-nodes">
        상태를 받아 작업을 수행하고 업데이트를 반환합니다. 라우팅 결정을 내려야 할 때 상태 업데이트와 다음 목적지를 모두 지정합니다.
    </Card>

    <Card title="오류는 흐름의 일부" icon="triangle-exclamation" href="#handle-errors-appropriately">
        일시적 실패는 재시도되고, LLM 복구 가능 오류는 컨텍스트와 함께 루프백되며, 사용자 수정 가능 문제는 입력을 위해 일시 중지되고, 예상치 못한 오류는 디버깅을 위해 버블업됩니다.
    </Card>

    <Card title="사람 입력은 일급 시민" icon="user" href="/oss/python/langgraph/interrupts">
        `interrupt()` 함수는 실행을 무기한 일시 중지하고, 모든 상태를 저장하며, 입력을 제공하면 중단된 지점에서 정확히 재개됩니다. 노드의 다른 작업과 결합될 때는 먼저 와야 합니다.
    </Card>

    <Card title="그래프 구조는 자연스럽게 나타남" icon="diagram-project" href="#step-5-wire-it-together">
        필수 연결을 정의하면 노드가 자체 라우팅 로직을 처리합니다. 이를 통해 제어 흐름이 명시적이고 추적 가능하게 유지됩니다. 현재 노드를 보면 에이전트가 다음에 무엇을 할지 항상 이해할 수 있습니다.
    </Card>
</CardGroup>

### 고급 고려사항

<Accordion title="노드 세분성 트레이드오프" icon="sliders">
<Info>
이 섹션에서는 노드 세분성 설계의 트레이드오프를 탐구합니다. 대부분의 애플리케이션은 이를 건너뛰고 위에 표시된 패턴을 사용할 수 있습니다.
</Info>

궁금할 수 있습니다: `Read Email`과 `Classify Intent`를 하나의 노드로 결합하지 않는 이유는 무엇일까요?

또는 Doc Search를 Draft Reply와 분리하는 이유는 무엇일까요?

답은 복원력과 관찰 가능성 간의 트레이드오프와 관련이 있습니다.

**복원력 고려사항:** LangGraph의 [내구성 있는 실행](/oss/python/langgraph/durable-execution)은 노드 경계에서 체크포인트를 생성합니다. 중단 또는 실패 후 워크플로우가 재개되면 실행이 중지된 노드의 시작 부분부터 시작됩니다. 노드가 작을수록 체크포인트가 더 자주 생성되며, 이는 문제가 발생했을 때 반복해야 할 작업이 줄어든다는 것을 의미합니다. 여러 작업을 하나의 큰 노드로 결합하면 끝 부분 근처에서 실패가 발생할 경우 해당 노드의 시작 부분부터 모든 것을 다시 실행해야 합니다.

이메일 에이전트에 대해 이러한 분해를 선택한 이유:

- **외부 서비스의 격리:** Doc Search와 Bug Track은 외부 API를 호출하기 때문에 별도의 노드입니다. 검색 서비스가 느리거나 실패하면 LLM 호출과 격리하고 싶습니다. 다른 노드에 영향을 주지 않고 이러한 특정 노드에 재시도 정책을 추가할 수 있습니다.

- **중간 가시성:** `Classify Intent`를 자체 노드로 두면 조치를 취하기 전에 LLM이 결정한 내용을 검사할 수 있습니다. 이는 디버깅 및 모니터링에 유용합니다. 에이전트가 언제 왜 사람 검토로 라우팅되는지 정확히 볼 수 있습니다.

- **다른 실패 모드:** LLM 호출, 데이터베이스 조회, 이메일 발송은 서로 다른 재시도 전략을 가집니다. 별도의 노드를 사용하면 이를 독립적으로 구성할 수 있습니다.

- **재사용성 및 테스트:** 작은 노드는 격리된 상태에서 테스트하고 다른 워크플로우에서 재사용하기가 더 쉽습니다.

다른 유효한 접근 방식: `Read Email`과 `Classify Intent`를 단일 노드로 결합할 수 있습니다. 분류 전에 원시 이메일을 검사하는 기능을 잃게 되고 해당 노드의 실패 시 두 작업을 모두 반복하게 됩니다. 대부분의 애플리케이션에서는 별도 노드의 관찰 가능성 및 디버깅 이점이 트레이드오프의 가치가 있습니다.

애플리케이션 수준 고려사항: 2단계의 캐싱 논의(검색 결과를 캐시할지 여부)는 LangGraph 프레임워크 기능이 아닌 애플리케이션 수준 결정입니다. 특정 요구사항에 따라 노드 함수 내에서 캐싱을 구현합니다. LangGraph는 이를 규정하지 않습니다.

성능 고려사항: 노드가 많다고 해서 실행이 느려지는 것은 아닙니다. LangGraph는 기본적으로 백그라운드에서 체크포인트를 작성하므로([비동기 내구성 모드](/oss/python/langgraph/durable-execution#durability-modes)), 그래프는 체크포인트가 완료될 때까지 기다리지 않고 계속 실행됩니다. 이는 최소한의 성능 영향으로 빈번한 체크포인트를 얻을 수 있음을 의미합니다. 필요한 경우 이 동작을 조정할 수 있습니다. 완료 시에만 체크포인트하려면 `"exit"` 모드를 사용하거나, 각 체크포인트가 작성될 때까지 실행을 차단하려면 `"sync"` 모드를 사용합니다.
</Accordion>

### 다음 단계

이것은 LangGraph로 에이전트를 구축하는 사고 방식에 대한 소개였습니다. 이 기초를 다음과 같이 확장할 수 있습니다:

<CardGroup cols={2}>
    <Card title="Human-in-the-loop 패턴" icon="user-check" href="/oss/python/langgraph/interrupts">
        실행 전 도구 승인, 일괄 승인 및 기타 패턴을 추가하는 방법을 배우세요
    </Card>

    <Card title="서브그래프" icon="diagram-nested" href="/oss/python/langgraph/use-subgraphs">
        복잡한 다단계 작업을 위한 서브그래프 생성
    </Card>

    <Card title="스트리밍" icon="tower-broadcast" href="/oss/python/langgraph/streaming">
        사용자에게 실시간 진행 상황을 표시하기 위한 스트리밍 추가
    </Card>

    <Card title="관찰 가능성" icon="chart-line" href="/oss/python/langgraph/observability">
        디버깅 및 모니터링을 위해 LangSmith로 관찰 가능성 추가
    </Card>

    <Card title="도구 통합" icon="wrench" href="/oss/python/langchain/tools">
        웹 검색, 데이터베이스 쿼리 및 API 호출을 위한 더 많은 도구 통합
    </Card>

    <Card title="재시도 로직" icon="rotate" href="/oss/python/langgraph/use-graph-api#add-retry-policies">
        실패한 작업에 대한 지수 백오프를 사용한 재시도 로직 구현
    </Card>
</CardGroup>

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/thinking-in-langgraph.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
