---
title: Guardrails
description: 에이전트를 위한 안전성 검사 및 콘텐츠 필터링 구현
---



Guardrails는 에이전트 실행의 주요 지점에서 콘텐츠를 검증하고 필터링하여 안전하고 규정을 준수하는 AI 애플리케이션을 구축하는 데 도움을 줍니다. 민감한 정보를 감지하고, 콘텐츠 정책을 시행하며, 출력을 검증하고, 문제가 발생하기 전에 안전하지 않은 동작을 방지할 수 있습니다.

일반적인 사용 사례는 다음과 같습니다:
- PII 유출 방지
- 프롬프트 인젝션 공격 감지 및 차단
- 부적절하거나 유해한 콘텐츠 차단
- 비즈니스 규칙 및 규정 준수 요구사항 시행
- 출력 품질 및 정확성 검증

[middleware](/oss/python/langchain/middleware)를 사용하여 전략적 지점(에이전트 시작 전, 완료 후, 또는 model 및 tool 호출 전후)에서 실행을 가로채는 방식으로 guardrails를 구현할 수 있습니다.

<div style={{ display: "flex", justifyContent: "center" }}>
  <img
    src="/oss/images/middleware_final.png"
    alt="Middleware 흐름 다이어그램"
    className="rounded-lg"
  />
</div>

Guardrails는 두 가지 보완적인 접근 방식을 사용하여 구현할 수 있습니다:

<CardGroup cols={2}>
  <Card title="결정론적 guardrails" icon="list-check">
    정규식 패턴, 키워드 매칭 또는 명시적 검사와 같은 규칙 기반 로직을 사용합니다. 빠르고 예측 가능하며 비용 효율적이지만, 미묘한 위반을 놓칠 수 있습니다.
  </Card>
  <Card title="모델 기반 guardrails" icon="brain">
    LLM 또는 classifier를 사용하여 의미론적 이해로 콘텐츠를 평가합니다. 규칙이 놓치는 미묘한 문제를 포착하지만, 더 느리고 비용이 많이 듭니다.
  </Card>
</CardGroup>

LangChain은 내장 guardrails([PII 감지](#pii-detection), [human-in-the-loop](#human-in-the-loop) 등)와 두 가지 접근 방식을 사용하여 커스텀 guardrails를 구축할 수 있는 유연한 middleware 시스템을 모두 제공합니다.

## 내장 guardrails

### PII 감지

LangChain은 대화에서 개인 식별 정보(PII)를 감지하고 처리하기 위한 내장 middleware를 제공합니다. 이 middleware는 이메일, 신용카드, IP 주소 등과 같은 일반적인 PII 유형을 감지할 수 있습니다.

PII 감지 middleware는 규정 준수 요구사항이 있는 의료 및 금융 애플리케이션, 로그를 정제해야 하는 고객 서비스 에이전트, 그리고 일반적으로 민감한 사용자 데이터를 처리하는 모든 애플리케이션에 유용합니다.

PII middleware는 감지된 PII를 처리하기 위한 여러 전략을 지원합니다:

| 전략 | 설명 | 예시 |
|----------|-------------|---------|
| `redact` | `[REDACTED_TYPE]`으로 대체 | `[REDACTED_EMAIL]` |
| `mask` | 부분적으로 가림 (예: 마지막 4자리) | `****-****-****-1234` |
| `hash` | 결정론적 해시로 대체 | `a8f5f167...` |
| `block` | 감지 시 예외 발생 | Error thrown |

```python
from langchain.agents import create_agent
from langchain.agents.middleware import PIIMiddleware


agent = create_agent(
    model="openai:gpt-4o",
    tools=[customer_service_tool, email_tool],
    middleware=[
        # Redact emails in user input before sending to model
        PIIMiddleware(
            "email",
            strategy="redact",
            apply_to_input=True,
        ),
        # Mask credit cards in user input
        PIIMiddleware(
            "credit_card",
            strategy="mask",
            apply_to_input=True,
        ),
        # Block API keys - raise error if detected
        PIIMiddleware(
            "api_key",
            detector=r"sk-[a-zA-Z0-9]{32}",
            strategy="block",
            apply_to_input=True,
        ),
    ],
)

# When user provides PII, it will be handled according to the strategy
result = agent.invoke({
    "messages": [{"role": "user", "content": "My email is john.doe@example.com and card is 4532-1234-5678-9010"}]
})
```




<Accordion title="내장 PII 유형 및 구성">

**내장 PII 유형:**
- `email` - 이메일 주소
- `credit_card` - 신용카드 번호 (Luhn 검증)
- `ip` - IP 주소
- `mac_address` - MAC 주소
- `url` - URL

**구성 옵션:**

Parameter | 설명 | 기본값
-----------|-------------|---------
`pii_type` | 감지할 PII 유형 (내장 또는 커스텀) | Required
`strategy` | 감지된 PII 처리 방법 (`"block"`, `"redact"`, `"mask"`, `"hash"`) | `"redact"`
`detector` | 커스텀 detector function 또는 regex 패턴 | `None` (내장 사용)
`apply_to_input` | model 호출 전 사용자 메시지 검사 | `True`
`apply_to_output` | model 호출 후 AI 메시지 검사 | `False`
`apply_to_tool_results` | 실행 후 tool 결과 메시지 검사 | `False`




</Accordion>

PII 감지 기능에 대한 자세한 내용은 [middleware 문서](/oss/python/langchain/middleware#pii-detection)를 참조하세요.

### Human-in-the-loop

LangChain은 민감한 작업을 실행하기 전에 사람의 승인을 요구하는 내장 middleware를 제공합니다. 이는 중요한 의사결정을 위한 가장 효과적인 guardrails 중 하나입니다.

Human-in-the-loop middleware는 금융 거래 및 이체, 프로덕션 데이터 삭제 또는 수정, 외부 당사자에게 통신 전송, 그리고 비즈니스에 중대한 영향을 미치는 모든 작업과 같은 경우에 유용합니다.

```python
from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.types import Command


agent = create_agent(
    model="openai:gpt-4o",
    tools=[search_tool, send_email_tool, delete_database_tool],
    middleware=[
        HumanInTheLoopMiddleware(
            interrupt_on={
                # Require approval for sensitive operations
                "send_email": True,
                "delete_database": True,
                # Auto-approve safe operations
                "search": False,
            }
        ),
    ],
    # Persist the state across interrupts
    checkpointer=InMemorySaver(),
)

# Human-in-the-loop requires a thread ID for persistence
config = {"configurable": {"thread_id": "some_id"}}

# Agent will pause and wait for approval before executing sensitive tools
result = agent.invoke(
    {"messages": [{"role": "user", "content": "Send an email to the team"}]},
    config=config
)

result = agent.invoke(
    Command(resume={"decisions": [{"type": "approve"}]}),
    config=config  # Same thread ID to resume the paused conversation
)
```




<Tip>
    승인 워크플로우 구현에 대한 자세한 내용은 [human-in-the-loop 문서](/oss/python/langchain/human-in-the-loop)를 참조하세요.
</Tip>

## 커스텀 guardrails

더 정교한 guardrails를 위해 에이전트 실행 전후에 실행되는 커스텀 middleware를 만들 수 있습니다. 이를 통해 검증 로직, 콘텐츠 필터링 및 안전성 검사를 완전히 제어할 수 있습니다.

### Before agent guardrails

"before agent" hook을 사용하여 각 호출 시작 시 한 번 요청을 검증합니다. 이는 인증, 속도 제한 또는 처리가 시작되기 전에 부적절한 요청을 차단하는 것과 같은 세션 수준 검사에 유용합니다.

<CodeGroup>

```python title="Class syntax"
from typing import Any

from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config
from langgraph.runtime import Runtime

class ContentFilterMiddleware(AgentMiddleware):
    """Deterministic guardrail: Block requests containing banned keywords."""

    def __init__(self, banned_keywords: list[str]):
        super().__init__()
        self.banned_keywords = [kw.lower() for kw in banned_keywords]

    @hook_config(can_jump_to=["end"])
    def before_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        # Get the first user message
        if not state["messages"]:
            return None

        first_message = state["messages"][0]
        if first_message.type != "human":
            return None

        content = first_message.content.lower()

        # Check for banned keywords
        for keyword in self.banned_keywords:
            if keyword in content:
                # Block execution before any processing
                return {
                    "messages": [{
                        "role": "assistant",
                        "content": "I cannot process requests containing inappropriate content. Please rephrase your request."
                    }],
                    "jump_to": "end"
                }

        return None

# Use the custom guardrail
from langchain.agents import create_agent

agent = create_agent(
    model="openai:gpt-4o",
    tools=[search_tool, calculator_tool],
    middleware=[
        ContentFilterMiddleware(
            banned_keywords=["hack", "exploit", "malware"]
        ),
    ],
)

# This request will be blocked before any processing
result = agent.invoke({
    "messages": [{"role": "user", "content": "How do I hack into a database?"}]
})
```

```python title="Decorator syntax"
from typing import Any

from langchain.agents.middleware import before_agent, AgentState, hook_config
from langgraph.runtime import Runtime

banned_keywords = ["hack", "exploit", "malware"]

@before_agent(can_jump_to=["end"])
def content_filter(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    """Deterministic guardrail: Block requests containing banned keywords."""
    # Get the first user message
    if not state["messages"]:
        return None

    first_message = state["messages"][0]
    if first_message.type != "human":
        return None

    content = first_message.content.lower()

    # Check for banned keywords
    for keyword in banned_keywords:
        if keyword in content:
            # Block execution before any processing
            return {
                "messages": [{
                    "role": "assistant",
                    "content": "I cannot process requests containing inappropriate content. Please rephrase your request."
                }],
                "jump_to": "end"
            }

    return None

# Use the custom guardrail
from langchain.agents import create_agent

agent = create_agent(
    model="openai:gpt-4o",
    tools=[search_tool, calculator_tool],
    middleware=[content_filter],
)

# This request will be blocked before any processing
result = agent.invoke({
    "messages": [{"role": "user", "content": "How do I hack into a database?"}]
})
```
</CodeGroup>





### After agent guardrails

"after agent" hook을 사용하여 사용자에게 반환하기 전에 최종 출력을 한 번 검증합니다. 이는 모델 기반 안전성 검사, 품질 검증 또는 완전한 에이전트 응답에 대한 최종 규정 준수 스캔에 유용합니다.

<CodeGroup>

```python title="Class syntax"
from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config
from langgraph.runtime import Runtime
from langchain_core.messages import AIMessage
from langchain.chat_models import init_chat_model
from typing import Any

class SafetyGuardrailMiddleware(AgentMiddleware):
    """Model-based guardrail: Use an LLM to evaluate response safety."""

    def __init__(self):
        super().__init__()
        self.safety_model = init_chat_model("openai:gpt-4o-mini")

    @hook_config(can_jump_to=["end"])
    def after_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        # Get the final AI response
        if not state["messages"]:
            return None

        last_message = state["messages"][-1]
        if not isinstance(last_message, AIMessage):
            return None

        # Use a model to evaluate safety
        safety_prompt = f"""Evaluate if this response is safe and appropriate.
        Respond with only 'SAFE' or 'UNSAFE'.

        Response: {last_message.content}"""

        result = self.safety_model.invoke([{"role": "user", "content": safety_prompt}])

        if "UNSAFE" in result.content:
            return {
                "messages": [{
                    "role": "assistant",
                    "content": "I cannot provide that response. Please rephrase your request."
                }],
                "jump_to": "end"
            }

        return None

# Use the safety guardrail
from langchain.agents import create_agent

agent = create_agent(
    model="openai:gpt-4o",
    tools=[search_tool, calculator_tool],
    middleware=[SafetyGuardrailMiddleware()],
)

result = agent.invoke({
    "messages": [{"role": "user", "content": "How do I make explosives?"}]
})
```

```python title="Decorator syntax"
from langchain.agents.middleware import after_agent, AgentState, hook_config
from langgraph.runtime import Runtime
from langchain_core.messages import AIMessage
from langchain.chat_models import init_chat_model
from typing import Any

safety_model = init_chat_model("openai:gpt-4o-mini")

@after_agent(can_jump_to=["end"])
def safety_guardrail(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    """Model-based guardrail: Use an LLM to evaluate response safety."""
    # Get the final AI response
    if not state["messages"]:
        return None

    last_message = state["messages"][-1]
    if not isinstance(last_message, AIMessage):
        return None

    # Use a model to evaluate safety
    safety_prompt = f"""Evaluate if this response is safe and appropriate.
    Respond with only 'SAFE' or 'UNSAFE'.

    Response: {last_message.content}"""

    result = safety_model.invoke([{"role": "user", "content": safety_prompt}])

    if "UNSAFE" in result.content:
        return {
            "messages": [{
                "role": "assistant",
                "content": "I cannot provide that response. Please rephrase your request."
            }],
            "jump_to": "end"
        }

    return None

# Use the safety guardrail
from langchain.agents import create_agent

agent = create_agent(
    model="openai:gpt-4o",
    tools=[search_tool, calculator_tool],
    middleware=[safety_guardrail],
)

result = agent.invoke({
    "messages": [{"role": "user", "content": "How do I make explosives?"}]
})
```

</CodeGroup>





### 여러 guardrails 결합

middleware 배열에 추가하여 여러 guardrails를 쌓을 수 있습니다. 순서대로 실행되어 계층화된 보호를 구축할 수 있습니다:

```python
from langchain.agents import create_agent
from langchain.agents.middleware import PIIMiddleware, HumanInTheLoopMiddleware

agent = create_agent(
    model="openai:gpt-4o",
    tools=[search_tool, send_email_tool],
    middleware=[
        # Layer 1: Deterministic input filter (before agent)
        ContentFilterMiddleware(banned_keywords=["hack", "exploit"]),

        # Layer 2: PII protection (before and after model)
        PIIMiddleware("email", strategy="redact", apply_to_input=True),
        PIIMiddleware("email", strategy="redact", apply_to_output=True),

        # Layer 3: Human approval for sensitive tools
        HumanInTheLoopMiddleware(interrupt_on={"send_email": True}),

        # Layer 4: Model-based safety check (after agent)
        SafetyGuardrailMiddleware(),
    ],
)
```




## 추가 리소스

- [Middleware 문서](/oss/python/langchain/middleware) - 커스텀 middleware 완전 가이드
- [Middleware API reference](https://reference.langchain.com/python/langchain/middleware/) - 커스텀 middleware 완전 가이드
- [Human-in-the-loop](/oss/python/langchain/human-in-the-loop) - 민감한 작업에 대한 사람의 검토 추가
- [에이전트 테스트](/oss/python/langchain/test) - 안전성 메커니즘 테스트 전략

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/guardrails.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
