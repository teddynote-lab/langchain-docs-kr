---
title: Model Context Protocol (MCP)
---



[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction)은 애플리케이션이 LLM에 도구와 컨텍스트를 제공하는 방법을 표준화하는 개방형 프로토콜입니다. LangChain agent는 [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) 라이브러리를 사용하여 MCP server에 정의된 도구를 사용할 수 있습니다.



## 설치

LangGraph에서 MCP 도구를 사용하려면 `langchain-mcp-adapters` 라이브러리를 설치하세요:

<CodeGroup>
```bash pip
pip install langchain-mcp-adapters
```

```bash uv
uv add langchain-mcp-adapters
```
</CodeGroup>




## Transport 타입

MCP는 클라이언트-서버 통신을 위한 다양한 transport 메커니즘을 지원합니다:

- stdio: 클라이언트가 서버를 하위 프로세스로 실행하고 표준 입력/출력을 통해 통신합니다. 로컬 도구와 간단한 설정에 가장 적합합니다.
- Streamable HTTP: 서버가 HTTP 요청을 처리하는 독립적인 프로세스로 실행됩니다. 원격 연결과 여러 클라이언트를 지원합니다.
- Server-Sent Events (SSE): 실시간 스트리밍 통신에 최적화된 streamable HTTP의 변형입니다.

## MCP 도구 사용

`langchain-mcp-adapters`는 agent가 하나 이상의 MCP server에 정의된 도구를 사용할 수 있게 합니다.

```python Accessing multiple MCP servers icon="server"
from langchain_mcp_adapters.client import MultiServerMCPClient  # [!code highlight]
from langchain.agents import create_agent


client = MultiServerMCPClient(  # [!code highlight]
    {
        "math": {
            "transport": "stdio",  # Local subprocess communication
            "command": "python",
            # Absolute path to your math_server.py file
            "args": ["/path/to/math_server.py"],
        },
        "weather": {
            "transport": "streamable_http",  # HTTP-based remote server
            # Ensure you start your weather server on port 8000
            "url": "http://localhost:8000/mcp",
        }
    }
)

tools = await client.get_tools()  # [!code highlight]
agent = create_agent(
    "anthropic:claude-sonnet-4-5",
    tools  # [!code highlight]
)
math_response = await agent.ainvoke(
    {"messages": [{"role": "user", "content": "what's (3 + 5) x 12?"}]}
)
weather_response = await agent.ainvoke(
    {"messages": [{"role": "user", "content": "what is the weather in nyc?"}]}
)
```





<Note>
    `MultiServerMCPClient`는 **기본적으로 stateless입니다**. 각 도구 호출은 새로운 MCP `ClientSession`을 생성하고, 도구를 실행한 다음 정리합니다.
</Note>

## 커스텀 MCP server

자체 MCP server를 생성하려면 `mcp` 라이브러리를 사용할 수 있습니다. 이 라이브러리는 [도구](https://modelcontextprotocol.io/docs/learn/server-concepts#tools-ai-actions)를 정의하고 server로 실행하는 간단한 방법을 제공합니다.

<CodeGroup>
```bash pip
pip install mcp
```

```bash uv
uv add mcp
```
</CodeGroup>




다음 참조 구현을 사용하여 MCP 도구 server로 agent를 테스트하세요.

```python title="Math server (stdio transport)" icon="floppy-disk"
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Math")

@mcp.tool()
def add(a: int, b: int) -> int:
    """Add two numbers"""
    return a + b

@mcp.tool()
def multiply(a: int, b: int) -> int:
    """Multiply two numbers"""
    return a * b

if __name__ == "__main__":
    mcp.run(transport="stdio")
```

```python title="Weather server (streamable HTTP transport)" icon="wifi"
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Weather")

@mcp.tool()
async def get_weather(location: str) -> str:
    """Get weather for location."""
    return "It's always sunny in New York"

if __name__ == "__main__":
    mcp.run(transport="streamable-http")
```




## Stateful 도구 사용

도구 호출 간에 컨텍스트를 유지하는 stateful server의 경우, `client.session()`을 사용하여 영구적인 `ClientSession`을 생성하세요.

```python Using MCP ClientSession for stateful tool usage
from langchain_mcp_adapters.tools import load_mcp_tools

client = MultiServerMCPClient({...})
async with client.session("math") as session:
    tools = await load_mcp_tools(session)
```




## 추가 리소스

* [MCP 문서](https://modelcontextprotocol.io/introduction)
* [MCP Transport 문서](https://modelcontextprotocol.io/docs/concepts/transports)
* [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters)

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/mcp.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
