---
title: Intel
---

>[Optimum Intel](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#optimum-intel)은 🤗 Transformers 및 Diffusers 라이브러리와 Intel에서 제공하는 다양한 도구 및 라이브러리 간의 인터페이스로, Intel 아키텍처에서 end-to-end 파이프라인을 가속화합니다.

>[Intel® Extension for Transformers](https://github.com/intel/intel-extension-for-transformers?tab=readme-ov-file#intel-extension-for-transformers) (ITREX)는 Intel Gaudi2, Intel CPU, Intel GPU를 포함한 다양한 Intel 플랫폼에서 Transformer 기반 모델의 최적 성능으로 GenAI/LLM을 어디서나 가속화하도록 설계된 혁신적인 툴킷입니다.

이 페이지는 LangChain에서 optimum-intel과 ITREX를 사용하는 방법을 다룹니다.

## Optimum-intel

[optimum-intel](https://github.com/huggingface/optimum-intel.git) 및 [IPEX](https://github.com/intel/intel-extension-for-pytorch)와 관련된 모든 기능입니다.

### Installation

다음을 사용하여 optimum-intel과 ipex를 설치합니다:

<CodeGroup>
```bash pip
pip install optimum[neural-compressor]
pip install intel_extension_for_pytorch
```

```bash uv
uv add optimum[neural-compressor]
uv add intel_extension_for_pytorch
```
</CodeGroup>

아래 지정된 대로 설치 지침을 따르세요:

* [여기](https://github.com/huggingface/optimum-intel)에 표시된 대로 optimum-intel을 설치합니다.
* [여기](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=cpu&version=v2.2.0%2Bcpu)에 표시된 대로 IPEX를 설치합니다.

### Embedding Models

[사용 예제](/oss/python/integrations/text_embedding/optimum_intel)를 참조하세요.
또한 RAG 파이프라인에서 embedder를 사용하기 위한 전체 튜토리얼 노트북 [`rag_with_quantized_embeddings.ipynb`](https://github.com/langchain-ai/langchain/blob/v0.3/cookbook/rag_with_quantized_embeddings.ipynb)도 제공합니다.

```python
from langchain_community.embeddings import QuantizedBiEncoderEmbeddings
```

## Intel® Extension for Transformers (ITREX)
(ITREX)는 Intel 플랫폼에서 Transformer 기반 모델을 가속화하는 혁신적인 툴킷으로, 특히 4세대 Intel Xeon Scalable 프로세서 Sapphire Rapids(코드명 Sapphire Rapids)에서 효과적입니다.

Quantization은 더 적은 수의 비트를 사용하여 가중치를 표현함으로써 이러한 가중치의 정밀도를 줄이는 프로세스입니다. Weight-only quantization은 특히 신경망의 가중치를 양자화하는 데 중점을 두면서 activation과 같은 다른 구성 요소는 원래 정밀도로 유지합니다.

대규모 언어 모델(LLM)이 더욱 보편화됨에 따라, 정확도를 유지하면서 이러한 현대 아키텍처의 계산 요구 사항을 충족할 수 있는 새롭고 개선된 양자화 방법에 대한 필요성이 증가하고 있습니다. W8A8과 같은 [일반 양자화](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/quantization.md)와 비교할 때, weight only quantization은 성능과 정확도의 균형을 맞추는 더 나은 절충안일 수 있습니다. 아래에서 볼 수 있듯이 LLM 배포의 병목 현상은 메모리 대역폭이며 일반적으로 weight only quantization이 더 나은 정확도를 제공할 수 있기 때문입니다.

여기서는 ITREX를 사용한 Embedding Models와 Transformers 대규모 언어 모델을 위한 Weight-only quantization을 소개합니다. Weight-only quantization은 딥러닝에서 신경망의 메모리 및 계산 요구 사항을 줄이는 데 사용되는 기술입니다. 심층 신경망의 맥락에서 모델 파라미터(가중치라고도 함)는 일반적으로 부동 소수점 숫자를 사용하여 표현되며, 이는 상당한 양의 메모리를 소비하고 집약적인 계산 리소스를 필요로 할 수 있습니다.

[intel-extension-for-transformers](https://github.com/intel/intel-extension-for-transformers)와 관련된 모든 기능입니다.

### Installation

intel-extension-for-transformers를 설치합니다. 시스템 요구 사항 및 기타 설치 팁은 [설치 가이드](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/installation.md)를 참조하세요.

<CodeGroup>
```bash pip
pip install intel-extension-for-transformers
```

```bash uv
uv add intel-extension-for-transformers
```
</CodeGroup>

기타 필요한 패키지를 설치합니다.

<CodeGroup>
```bash pip
pip install -U torch onnx accelerate datasets
```

```bash uv
uv add torch onnx accelerate datasets
```
</CodeGroup>

### Embedding Models

[사용 예제](/oss/python/integrations/text_embedding/itrex)를 참조하세요.

```python
from langchain_community.embeddings import QuantizedBgeEmbeddings
```

### Weight-Only Quantization with ITREX

[사용 예제](/oss/python/integrations/llms/weight_only_quantization)를 참조하세요.

## Detail of Configuration Parameters

다음은 `WeightOnlyQuantConfig` 클래스의 세부 사항입니다.

#### weight_dtype (string): Weight Data Type, 기본값은 "nf4"입니다.
저장을 위해 가중치를 다음 데이터 타입으로 양자화하는 것을 지원합니다(WeightOnlyQuantConfig의 weight_dtype):
* **int8**: 8비트 데이터 타입을 사용합니다.
* **int4_fullrange**: 일반 int4 범위 [-7,7]와 비교하여 int4 범위의 -8 값을 사용합니다.
* **int4_clip**: int4 범위 내의 값을 클리핑하고 유지하며, 다른 값은 0으로 설정합니다.
* **nf4**: 정규화된 float 4비트 데이터 타입을 사용합니다.
* **fp4_e2m1**: 일반 float 4비트 데이터 타입을 사용합니다. "e2"는 지수에 2비트가 사용됨을 의미하고, "m1"은 가수에 1비트가 사용됨을 의미합니다.

#### compute_dtype (string): Computing Data Type, 기본값은 "fp32"입니다.
이러한 기술은 가중치를 4비트 또는 8비트로 저장하지만, 계산은 여전히 float32, bfloat16 또는 int8로 수행됩니다(WeightOnlyQuantConfig의 compute_dtype):
* **fp32**: float32 데이터 타입을 사용하여 계산합니다.
* **bf16**: bfloat16 데이터 타입을 사용하여 계산합니다.
* **int8**: 8비트 데이터 타입을 사용하여 계산합니다.

#### llm_int8_skip_modules (list of module's name): Modules to Skip Quantization, 기본값은 None입니다.
양자화를 건너뛸 모듈의 목록입니다.

#### scale_dtype (string): The Scale Data Type, 기본값은 "fp32"입니다.
현재 "fp32"(float32)만 지원합니다.

#### mse_range (boolean): Whether to Search for The Best Clip Range from Range [0.805, 1.0, 0.005], 기본값은 False입니다.
#### use_double_quant (boolean): Whether to Quantize Scale, 기본값은 False입니다.
아직 지원되지 않습니다.
#### double_quant_dtype (string): Reserve for Double Quantization.
#### double_quant_scale_dtype (string): Reserve for Double Quantization.
#### group_size (int): Group Size When Auantization.
#### scheme (string): Which Format Weight Be Quantize to. 기본값은 "sym"입니다.
* **sym**: Symmetric.
* **asym**: Asymmetric.
#### algorithm (string): Which Algorithm to Improve the Accuracy . 기본값은 "RTN"입니다.
* **RTN**: Round-to-nearest (RTN)는 매우 직관적으로 생각할 수 있는 양자화 방법입니다.
* **AWQ**: 중요한 가중치의 1%만 보호해도 양자화 오류를 크게 줄일 수 있습니다. 중요한 가중치 채널은 채널별 activation 및 가중치의 분포를 관찰하여 선택됩니다. 중요한 가중치는 보존을 위해 양자화 전에 큰 스케일 팩터를 곱한 후에도 양자화됩니다.
* **TEQ**: weight-only quantization에서 FP32 정밀도를 보존하는 훈련 가능한 등가 변환입니다.

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/intel.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
