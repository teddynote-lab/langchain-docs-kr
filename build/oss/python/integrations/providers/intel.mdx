---
title: Intel
---

>[Optimum Intel](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#optimum-intel)ì€ ğŸ¤— Transformers ë° Diffusers ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ Intelì—ì„œ ì œê³µí•˜ëŠ” ë‹¤ì–‘í•œ ë„êµ¬ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°„ì˜ ì¸í„°í˜ì´ìŠ¤ë¡œ, Intel ì•„í‚¤í…ì²˜ì—ì„œ end-to-end íŒŒì´í”„ë¼ì¸ì„ ê°€ì†í™”í•©ë‹ˆë‹¤.

>[IntelÂ® Extension for Transformers](https://github.com/intel/intel-extension-for-transformers?tab=readme-ov-file#intel-extension-for-transformers) (ITREX)ëŠ” Intel Gaudi2, Intel CPU, Intel GPUë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ Intel í”Œë«í¼ì—ì„œ Transformer ê¸°ë°˜ ëª¨ë¸ì˜ ìµœì  ì„±ëŠ¥ìœ¼ë¡œ GenAI/LLMì„ ì–´ë””ì„œë‚˜ ê°€ì†í™”í•˜ë„ë¡ ì„¤ê³„ëœ í˜ì‹ ì ì¸ íˆ´í‚·ì…ë‹ˆë‹¤.

ì´ í˜ì´ì§€ëŠ” LangChainì—ì„œ optimum-intelê³¼ ITREXë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## Optimum-intel

[optimum-intel](https://github.com/huggingface/optimum-intel.git) ë° [IPEX](https://github.com/intel/intel-extension-for-pytorch)ì™€ ê´€ë ¨ëœ ëª¨ë“  ê¸°ëŠ¥ì…ë‹ˆë‹¤.

### Installation

ë‹¤ìŒì„ ì‚¬ìš©í•˜ì—¬ optimum-intelê³¼ ipexë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤:

<CodeGroup>
```bash pip
pip install optimum[neural-compressor]
pip install intel_extension_for_pytorch
```

```bash uv
uv add optimum[neural-compressor]
uv add intel_extension_for_pytorch
```
</CodeGroup>

ì•„ë˜ ì§€ì •ëœ ëŒ€ë¡œ ì„¤ì¹˜ ì§€ì¹¨ì„ ë”°ë¥´ì„¸ìš”:

* [ì—¬ê¸°](https://github.com/huggingface/optimum-intel)ì— í‘œì‹œëœ ëŒ€ë¡œ optimum-intelì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.
* [ì—¬ê¸°](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=cpu&version=v2.2.0%2Bcpu)ì— í‘œì‹œëœ ëŒ€ë¡œ IPEXë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.

### Embedding Models

[ì‚¬ìš© ì˜ˆì œ](/oss/python/integrations/text_embedding/optimum_intel)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
ë˜í•œ RAG íŒŒì´í”„ë¼ì¸ì—ì„œ embedderë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì „ì²´ íŠœí† ë¦¬ì–¼ ë…¸íŠ¸ë¶ [`rag_with_quantized_embeddings.ipynb`](https://github.com/langchain-ai/langchain/blob/v0.3/cookbook/rag_with_quantized_embeddings.ipynb)ë„ ì œê³µí•©ë‹ˆë‹¤.

```python
from langchain_community.embeddings import QuantizedBiEncoderEmbeddings
```

## IntelÂ® Extension for Transformers (ITREX)
(ITREX)ëŠ” Intel í”Œë«í¼ì—ì„œ Transformer ê¸°ë°˜ ëª¨ë¸ì„ ê°€ì†í™”í•˜ëŠ” í˜ì‹ ì ì¸ íˆ´í‚·ìœ¼ë¡œ, íŠ¹íˆ 4ì„¸ëŒ€ Intel Xeon Scalable í”„ë¡œì„¸ì„œ Sapphire Rapids(ì½”ë“œëª… Sapphire Rapids)ì—ì„œ íš¨ê³¼ì ì…ë‹ˆë‹¤.

Quantizationì€ ë” ì ì€ ìˆ˜ì˜ ë¹„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ í‘œí˜„í•¨ìœ¼ë¡œì¨ ì´ëŸ¬í•œ ê°€ì¤‘ì¹˜ì˜ ì •ë°€ë„ë¥¼ ì¤„ì´ëŠ” í”„ë¡œì„¸ìŠ¤ì…ë‹ˆë‹¤. Weight-only quantizationì€ íŠ¹íˆ ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì–‘ìí™”í•˜ëŠ” ë° ì¤‘ì ì„ ë‘ë©´ì„œ activationê³¼ ê°™ì€ ë‹¤ë¥¸ êµ¬ì„± ìš”ì†ŒëŠ” ì›ë˜ ì •ë°€ë„ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.

ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë”ìš± ë³´í¸í™”ë¨ì— ë”°ë¼, ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì´ëŸ¬í•œ í˜„ëŒ€ ì•„í‚¤í…ì²˜ì˜ ê³„ì‚° ìš”êµ¬ ì‚¬í•­ì„ ì¶©ì¡±í•  ìˆ˜ ìˆëŠ” ìƒˆë¡­ê³  ê°œì„ ëœ ì–‘ìí™” ë°©ë²•ì— ëŒ€í•œ í•„ìš”ì„±ì´ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤. W8A8ê³¼ ê°™ì€ [ì¼ë°˜ ì–‘ìí™”](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/quantization.md)ì™€ ë¹„êµí•  ë•Œ, weight only quantizationì€ ì„±ëŠ¥ê³¼ ì •í™•ë„ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ë” ë‚˜ì€ ì ˆì¶©ì•ˆì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ LLM ë°°í¬ì˜ ë³‘ëª© í˜„ìƒì€ ë©”ëª¨ë¦¬ ëŒ€ì—­í­ì´ë©° ì¼ë°˜ì ìœ¼ë¡œ weight only quantizationì´ ë” ë‚˜ì€ ì •í™•ë„ë¥¼ ì œê³µí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

ì—¬ê¸°ì„œëŠ” ITREXë¥¼ ì‚¬ìš©í•œ Embedding Modelsì™€ Transformers ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ìœ„í•œ Weight-only quantizationì„ ì†Œê°œí•©ë‹ˆë‹¤. Weight-only quantizationì€ ë”¥ëŸ¬ë‹ì—ì„œ ì‹ ê²½ë§ì˜ ë©”ëª¨ë¦¬ ë° ê³„ì‚° ìš”êµ¬ ì‚¬í•­ì„ ì¤„ì´ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì‹¬ì¸µ ì‹ ê²½ë§ì˜ ë§¥ë½ì—ì„œ ëª¨ë¸ íŒŒë¼ë¯¸í„°(ê°€ì¤‘ì¹˜ë¼ê³ ë„ í•¨)ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë¶€ë™ ì†Œìˆ˜ì  ìˆ«ìë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œí˜„ë˜ë©°, ì´ëŠ” ìƒë‹¹í•œ ì–‘ì˜ ë©”ëª¨ë¦¬ë¥¼ ì†Œë¹„í•˜ê³  ì§‘ì•½ì ì¸ ê³„ì‚° ë¦¬ì†ŒìŠ¤ë¥¼ í•„ìš”ë¡œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

[intel-extension-for-transformers](https://github.com/intel/intel-extension-for-transformers)ì™€ ê´€ë ¨ëœ ëª¨ë“  ê¸°ëŠ¥ì…ë‹ˆë‹¤.

### Installation

intel-extension-for-transformersë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œ ìš”êµ¬ ì‚¬í•­ ë° ê¸°íƒ€ ì„¤ì¹˜ íŒì€ [ì„¤ì¹˜ ê°€ì´ë“œ](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/installation.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

<CodeGroup>
```bash pip
pip install intel-extension-for-transformers
```

```bash uv
uv add intel-extension-for-transformers
```
</CodeGroup>

ê¸°íƒ€ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.

<CodeGroup>
```bash pip
pip install -U torch onnx accelerate datasets
```

```bash uv
uv add torch onnx accelerate datasets
```
</CodeGroup>

### Embedding Models

[ì‚¬ìš© ì˜ˆì œ](/oss/python/integrations/text_embedding/itrex)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

```python
from langchain_community.embeddings import QuantizedBgeEmbeddings
```

### Weight-Only Quantization with ITREX

[ì‚¬ìš© ì˜ˆì œ](/oss/python/integrations/llms/weight_only_quantization)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

## Detail of Configuration Parameters

ë‹¤ìŒì€ `WeightOnlyQuantConfig` í´ë˜ìŠ¤ì˜ ì„¸ë¶€ ì‚¬í•­ì…ë‹ˆë‹¤.

#### weight_dtype (string): Weight Data Type, ê¸°ë³¸ê°’ì€ "nf4"ì…ë‹ˆë‹¤.
ì €ì¥ì„ ìœ„í•´ ê°€ì¤‘ì¹˜ë¥¼ ë‹¤ìŒ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ì–‘ìí™”í•˜ëŠ” ê²ƒì„ ì§€ì›í•©ë‹ˆë‹¤(WeightOnlyQuantConfigì˜ weight_dtype):
* **int8**: 8ë¹„íŠ¸ ë°ì´í„° íƒ€ì…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
* **int4_fullrange**: ì¼ë°˜ int4 ë²”ìœ„ [-7,7]ì™€ ë¹„êµí•˜ì—¬ int4 ë²”ìœ„ì˜ -8 ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
* **int4_clip**: int4 ë²”ìœ„ ë‚´ì˜ ê°’ì„ í´ë¦¬í•‘í•˜ê³  ìœ ì§€í•˜ë©°, ë‹¤ë¥¸ ê°’ì€ 0ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
* **nf4**: ì •ê·œí™”ëœ float 4ë¹„íŠ¸ ë°ì´í„° íƒ€ì…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
* **fp4_e2m1**: ì¼ë°˜ float 4ë¹„íŠ¸ ë°ì´í„° íƒ€ì…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. "e2"ëŠ” ì§€ìˆ˜ì— 2ë¹„íŠ¸ê°€ ì‚¬ìš©ë¨ì„ ì˜ë¯¸í•˜ê³ , "m1"ì€ ê°€ìˆ˜ì— 1ë¹„íŠ¸ê°€ ì‚¬ìš©ë¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

#### compute_dtype (string): Computing Data Type, ê¸°ë³¸ê°’ì€ "fp32"ì…ë‹ˆë‹¤.
ì´ëŸ¬í•œ ê¸°ìˆ ì€ ê°€ì¤‘ì¹˜ë¥¼ 4ë¹„íŠ¸ ë˜ëŠ” 8ë¹„íŠ¸ë¡œ ì €ì¥í•˜ì§€ë§Œ, ê³„ì‚°ì€ ì—¬ì „íˆ float32, bfloat16 ë˜ëŠ” int8ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤(WeightOnlyQuantConfigì˜ compute_dtype):
* **fp32**: float32 ë°ì´í„° íƒ€ì…ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.
* **bf16**: bfloat16 ë°ì´í„° íƒ€ì…ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.
* **int8**: 8ë¹„íŠ¸ ë°ì´í„° íƒ€ì…ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.

#### llm_int8_skip_modules (list of module's name): Modules to Skip Quantization, ê¸°ë³¸ê°’ì€ Noneì…ë‹ˆë‹¤.
ì–‘ìí™”ë¥¼ ê±´ë„ˆë›¸ ëª¨ë“ˆì˜ ëª©ë¡ì…ë‹ˆë‹¤.

#### scale_dtype (string): The Scale Data Type, ê¸°ë³¸ê°’ì€ "fp32"ì…ë‹ˆë‹¤.
í˜„ì¬ "fp32"(float32)ë§Œ ì§€ì›í•©ë‹ˆë‹¤.

#### mse_range (boolean): Whether to Search for The Best Clip Range from Range [0.805, 1.0, 0.005], ê¸°ë³¸ê°’ì€ Falseì…ë‹ˆë‹¤.
#### use_double_quant (boolean): Whether to Quantize Scale, ê¸°ë³¸ê°’ì€ Falseì…ë‹ˆë‹¤.
ì•„ì§ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
#### double_quant_dtype (string): Reserve for Double Quantization.
#### double_quant_scale_dtype (string): Reserve for Double Quantization.
#### group_size (int): Group Size When Auantization.
#### scheme (string): Which Format Weight Be Quantize to. ê¸°ë³¸ê°’ì€ "sym"ì…ë‹ˆë‹¤.
* **sym**: Symmetric.
* **asym**: Asymmetric.
#### algorithm (string): Which Algorithm to Improve the Accuracy . ê¸°ë³¸ê°’ì€ "RTN"ì…ë‹ˆë‹¤.
* **RTN**: Round-to-nearest (RTN)ëŠ” ë§¤ìš° ì§ê´€ì ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆëŠ” ì–‘ìí™” ë°©ë²•ì…ë‹ˆë‹¤.
* **AWQ**: ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ì˜ 1%ë§Œ ë³´í˜¸í•´ë„ ì–‘ìí™” ì˜¤ë¥˜ë¥¼ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ ì±„ë„ì€ ì±„ë„ë³„ activation ë° ê°€ì¤‘ì¹˜ì˜ ë¶„í¬ë¥¼ ê´€ì°°í•˜ì—¬ ì„ íƒë©ë‹ˆë‹¤. ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ëŠ” ë³´ì¡´ì„ ìœ„í•´ ì–‘ìí™” ì „ì— í° ìŠ¤ì¼€ì¼ íŒ©í„°ë¥¼ ê³±í•œ í›„ì—ë„ ì–‘ìí™”ë©ë‹ˆë‹¤.
* **TEQ**: weight-only quantizationì—ì„œ FP32 ì •ë°€ë„ë¥¼ ë³´ì¡´í•˜ëŠ” í›ˆë ¨ ê°€ëŠ¥í•œ ë“±ê°€ ë³€í™˜ì…ë‹ˆë‹¤.

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/intel.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
