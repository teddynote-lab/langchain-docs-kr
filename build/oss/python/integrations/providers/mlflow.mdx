---
title: LLM을 위한 MLflow AI Gateway
---

>[LLM을 위한 MLflow AI Gateway](https://www.mlflow.org/docs/latest/llms/deployments/index.html)는 조직 내에서 OpenAI 및 Anthropic과 같은 다양한 대규모
> 언어 모델(LLM) provider의 사용 및 관리를 간소화하도록 설계된 강력한 도구입니다. 특정 LLM 관련 요청을 처리하기 위한 통합 endpoint를 제공하여
> 이러한 서비스와의 상호 작용을 단순화하는 high-level interface를 제공합니다.

## Installation 및 Setup

MLflow GenAI dependencies와 함께 `mlflow`를 설치합니다:

<CodeGroup>
```bash pip
pip install 'mlflow[genai]'
```

```bash uv
uv add mlflow[genai]
```
</CodeGroup>

OpenAI API key를 environment variable로 설정합니다:

```sh
export OPENAI_API_KEY=...
```

configuration file을 생성합니다:

```yaml
endpoints:
  - name: completions
    endpoint_type: llm/v1/completions
    model:
      provider: openai
      name: text-davinci-003
      config:
        openai_api_key: $OPENAI_API_KEY

  - name: embeddings
    endpoint_type: llm/v1/embeddings
    model:
      provider: openai
      name: text-embedding-ada-002
      config:
        openai_api_key: $OPENAI_API_KEY
```

gateway server를 시작합니다:

```sh
mlflow gateway start --config-path /path/to/config.yaml
```

## `MLflow`에서 제공하는 예제

>`mlflow.langchain` module은 `LangChain` model을 logging하고 loading하기 위한 API를 제공합니다.
> 이 module은 langchain flavor의 multivariate LangChain model과 pyfunc flavor의 univariate LangChain
> model을 export합니다.

자세한 내용은 [API documentation 및 예제](https://www.mlflow.org/docs/latest/llms/langchain/index.html)를 참조하세요.

## Completions 예제

```python
import mlflow
from langchain.chains import LLMChain, PromptTemplate
from langchain_community.llms import Mlflow

llm = Mlflow(
    target_uri="http://127.0.0.1:5000",
    endpoint="completions",
)

llm_chain = LLMChain(
    llm=Mlflow,
    prompt=PromptTemplate(
        input_variables=["adjective"],
        template="Tell me a {adjective} joke",
    ),
)
result = llm_chain.run(adjective="funny")
print(result)

with mlflow.start_run():
    model_info = mlflow.langchain.log_model(chain, "model")

model = mlflow.pyfunc.load_model(model_info.model_uri)
print(model.predict([{"adjective": "funny"}]))
```

## Embeddings 예제

```python
from langchain_community.embeddings import MlflowEmbeddings

embeddings = MlflowEmbeddings(
    target_uri="http://127.0.0.1:5000",
    endpoint="embeddings",
)

print(embeddings.embed_query("hello"))
print(embeddings.embed_documents(["hello"]))
```

## Chat 예제

```python
from langchain_community.chat_models import ChatMlflow
from langchain.messages import HumanMessage, SystemMessage

chat = ChatMlflow(
    target_uri="http://127.0.0.1:5000",
    endpoint="chat",
)

messages = [
    SystemMessage(
        content="You are a helpful assistant that translates English to French."
    ),
    HumanMessage(
        content="Translate this sentence from English to French: I love programming."
    ),
]
print(chat(messages))
```

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/mlflow.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
