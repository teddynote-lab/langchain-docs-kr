---
title: Runpod
---

[RunPod](https://www.runpod.io/)는 AI 모델 배포 및 확장에 최적화된 Serverless endpoint를 포함한 GPU 클라우드 인프라를 제공합니다.

이 가이드는 `langchain-runpod` integration package를 사용하여 LangChain 애플리케이션을 [RunPod Serverless](https://www.runpod.io/serverless-gpu)에 호스팅된 모델에 연결하는 방법을 다룹니다.

이 integration은 표준 Language Model(LLM)과 Chat Model 모두에 대한 인터페이스를 제공합니다.

## 설치

전용 partner package를 설치하세요:

```python
pip install -qU langchain-runpod
```

## 설정

### 1. RunPod에서 Endpoint 배포하기

- [RunPod Serverless Console](https://www.runpod.io/console/serverless/user/endpoints)로 이동하세요.
- "New Endpoint"를 생성하고, 모델과 예상되는 입력/출력 형식에 호환되는 적절한 GPU와 template(예: vLLM, TGI, text-generation-webui)을 선택하세요(component 가이드 또는 package [README](https://github.com/runpod/langchain-runpod) 참조).
- 설정을 구성하고 배포하세요.
- **배포 후 Endpoint ID를 반드시 복사하세요**.

### 2. API 자격 증명 설정

이 integration은 RunPod API Key와 Endpoint ID가 필요합니다. 안전한 액세스를 위해 환경 변수로 설정하세요:

```python
import getpass
import os

os.environ["RUNPOD_API_KEY"] = getpass.getpass("Enter your RunPod API Key: ")
os.environ["RUNPOD_ENDPOINT_ID"] = input("Enter your RunPod Endpoint ID: ")
```

*(선택 사항)* LLM과 Chat model에 서로 다른 endpoint를 사용하는 경우, `RUNPOD_CHAT_ENDPOINT_ID`를 설정하거나 초기화 중에 ID를 직접 전달해야 할 수 있습니다.

## Component

이 package는 두 가지 주요 component를 제공합니다:

### 1. LLM

표준 텍스트 completion 모델과 상호작용하기 위한 component입니다.

자세한 사용법은 [RunPod LLM Integration 가이드](/oss/python/integrations/llms/runpod)를 참조하세요

```python
from langchain_runpod import RunPod

# Example initialization (uses environment variables)
llm = RunPod(model_kwargs={"max_new_tokens": 100})  # Add generation params here

# Example Invocation
try:
    response = llm.invoke("Write a short poem about the cloud.")
    print(response)
except Exception as e:
    print(
        f"Error invoking LLM: {e}. Ensure endpoint ID and API key are correct and endpoint is active."
    )
```

### 2. Chat Model

대화형 모델과 상호작용하기 위한 component입니다.

자세한 사용법과 기능 지원은 [RunPod Chat Model Integration 가이드](/oss/python/integrations/chat/runpod)를 참조하세요.

```python
from langchain.messages import HumanMessage
from langchain_runpod import ChatRunPod

# Example initialization (uses environment variables)
chat = ChatRunPod(model_kwargs={"temperature": 0.8})  # Add generation params here

# Example Invocation
try:
    response = chat.invoke(
        [HumanMessage(content="Explain RunPod Serverless in one sentence.")]
    )
    print(response.content)
except Exception as e:
    print(
        f"Error invoking Chat Model: {e}. Ensure endpoint ID and API key are correct and endpoint is active."
    )
```

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/runpod.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
