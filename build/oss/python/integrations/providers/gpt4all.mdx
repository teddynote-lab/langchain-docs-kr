---
title: GPT4All
---

이 페이지는 LangChain 내에서 `GPT4All` wrapper를 사용하는 방법을 다룹니다. 튜토리얼은 설치 및 설정, 그리고 예제를 통한 사용법의 두 부분으로 나뉩니다.

## 설치 및 설정

- `pip install gpt4all`로 Python package를 설치합니다
- [GPT4All model](https://gpt4all.io/index.html)을 다운로드하고 원하는 디렉토리에 배치합니다

이 예제에서는 `mistral-7b-openorca.Q4_0.gguf`를 사용합니다:

```bash
mkdir models
wget https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf -O models/mistral-7b-openorca.Q4_0.gguf
```

## 사용법

### GPT4All

GPT4All wrapper를 사용하려면 사전 학습된 model file의 경로와 model의 configuration을 제공해야 합니다.

```python
from langchain_community.llms import GPT4All

# Instantiate the model. Callbacks support token-wise streaming
model = GPT4All(model="./models/mistral-7b-openorca.Q4_0.gguf", n_threads=8)

# Generate text
response = model.invoke("Once upon a time, ")
```

`n_predict`, `temp`, `top_p`, `top_k` 등의 generation parameter를 사용자 정의할 수도 있습니다.

model의 prediction을 streaming하려면 CallbackManager를 추가하세요.

```python
from langchain_community.llms import GPT4All
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# There are many CallbackHandlers supported, such as
# from langchain.callbacks.streamlit import StreamlitCallbackHandler

callbacks = [StreamingStdOutCallbackHandler()]
model = GPT4All(model="./models/mistral-7b-openorca.Q4_0.gguf", n_threads=8)

# Generate text. Tokens are streamed through the callback manager.
model.invoke("Once upon a time, ", callbacks=callbacks)
```

## Model File

GPT4All client에서 model file을 다운로드할 수 있습니다. [GPT4All](https://gpt4all.io/index.html) 웹사이트에서 client를 다운로드할 수 있습니다.

더 자세한 안내는 [이 notebook](/oss/python/integrations/llms/gpt4all)을 참조하세요

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/gpt4all.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
