---
title: TruLens
---

>[TruLens](https://trulens.org)는 대규모 언어 모델(LLM) 기반 애플리케이션을 위한 계측 및 평가 도구를 제공하는 [오픈소스](https://github.com/truera/trulens) 패키지입니다.

이 페이지에서는 [TruLens](https://trulens.org)를 사용하여 langchain으로 구축된 LLM 앱을 평가하고 추적하는 방법을 다룹니다.


## Installation and Setup

`trulens-eval` python 패키지를 설치합니다.

<CodeGroup>
```bash pip
pip install trulens-eval
```

```bash uv
uv add trulens-eval
```
</CodeGroup>

## Quickstart

통합 세부 정보는 [TruLens 문서](https://www.trulens.org/trulens_eval/getting_started/quickstarts/langchain_quickstart/)를 참조하세요.

### Tracking

LLM chain을 생성한 후에는 TruLens를 사용하여 평가 및 추적을 수행할 수 있습니다.
TruLens는 다양한 [기본 제공 Feedback Function](https://www.trulens.org/trulens_eval/evaluation/feedback_functions/)을 제공하며,
LLM 평가를 위한 확장 가능한 프레임워크이기도 합니다.

Feedback Function을 생성합니다:

```python
from trulens_eval.feedback import Feedback, Huggingface,

# Initialize HuggingFace-based feedback function collection class:
hugs = Huggingface()
openai = OpenAI()

# Define a language match feedback function using HuggingFace.
lang_match = Feedback(hugs.language_match).on_input_output()
# By default this will check language match on the main app input and main app
# output.

# Question/answer relevance between overall question and answer.
qa_relevance = Feedback(openai.relevance).on_input_output()
# By default this will evaluate feedback on main app input and main app output.

# Toxicity of input
toxicity = Feedback(openai.toxicity).on_input()
```

### Chains

LLM을 평가하기 위한 Feedback Function을 설정한 후, TruChain으로 애플리케이션을 래핑하여
LLM 앱의 상세한 추적, 로깅 및 평가를 수행할 수 있습니다.

참고: `chain` 생성을 위한 코드는
[TruLens 문서](https://www.trulens.org/trulens_eval/getting_started/quickstarts/langchain_quickstart/)를 참조하세요.

```python
from trulens_eval import TruChain

# wrap your chain with TruChain
truchain = TruChain(
    chain,
    app_id='Chain1_ChatApplication',
    feedbacks=[lang_match, qa_relevance, toxicity]
)
# Note: any `feedbacks` specified here will be evaluated and logged whenever the chain is used.
truchain("que hora es?")
```

### Evaluation

이제 LLM 기반 애플리케이션을 탐색할 수 있습니다!

이를 통해 LLM 애플리케이션의 성능을 한눈에 파악할 수 있습니다. LLM 애플리케이션의 새 버전을 반복하면서 설정한 다양한 품질 메트릭 전반에 걸쳐 성능을 비교할 수 있습니다. 또한 레코드 수준에서 평가를 확인하고 각 레코드의 chain 메타데이터를 탐색할 수 있습니다.

```python
from trulens_eval import Tru

tru = Tru()
tru.run_dashboard() # open a Streamlit app to explore
```

TruLens에 대한 자세한 내용은 [trulens.org](https://www.trulens.org/)를 방문하세요.

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/trulens.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
