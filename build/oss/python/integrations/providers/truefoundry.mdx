---
title: TrueFoundry
---

TrueFoundry는 LangChain과 같은 에이전트 프레임워크에 거버넌스와 관찰성을 제공하는 엔터프라이즈급 [AI Gateway](https://www.truefoundry.com/ai-gateway)를 제공합니다. TrueFoundry AI Gateway는 LLM 액세스를 위한 통합 인터페이스 역할을 하며 다음을 제공합니다:

- **통합 API 액세스**: 하나의 API를 통해 250개 이상의 LLM(OpenAI, Claude, Gemini, Groq, Mistral)에 연결
- **낮은 지연 시간**: 지능형 라우팅 및 로드 밸런싱으로 3ms 미만의 내부 지연 시간
- **엔터프라이즈 보안**: RBAC 및 감사 로깅을 갖춘 SOC 2, HIPAA, GDPR 규정 준수
- **할당량 및 비용 관리**: 토큰 기반 할당량, 속도 제한 및 포괄적인 사용량 추적
- **관찰성**: 사용자 정의 가능한 보존 기간을 가진 전체 요청/응답 로깅, 메트릭 및 추적


## 사전 요구 사항

LangChain을 TrueFoundry와 통합하기 전에 다음을 확인하세요:

1. **TrueFoundry 계정**: 최소 하나의 모델 제공자가 구성된 [TrueFoundry 계정](https://www.truefoundry.com/register). [여기](https://docs.truefoundry.com/gateway/quick-start)에서 빠른 시작 가이드를 따르세요
2. **Personal Access Token**: [TrueFoundry 토큰 생성 가이드](https://docs.truefoundry.com/gateway/authentication)를 따라 토큰을 생성하세요

## 빠른 시작

[`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI/) 인터페이스를 통해 TrueFoundry의 통합 LLM gateway에 연결할 수 있습니다.

- `base_url`을 TrueFoundry endpoint로 설정하세요 (아래 설명)
- `api_key`를 TrueFoundry [PAT (Personal Access Token)](https://docs.truefoundry.com/gateway/authentication#personal-access-token-pat)로 설정하세요
- 통합 코드 스니펫에 표시된 것과 동일한 `model-name`을 사용하세요


### 설치

<CodeGroup>
```bash pip
pip install langchain-openai
```

```bash uv
uv add langchain-openai
```
</CodeGroup>

### 기본 설정

LangChain에서 [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI/) 모델을 업데이트하여 TrueFoundry에 연결하세요:

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    api_key=TRUEFOUNDRY_API_KEY,
    base_url=TRUEFOUNDRY_GATEWAY_BASE_URL,
    model="openai-main/gpt-4o"  # Similarly you can call any model from any model provider
)

llm.invoke("What is the meaning of life, universe and everything?")
```

요청은 TrueFoundry gateway를 통해 지정된 모델 제공자로 라우팅됩니다. TrueFoundry는 속도 제한, 로드 밸런싱 및 관찰성을 자동으로 처리합니다.

### LangGraph 통합


```python
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState
from langchain.messages import HumanMessage

# Define your LangGraph workflow
def call_model(state: MessagesState):
    model = ChatOpenAI(
        api_key=TRUEFOUNDRY_API_KEY,
        base_url=TRUEFOUNDRY_GATEWAY_BASE_URL,
        # Copy the exact model name from gateway
        model="openai-main/gpt-4o"
    )
    response = model.invoke(state["messages"])
    return {"messages": [response]}

# Build workflow
workflow = StateGraph(MessagesState)
workflow.add_node("agent", call_model)
workflow.set_entry_point("agent")
workflow.set_finish_point("agent")

app = workflow.compile()

# Run agent through TrueFoundry
result = app.invoke({"messages": [HumanMessage(content="Hello!")]})
```


## 관찰성 및 거버넌스

Metrics Dashboard를 통해 다음을 모니터링하고 분석할 수 있습니다:

- **성능 메트릭**: Request Latency, Time to First Token (TTFS), Inter-Token Latency (ITL)와 같은 주요 지연 시간 메트릭을 P99, P90, P50 백분위수로 추적
- **비용 및 토큰 사용량**: 각 모델에 대한 입력/출력 토큰 및 관련 비용의 상세한 분석을 통해 애플리케이션 비용에 대한 가시성 확보
- **사용 패턴**: 사용자 활동, 모델 분포 및 팀 기반 사용량에 대한 상세한 분석을 통해 애플리케이션 사용 방식 이해
- **속도 제한 및 로드 밸런싱**: 제한 구성, 모델 간 트래픽 분산 및 폴백 설정

## 지원

질문, 문제 또는 지원이 필요한 경우:

- **이메일**: [support@truefoundry.com](mailto:support@truefoundry.com)
- **문서**: [https://docs.truefoundry.com/](https://docs.truefoundry.com/)

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/truefoundry.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
