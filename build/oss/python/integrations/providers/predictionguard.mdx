---
title: Prediction Guard
---

이 페이지는 LangChain 내에서 Prediction Guard 생태계를 사용하는 방법을 다룹니다.
설치 및 설정, 그리고 특정 Prediction Guard wrapper에 대한 참조, 이렇게 두 부분으로 나뉩니다.

이 통합은 [langchain-predictionguard](https://github.com/predictionguard/langchain-predictionguard) 패키지에서 유지 관리됩니다.

## 설치 및 설정

- PredictionGuard LangChain partner 패키지를 설치합니다:

<CodeGroup>
```bash pip
pip install langchain-predictionguard
```

```bash uv
uv add langchain-predictionguard
```
</CodeGroup>

- Prediction Guard API key를 ([여기](https://docs.predictionguard.com/)에 설명된 대로) 받아서 환경 변수(`PREDICTIONGUARD_API_KEY`)로 설정합니다

## Prediction Guard LangChain 통합
|API|설명|Endpoint 문서| Import                                                  | 사용 예시                                                                 |
|---|---|---|---------------------------------------------------------|-------------------------------------------------------------------------------|
|Chat|Chat Bot 구축|[Chat](https://docs.predictionguard.com/api-reference/api-reference/chat-completions)| `from langchain_predictionguard import ChatPredictionGuard` | [ChatPredictionGuard.ipynb](/oss/python/integrations/chat/predictionguard)             |
|Completions|텍스트 생성|[Completions](https://docs.predictionguard.com/api-reference/api-reference/completions)| `from langchain_predictionguard import PredictionGuard` | [PredictionGuard.ipynb](/oss/python/integrations/llms/predictionguard)                     |
|Text Embedding|문자열을 벡터로 임베딩|[Embeddings](https://docs.predictionguard.com/api-reference/api-reference/embeddings)| `from langchain_predictionguard import PredictionGuardEmbeddings` | [PredictionGuardEmbeddings.ipynb](/oss/python/integrations/text_embedding/predictionguard) |

## 시작하기

## Chat Model

### Prediction Guard Chat

[사용 예시](/oss/python/integrations/chat/predictionguard)를 참조하세요

```python
from langchain_predictionguard import ChatPredictionGuard
```

#### 사용법

```python
# If predictionguard_api_key is not passed, default behavior is to use the `PREDICTIONGUARD_API_KEY` environment variable.
chat = ChatPredictionGuard(model="Hermes-3-Llama-3.1-8B")

chat.invoke("Tell me a joke")
```

## Embedding Model

### Prediction Guard Embedding

[사용 예시](/oss/python/integrations/text_embedding/predictionguard)를 참조하세요

```python
from langchain_predictionguard import PredictionGuardEmbeddings
```

#### 사용법
```python
# If predictionguard_api_key is not passed, default behavior is to use the `PREDICTIONGUARD_API_KEY` environment variable.
embeddings = PredictionGuardEmbeddings(model="bridgetower-large-itm-mlm-itc")

text = "This is an embedding example."
output = embeddings.embed_query(text)
```

## LLM

### Prediction Guard LLM

[사용 예시](/oss/python/integrations/llms/predictionguard)를 참조하세요

```python
from langchain_predictionguard import PredictionGuard
```

#### 사용법
```python
# If predictionguard_api_key is not passed, default behavior is to use the `PREDICTIONGUARD_API_KEY` environment variable.
llm = PredictionGuard(model="Hermes-2-Pro-Llama-3-8B")

llm.invoke("Tell me a joke about bears")
```
```

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/predictionguard.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
