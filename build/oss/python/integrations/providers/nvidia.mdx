---
title: NVIDIA
---

`langchain-nvidia-ai-endpoints` 패키지는 NVIDIA NIM inference microservice의 모델을 사용하여 애플리케이션을 구축하는 LangChain integration을 포함하고 있습니다. NIM은 커뮤니티와 NVIDIA의 chat, embedding, re-ranking 모델과 같은 다양한 도메인의 모델을 지원합니다. 이러한 모델들은 NVIDIA 가속 인프라에서 최고의 성능을 제공하도록 NVIDIA에 의해 최적화되었으며, NVIDIA 가속 인프라에서 단일 명령으로 어디서나 배포할 수 있는 사용하기 쉬운 사전 구축 컨테이너인 NIM으로 배포됩니다.

NVIDIA가 호스팅하는 NIM 배포는 [NVIDIA API catalog](https://build.nvidia.com/)에서 테스트할 수 있습니다. 테스트 후, NIM은 NVIDIA AI Enterprise 라이선스를 사용하여 NVIDIA의 API catalog에서 내보낼 수 있으며, 온프레미스 또는 클라우드에서 실행할 수 있어 기업이 IP와 AI 애플리케이션에 대한 소유권과 완전한 제어권을 가질 수 있습니다.

NIM은 모델별로 컨테이너 이미지로 패키징되며 NVIDIA NGC Catalog을 통해 NGC 컨테이너 이미지로 배포됩니다. 핵심적으로 NIM은 AI 모델에서 inference를 실행하기 위한 쉽고 일관되며 친숙한 API를 제공합니다.

다음은 text-generative 및 embedding 모델과 관련된 일반적인 기능을 사용하는 방법에 대한 예제입니다.

## Installation

```python
pip install -qU langchain-nvidia-ai-endpoints
```

## Setup

**시작하기:**

1. NVIDIA AI Foundation 모델을 호스팅하는 [NVIDIA](https://build.nvidia.com/)에서 무료 계정을 생성합니다.

2. 원하는 모델을 클릭합니다.

3. Input에서 Python 탭을 선택하고 `Get API Key`를 클릭합니다. 그런 다음 `Generate Key`를 클릭합니다.

4. 생성된 키를 NVIDIA_API_KEY로 복사하고 저장합니다. 그러면 endpoint에 액세스할 수 있습니다.

```python
import getpass
import os

if not os.environ.get("NVIDIA_API_KEY", "").startswith("nvapi-"):
    nvidia_api_key = getpass.getpass("Enter your NVIDIA API key: ")
    assert nvidia_api_key.startswith("nvapi-"), f"{nvidia_api_key[:5]}... is not a valid key"
    os.environ["NVIDIA_API_KEY"] = nvidia_api_key
```
## Working with NVIDIA API Catalog

```python
from langchain_nvidia_ai_endpoints import ChatNVIDIA

llm = ChatNVIDIA(model="mistralai/mixtral-8x22b-instruct-v0.1")
result = llm.invoke("Write a ballad about LangChain.")
print(result.content)
```

API를 사용하면 NVIDIA API Catalog에서 사용 가능한 live endpoint를 쿼리하여 DGX 호스팅 클라우드 컴퓨팅 환경에서 빠른 결과를 얻을 수 있습니다. 모든 모델은 소스에 액세스할 수 있으며 다음 섹션 [Working with NVIDIA NIMs](#working-with-nvidia-nims)에 표시된 NVIDIA AI Enterprise의 일부인 NVIDIA NIM을 사용하여 자체 컴퓨팅 클러스터에 배포할 수 있습니다.

## Working with NVIDIA NIMs
배포할 준비가 되면 NVIDIA AI Enterprise 소프트웨어 라이선스에 포함된 NVIDIA NIM으로 모델을 자체 호스팅하고 어디서나 실행할 수 있어 커스터마이징에 대한 소유권과 지적 재산권(IP) 및 AI 애플리케이션에 대한 완전한 제어권을 가질 수 있습니다.

[NIM에 대해 자세히 알아보기](https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/)

```python
from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings, NVIDIARerank

# connect to a chat NIM running at localhost:8000, specifying a model
llm = ChatNVIDIA(base_url="http://localhost:8000/v1", model="meta/llama3-8b-instruct")

# connect to an embedding NIM running at localhost:8080
embedder = NVIDIAEmbeddings(base_url="http://localhost:8080/v1")

# connect to a reranking NIM running at localhost:2016
ranker = NVIDIARerank(base_url="http://localhost:2016/v1")
```

## Using NVIDIA AI Foundation Endpoints

선별된 NVIDIA AI Foundation 모델들이 친숙한 API로 LangChain에서 직접 지원됩니다.

지원되는 활성 모델은 [API Catalog](https://build.nvidia.com/)에서 확인할 수 있습니다.

**다음은 시작하는 데 도움이 될 수 있는 유용한 예제입니다:**
- **[`ChatNVIDIA` Model](/oss/python/integrations/chat/nvidia_ai_endpoints).**
- **[`NVIDIAEmbeddings` Model for RAG Workflows](/oss/python/integrations/text_embedding/nvidia_ai_endpoints).**

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/nvidia.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
