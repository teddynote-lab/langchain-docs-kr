---
title: ApertureDB
---

[ApertureDB](https://docs.aperturedata.io)는 텍스트, 이미지, 비디오, bounding box, embedding과 같은 멀티모달 데이터를 관련 메타데이터와 함께 저장, 인덱싱 및 관리하는 데이터베이스입니다.

이 노트북은 ApertureDB의 embedding 기능을 사용하는 방법을 설명합니다.

## ApertureDB Python SDK 설치

ApertureDB용 클라이언트 코드를 작성하는 데 사용되는 [Python SDK](https://docs.aperturedata.io/category/aperturedb-python-sdk)를 설치합니다.

```python
pip install -qU aperturedb
```

```output
Note: you may need to restart the kernel to use updated packages.
```

## ApertureDB 인스턴스 실행

계속하려면 [ApertureDB 인스턴스가 실행 중](https://docs.aperturedata.io/HowToGuides/start/Setup)이어야 하며 이를 사용하도록 환경을 구성해야 합니다.
다양한 방법이 있으며, 예를 들면 다음과 같습니다:

```bash
docker run --publish 55555:55555 aperturedata/aperturedb-standalone
adb config create local --active --no-interactive
```

## 웹 문서 다운로드

여기서는 하나의 웹 페이지에 대한 미니 크롤링을 수행합니다.

```python
# For loading documents from web
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://docs.aperturedata.io")
docs = loader.load()
```

```output
USER_AGENT environment variable not set, consider setting it to identify your requests.
```

## Embedding 모델 선택

OllamaEmbeddings를 사용하려면 필요한 모듈을 import해야 합니다.

Ollama는 [문서](https://hub.docker.com/r/ollama/ollama)에 설명된 대로 docker container로 설정할 수 있습니다. 예를 들면:

```bash
# Run server
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
# Tell server to load a specific model
docker exec ollama ollama run llama2
```

```python
from langchain_community.embeddings import OllamaEmbeddings

embeddings = OllamaEmbeddings()
```

## 문서를 세그먼트로 분할

단일 문서를 여러 세그먼트로 나누려고 합니다.

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter()
documents = text_splitter.split_documents(docs)
```

## 문서와 embedding으로 vectorstore 생성

이 코드는 ApertureDB 인스턴스에 vectorstore를 생성합니다.
인스턴스 내에서 이 vectorstore는 "[descriptor set](https://docs.aperturedata.io/category/descriptorset-commands)"으로 표현됩니다.
기본적으로 descriptor set의 이름은 `langchain`입니다. 다음 코드는 각 문서에 대한 embedding을 생성하고 이를 descriptor로 ApertureDB에 저장합니다. embedding이 생성되는 동안 몇 초가 걸립니다.

```python
from langchain_community.vectorstores import ApertureDB

vector_db = ApertureDB.from_documents(documents, embeddings)
```

## Large language model 선택

다시 로컬 처리를 위해 설정한 Ollama 서버를 사용합니다.

```python
from langchain_community.llms import Ollama

llm = Ollama(model="llama2")
```

## RAG chain 구축

이제 RAG (Retrieval-Augmented Generation) chain을 생성하는 데 필요한 모든 구성 요소가 준비되었습니다. 이 chain은 다음을 수행합니다:

1. 사용자 쿼리에 대한 embedding descriptor 생성
2. Vector store를 사용하여 사용자 쿼리와 유사한 텍스트 세그먼트 찾기
3. Prompt template을 사용하여 사용자 쿼리와 context 문서를 LLM에 전달
4. LLM의 답변 반환

```python
# Create prompt
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("""Answer the following question based only on the provided context:

<context>
{context}
</context>

Question: {input}""")


# Create a chain that passes documents to an LLM
from langchain.chains.combine_documents import create_stuff_documents_chain

document_chain = create_stuff_documents_chain(llm, prompt)


# Treat the vectorstore as a document retriever
retriever = vector_db.as_retriever()


# Create a RAG chain that connects the retriever to the LLM
from langchain.chains import create_retrieval_chain

retrieval_chain = create_retrieval_chain(retriever, document_chain)
```

```output
Based on the provided context, ApertureDB can store images. In fact, it is specifically designed to manage multimodal data such as images, videos, documents, embeddings, and associated metadata including annotations. So, ApertureDB has the capability to store and manage images.
```

## RAG chain 실행

마지막으로 chain에 질문을 전달하고 답변을 받습니다. LLM이 쿼리와 context 문서로부터 답변을 생성하는 동안 몇 초가 걸립니다.

```python
user_query = "How can ApertureDB store images?"
response = retrieval_chain.invoke({"input": user_query})
print(response["answer"])
```

```output
Based on the provided context, ApertureDB can store images in several ways:

1. Multimodal data management: ApertureDB offers a unified interface to manage multimodal data such as images, videos, documents, embeddings, and associated metadata including annotations. This means that images can be stored along with other types of data in a single database instance.
2. Image storage: ApertureDB provides image storage capabilities through its integration with the public cloud providers or on-premise installations. This allows customers to host their own ApertureDB instances and store images on their preferred cloud provider or on-premise infrastructure.
3. Vector database: ApertureDB also offers a vector database that enables efficient similarity search and classification of images based on their semantic meaning. This can be useful for applications where image search and classification are important, such as in computer vision or machine learning workflows.

Overall, ApertureDB provides flexible and scalable storage options for images, allowing customers to choose the deployment model that best suits their needs.
```

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/vectorstores/aperturedb.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
