---
title: ZHIPU AI
---

이 노트북은 langchain.chat_models.ChatZhipuAI를 사용하여 LangChain에서 [ZHIPU AI API](https://open.bigmodel.cn/dev/api)를 사용하는 방법을 보여줍니다.

>[*GLM-4*](https://open.bigmodel.cn/)는 인간의 의도에 맞춰 정렬된 다국어 대규모 언어 모델로, Q&A, 다중 턴 대화, 코드 생성 기능을 갖추고 있습니다. 새로운 세대의 기본 모델 GLM-4는 이전 세대에 비해 전반적인 성능이 크게 향상되었으며, 더 긴 컨텍스트를 지원합니다. 더 강력한 멀티모달리티, 더 빠른 추론 속도와 더 많은 동시성을 지원하여 추론 비용을 크게 절감합니다. 또한 GLM-4는 지능형 에이전트의 기능을 강화했습니다.

## 시작하기

### 설치

먼저 Python 환경에 zhipuai 패키지가 설치되어 있는지 확인하세요. 다음 명령을 실행하세요:

```python
#!pip install -U httpx httpx-sse PyJWT
```

### 필요한 모듈 가져오기

설치 후 Python 스크립트에 필요한 모듈을 가져오세요:

```python
from langchain_community.chat_models import ChatZhipuAI
from langchain.messages import AIMessage, HumanMessage, SystemMessage
```

### API Key 설정하기

모델에 액세스하려면 [ZHIPU AI](https://open.bigmodel.cn/login?redirect=%2Fusercenter%2Fapikeys)에 로그인하여 API Key를 받으세요.

```python
import os

os.environ["ZHIPUAI_API_KEY"] = "zhipuai_api_key"
```

### ZHIPU AI Chat Model 초기화

다음은 chat model을 초기화하는 방법입니다:

```python
chat = ChatZhipuAI(
    model="glm-4",
    temperature=0.5,
)
```

### 기본 사용법

다음과 같이 system 및 human 메시지로 모델을 호출하세요:

```python
messages = [
    AIMessage(content="Hi."),
    SystemMessage(content="Your role is a poet."),
    HumanMessage(content="Write a short poem about AI in four lines."),
]
```

```python
response = chat.invoke(messages)
print(response.content)  # Displays the AI-generated poem
```

## 고급 기능

### Streaming 지원

지속적인 상호작용을 위해 streaming 기능을 사용하세요:

```python
from langchain_core.callbacks.manager import CallbackManager
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
```

```python
streaming_chat = ChatZhipuAI(
    model="glm-4",
    temperature=0.5,
    streaming=True,
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
)
```

```python
streaming_chat(messages)
```

### 비동기 호출

논블로킹 호출을 위해 비동기 방식을 사용하세요:

```python
async_chat = ChatZhipuAI(
    model="glm-4",
    temperature=0.5,
)
```

```python
response = await async_chat.agenerate([messages])
print(response)
```

### Functions Call과 함께 사용하기

GLM-4 모델은 function call과 함께 사용할 수도 있습니다. 다음 코드를 사용하여 간단한 LangChain json_chat_agent를 실행하세요.

```python
os.environ["TAVILY_API_KEY"] = "tavily_api_key"
```

```python
from langchain_classic import hub
from langchain.agents import AgentExecutor, create_json_chat_agent
from langchain_community.tools.tavily_search import TavilySearchResults

tools = [TavilySearchResults(max_results=1)]
prompt = hub.pull("hwchase17/react-chat-json")
llm = ChatZhipuAI(temperature=0.01, model="glm-4")

agent = create_json_chat_agent(llm, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True
)
```

```python
agent_executor.invoke({"input": "what is LangChain?"})
```

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/chat/zhipuai.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
