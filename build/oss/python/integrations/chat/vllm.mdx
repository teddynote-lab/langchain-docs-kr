---
title: vLLM Chat
---

vLLM은 OpenAI API 프로토콜을 모방하는 서버로 배포될 수 있습니다. 이를 통해 vLLM을 OpenAI API를 사용하는 애플리케이션의 드롭인 대체제로 사용할 수 있습니다. 이 서버는 OpenAI API와 동일한 형식으로 쿼리할 수 있습니다.

## Overview

이 가이드는 `langchain-openai` 패키지를 활용하는 vLLM [chat models](/oss/python/langchain/models)를 시작하는 데 도움이 됩니다. [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI/)의 모든 기능과 구성에 대한 자세한 문서는 [API reference](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)를 참조하세요.

### Integration details

| Class | Package | Local | Serializable | JS support | Downloads | Version |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [langchain_openai](https://python.langchain.com/api_reference/openai/) | ✅ | beta | ❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain_openai?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain_openai?style=flat-square&label=%20) |

### Model features

tool calling, multi-modal input 지원, token-level streaming 지원 등과 같은 특정 모델 기능은 호스팅되는 모델에 따라 달라집니다.

## Setup

vLLM 문서는 [여기](https://docs.vllm.ai/en/latest/)를 참조하세요.

LangChain을 통해 vLLM 모델에 액세스하려면 `langchain-openai` integration package를 설치해야 합니다.

### Credentials

인증은 inference server의 세부 사항에 따라 달라집니다.

모델 호출의 자동 추적을 활성화하려면 [LangSmith](https://docs.smith.langchain.com/) API key를 설정하세요:

```python
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
```

### Installation

LangChain vLLM integration은 `langchain-openai` 패키지를 통해 액세스할 수 있습니다:

```python
pip install -qU langchain-openai
```

## Instantiation

이제 model object를 인스턴스화하고 chat completion을 생성할 수 있습니다:

```python
from langchain.messages import HumanMessage, SystemMessage
from langchain_core.prompts.chat import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
)
from langchain_openai import ChatOpenAI
```

```python
inference_server_url = "http://localhost:8000/v1"

llm = ChatOpenAI(
    model="mosaicml/mpt-7b",
    openai_api_key="EMPTY",
    openai_api_base=inference_server_url,
    max_tokens=5,
    temperature=0,
)
```

## Invocation

```python
messages = [
    SystemMessage(
        content="You are a helpful assistant that translates English to Italian."
    ),
    HumanMessage(
        content="Translate the following sentence from English to Italian: I love programming."
    ),
]
llm.invoke(messages)
```

```output
AIMessage(content=' Io amo programmare', additional_kwargs={}, example=False)
```

## API reference

`langchain-openai`를 통해 노출되는 모든 기능과 구성에 대한 자세한 문서는 API reference를 참조하세요: [python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

vLLM [documentation](https://docs.vllm.ai/en/latest/)도 참조하세요.

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/chat/vllm.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
