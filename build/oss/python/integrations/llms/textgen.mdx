---
title: TextGen
---

[GitHub:oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) LLaMA, llama.cpp, GPT-J, Pythia, OPT, GALACTICA와 같은 대규모 언어 모델을 실행하기 위한 gradio web UI입니다.

이 예제는 `text-generation-webui` API 통합을 통해 LLM 모델과 상호작용하기 위해 LangChain을 사용하는 방법을 다룹니다.

`text-generation-webui`가 구성되어 있고 LLM이 설치되어 있는지 확인하세요. 사용 중인 OS에 [적합한 원클릭 설치 프로그램](https://github.com/oobabooga/text-generation-webui#one-click-installers)을 통한 설치를 권장합니다.

`text-generation-webui`가 설치되고 웹 인터페이스를 통해 정상 작동이 확인되면, 웹 모델 구성 탭을 통해 `api` 옵션을 활성화하거나 시작 명령에 런타임 인자 `--api`를 추가하세요.

## model_url 설정 및 예제 실행

```python
model_url = "http://localhost:5000"
```

```python
from langchain.chains import LLMChain
from langchain.globals import set_debug
from langchain_community.llms import TextGen
from langchain_core.prompts import PromptTemplate

set_debug(True)

template = """Question: {question}

Answer: Let's think step by step."""


prompt = PromptTemplate.from_template(template)
llm = TextGen(model_url=model_url)
llm_chain = LLMChain(prompt=prompt, llm=llm)
question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"

llm_chain.run(question)
```

### Streaming 버전

이 기능을 사용하려면 websocket-client를 설치해야 합니다.
`pip install websocket-client`

```python
model_url = "ws://localhost:5005"
```

```python
from langchain.chains import LLMChain
from langchain.globals import set_debug
from langchain_community.llms import TextGen
from langchain_core.callbacks import StreamingStdOutCallbackHandler
from langchain_core.prompts import PromptTemplate

set_debug(True)

template = """Question: {question}

Answer: Let's think step by step."""


prompt = PromptTemplate.from_template(template)
llm = TextGen(
    model_url=model_url, streaming=True, callbacks=[StreamingStdOutCallbackHandler()]
)
llm_chain = LLMChain(prompt=prompt, llm=llm)
question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"

llm_chain.run(question)
```

```python
llm = TextGen(model_url=model_url, streaming=True)
for chunk in llm.stream("Ask 'Hi, how are you?' like a pirate:'", stop=["'", "\n"]):
    print(chunk, end="", flush=True)
```

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/llms/textgen.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
