---
title: Modal
---

[Modal cloud platform](https://modal.com/docs/guide)은 로컬 컴퓨터의 Python 스크립트에서 서버리스 클라우드 컴퓨팅에 편리하게 온디맨드로 액세스할 수 있는 기능을 제공합니다.
`modal`을 사용하여 LLM API에 의존하는 대신 자체 커스텀 LLM 모델을 실행할 수 있습니다.

이 예제는 LangChain을 사용하여 `modal` HTTPS [web endpoint](https://modal.com/docs/guide/webhooks)와 상호작용하는 방법을 다룹니다.

[_Question-answering with LangChain_](https://modal.com/docs/guide/ex/potus_speech_qanda)은 `Modal`과 함께 LangChain을 사용하는 또 다른 예제입니다. 해당 예제에서는 Modal이 LangChain 애플리케이션을 엔드투엔드로 실행하고 OpenAI를 LLM API로 사용합니다.

```python
pip install -qU  modal
```

```python
# Register an account with Modal and get a new token.

!modal token new
```

```output
Launching login page in your browser window...
If this is not showing up, please copy this URL into your web browser manually:
https://modal.com/token-flow/tf-Dzm3Y01234mqmm1234Vcu3
```

[`langchain.llms.modal.Modal`](https://github.com/langchain-ai/langchain/blame/master/langchain/llms/modal.py) integration class는 다음 JSON 인터페이스를 준수하는 web endpoint가 있는 Modal 애플리케이션을 배포해야 합니다:

1. LLM prompt는 `"prompt"` 키 아래에 `str` 값으로 전달됩니다
2. LLM response는 `"prompt"` 키 아래에 `str` 값으로 반환됩니다

**요청 JSON 예제:**

```json
{
    "prompt": "Identify yourself, bot!",
    "extra": "args are allowed",
}
```

**응답 JSON 예제:**

```json
{
    "prompt": "This is the LLM speaking",
}
```

이 인터페이스를 충족하는 'dummy' Modal web endpoint 함수 예제는 다음과 같습니다

```python
...
...

class Request(BaseModel):
    prompt: str

@stub.function()
@modal.web_endpoint(method="POST")
def web(request: Request):
    _ = request  # ignore input
    return {"prompt": "hello world"}
```

* 이 인터페이스를 충족하는 endpoint 설정의 기본 사항은 Modal의 [web endpoints](https://modal.com/docs/guide/webhooks#passing-arguments-to-web-endpoints) 가이드를 참조하세요.
* 커스텀 LLM의 시작점으로 Modal의 ['Run Falcon-40B with AutoGPTQ'](https://modal.com/docs/guide/ex/falcon_gptq) 오픈소스 LLM 예제를 참조하세요!

배포된 Modal web endpoint가 있으면 해당 URL을 `langchain.llms.modal.Modal` LLM class에 전달할 수 있습니다. 이 class는 chain의 구성 요소로 기능할 수 있습니다.

```python
from langchain.chains import LLMChain
from langchain_community.llms import Modal
from langchain_core.prompts import PromptTemplate
```

```python
template = """Question: {question}

Answer: Let's think step by step."""

prompt = PromptTemplate.from_template(template)
```

```python
endpoint_url = "https://ecorp--custom-llm-endpoint.modal.run"  # REPLACE ME with your deployed Modal web endpoint's URL
llm = Modal(endpoint_url=endpoint_url)
```

```python
llm_chain = LLMChain(prompt=prompt, llm=llm)
```

```python
question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"

llm_chain.run(question)
```

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/llms/modal.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
