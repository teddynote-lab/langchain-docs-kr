---
title: Hyperbrowser 웹 스크래핑 도구
---

[Hyperbrowser](https://hyperbrowser.ai)는 headless 브라우저를 실행하고 확장하기 위한 플랫폼입니다. 대규모로 브라우저 세션을 시작하고 관리할 수 있으며, 단일 페이지 스크래핑이나 전체 사이트 크롤링과 같은 모든 웹 스크래핑 요구사항에 대해 사용하기 쉬운 솔루션을 제공합니다.

주요 기능:

- 즉각적인 확장성 - 인프라 문제 없이 수백 개의 브라우저 세션을 몇 초 만에 시작
- 간편한 통합 - Puppeteer 및 Playwright와 같은 인기 있는 도구와 원활하게 작동
- 강력한 API - 모든 사이트를 스크래핑/크롤링하기 위한 사용하기 쉬운 API 등
- 봇 방지 조치 우회 - 내장된 스텔스 모드, 광고 차단, 자동 CAPTCHA 해결 및 순환 프록시

이 가이드는 Hyperbrowser 웹 도구를 시작하기 위한 빠른 개요를 제공합니다.

Hyperbrowser에 대한 자세한 내용은 [Hyperbrowser 웹사이트](https://hyperbrowser.ai)를 방문하거나, 문서를 확인하려면 [Hyperbrowser 문서](https://docs.hyperbrowser.ai)를 방문하세요.

## 주요 기능

### Scrape

Hyperbrowser는 모든 웹페이지에서 데이터를 추출할 수 있는 강력한 스크래핑 기능을 제공합니다. 스크래핑 도구는 웹 콘텐츠를 markdown이나 HTML과 같은 구조화된 형식으로 변환하여 데이터를 쉽게 처리하고 분석할 수 있습니다.

### Crawl

크롤링 기능을 사용하면 웹사이트의 여러 페이지를 자동으로 탐색할 수 있습니다. 페이지 제한과 같은 매개변수를 설정하여 크롤러가 사이트를 얼마나 광범위하게 탐색할지 제어하고, 방문하는 각 페이지에서 데이터를 수집할 수 있습니다.

### Extract

Hyperbrowser의 추출 기능은 AI를 사용하여 정의된 스키마에 따라 웹페이지에서 특정 정보를 추출합니다. 이를 통해 구조화되지 않은 웹 콘텐츠를 정확한 요구사항에 맞는 구조화된 데이터로 변환할 수 있습니다.

## 개요

### 통합 세부정보

| Tool         | Package                | Local | Serializable | JS support |
| :----------- | :--------------------- | :---: | :----------: | :--------: |
| Crawl Tool   | langchain-hyperbrowser |  ❌   |      ❌      |     ❌     |
| Scrape Tool  | langchain-hyperbrowser |  ❌   |      ❌      |     ❌     |
| Extract Tool | langchain-hyperbrowser |  ❌   |      ❌      |     ❌     |

## 설정

Hyperbrowser 웹 도구에 액세스하려면 `langchain-hyperbrowser` 통합 패키지를 설치하고, Hyperbrowser 계정을 생성하여 API 키를 받아야 합니다.

### 자격 증명

[Hyperbrowser](https://app.hyperbrowser.ai/)로 이동하여 가입하고 API 키를 생성하세요. 완료되면 HYPERBROWSER_API_KEY 환경 변수를 설정하세요:

```bash
export HYPERBROWSER_API_KEY=<your-api-key>
```

### 설치

**langchain-hyperbrowser**를 설치하세요.

```python
pip install -qU langchain-hyperbrowser
```

## 인스턴스화

### Crawl Tool

`HyperbrowserCrawlTool`은 주어진 URL에서 시작하여 전체 웹사이트를 크롤링할 수 있는 강력한 도구입니다. 구성 가능한 페이지 제한 및 스크래핑 옵션을 지원합니다.

```python
from langchain_hyperbrowser import HyperbrowserCrawlTool
tool = HyperbrowserCrawlTool()
```

### Scrape Tool

`HyperbrowserScrapeTool`은 웹 페이지에서 콘텐츠를 스크래핑할 수 있는 도구입니다. markdown 및 HTML 출력 형식과 메타데이터 추출을 지원합니다.

```python
from langchain_hyperbrowser import HyperbrowserScrapeTool
tool = HyperbrowserScrapeTool()
```

### Extract Tool

`HyperbrowserExtractTool`은 AI를 사용하여 웹 페이지에서 구조화된 데이터를 추출하는 강력한 도구입니다. 사전 정의된 스키마를 기반으로 정보를 추출할 수 있습니다.

```python
from langchain_hyperbrowser import HyperbrowserExtractTool
tool = HyperbrowserExtractTool()
```

## 호출

### 기본 사용법

#### Crawl Tool

```python
from langchain_hyperbrowser import HyperbrowserCrawlTool

result = HyperbrowserCrawlTool().invoke(
    {
        "url": "https://example.com",
        "max_pages": 2,
        "scrape_options": {"formats": ["markdown"]},
    }
)
print(result)
```

```output
{'data': [CrawledPage(metadata={'url': 'https://www.example.com/', 'title': 'Example Domain', 'viewport': 'width=device-width, initial-scale=1', 'sourceURL': 'https://example.com'}, html=None, markdown='Example Domain\n\n# Example Domain\n\nThis domain is for use in illustrative examples in documents. You may use this\ndomain in literature without prior coordination or asking for permission.\n\n[More information...](https://www.iana.org/domains/example)', links=None, screenshot=None, url='https://example.com', status='completed', error=None)], 'error': None}
```

#### Scrape Tool

```python
from langchain_hyperbrowser import HyperbrowserScrapeTool

result = HyperbrowserScrapeTool().invoke(
    {"url": "https://example.com", "scrape_options": {"formats": ["markdown"]}}
)
print(result)
```

```output
{'data': ScrapeJobData(metadata={'url': 'https://www.example.com/', 'title': 'Example Domain', 'viewport': 'width=device-width, initial-scale=1', 'sourceURL': 'https://example.com'}, html=None, markdown='Example Domain\n\n# Example Domain\n\nThis domain is for use in illustrative examples in documents. You may use this\ndomain in literature without prior coordination or asking for permission.\n\n[More information...](https://www.iana.org/domains/example)', links=None, screenshot=None), 'error': None}
```

#### Extract Tool

```python
from langchain_hyperbrowser import HyperbrowserExtractTool
from pydantic import BaseModel


class SimpleExtractionModel(BaseModel):
    title: str


result = HyperbrowserExtractTool().invoke(
    {
        "url": "https://example.com",
        "schema": SimpleExtractionModel,
    }
)
print(result)
```

```output
{'data': {'title': 'Example Domain'}, 'error': None}
```

### 사용자 정의 옵션 사용

#### 사용자 정의 옵션을 사용한 Crawl Tool

```python
result = HyperbrowserCrawlTool().run(
    {
        "url": "https://example.com",
        "max_pages": 2,
        "scrape_options": {
            "formats": ["markdown", "html"],
        },
        "session_options": {"use_proxy": True, "solve_captchas": True},
    }
)
print(result)
```

```output
{'data': [CrawledPage(metadata={'url': 'https://www.example.com/', 'title': 'Example Domain', 'viewport': 'width=device-width, initial-scale=1', 'sourceURL': 'https://example.com'}, html=None, markdown='Example Domain\n\n# Example Domain\n\nThis domain is for use in illustrative examples in documents. You may use this\ndomain in literature without prior coordination or asking for permission.\n\n[More information...](https://www.iana.org/domains/example)', links=None, screenshot=None, url='https://example.com', status='completed', error=None)], 'error': None}
```

#### 사용자 정의 옵션을 사용한 Scrape Tool

```python
result = HyperbrowserScrapeTool().run(
    {
        "url": "https://example.com",
        "scrape_options": {
            "formats": ["markdown", "html"],
        },
        "session_options": {"use_proxy": True, "solve_captchas": True},
    }
)
print(result)
```

```output
{'data': ScrapeJobData(metadata={'url': 'https://www.example.com/', 'title': 'Example Domain', 'viewport': 'width=device-width, initial-scale=1', 'sourceURL': 'https://example.com'}, html='<html><head>\n    <title>Example Domain</title>\n\n    <meta charset="utf-8">\n    <meta http-equiv="Content-type" content="text/html; charset=utf-8">\n    <meta name="viewport" content="width=device-width, initial-scale=1">\n        \n</head>\n\n<body>\n<div>\n    <h1>Example Domain</h1>\n    <p>This domain is for use in illustrative examples in documents. You may use this\n    domain in literature without prior coordination or asking for permission.</p>\n    <p><a href="https://www.iana.org/domains/example">More information...</a></p>\n</div>\n\n\n</body></html>', markdown='Example Domain\n\n# Example Domain\n\nThis domain is for use in illustrative examples in documents. You may use this\ndomain in literature without prior coordination or asking for permission.\n\n[More information...](https://www.iana.org/domains/example)', links=None, screenshot=None), 'error': None}
```

#### 사용자 정의 스키마를 사용한 Extract Tool

```python
from typing import List

from pydantic import BaseModel


class ProductSchema(BaseModel):
    title: str
    price: float


class ProductsSchema(BaseModel):
    products: List[ProductSchema]


result = HyperbrowserExtractTool().run(
    {
        "url": "https://dummyjson.com/products?limit=10",
        "schema": ProductsSchema,
        "session_options": {"session_options": {"use_proxy": True}},
    }
)
print(result)
```

```output
{'data': {'products': [{'price': 9.99, 'title': 'Essence Mascara Lash Princess'}, {'price': 19.99, 'title': 'Eyeshadow Palette with Mirror'}, {'price': 14.99, 'title': 'Powder Canister'}, {'price': 12.99, 'title': 'Red Lipstick'}, {'price': 8.99, 'title': 'Red Nail Polish'}, {'price': 49.99, 'title': 'Calvin Klein CK One'}, {'price': 129.99, 'title': 'Chanel Coco Noir Eau De'}, {'price': 89.99, 'title': "Dior J'adore"}, {'price': 69.99, 'title': 'Dolce Shine Eau de'}, {'price': 79.99, 'title': 'Gucci Bloom Eau de'}]}, 'error': None}
```

### 비동기 사용

모든 도구는 비동기 사용을 지원합니다:

```python
from typing import List

from langchain_hyperbrowser import (
    HyperbrowserCrawlTool,
    HyperbrowserExtractTool,
    HyperbrowserScrapeTool,
)
from pydantic import BaseModel


class ExtractionSchema(BaseModel):
    popular_library_name: List[str]


async def web_operations():
    # Crawl
    crawl_tool = HyperbrowserCrawlTool()
    crawl_result = await crawl_tool.arun(
        {
            "url": "https://example.com",
            "max_pages": 5,
            "scrape_options": {"formats": ["markdown"]},
        }
    )

    # Scrape
    scrape_tool = HyperbrowserScrapeTool()
    scrape_result = await scrape_tool.arun(
        {"url": "https://example.com", "scrape_options": {"formats": ["markdown"]}}
    )

    # Extract
    extract_tool = HyperbrowserExtractTool()
    extract_result = await extract_tool.arun(
        {
            "url": "https://npmjs.com",
            "schema": ExtractionSchema,
        }
    )

    return crawl_result, scrape_result, extract_result


results = await web_operations()
print(results)
```

```output
---------------------------------------------------------------------------
```
```output
NameError                                 Traceback (most recent call last)
```
```output
Cell In[6], line 10
      1 from langchain_hyperbrowser import (
      2     HyperbrowserCrawlTool,
      3     HyperbrowserExtractTool,
      4     HyperbrowserScrapeTool,
      5 )
      7 from pydantic import BaseModel
---> 10 class ExtractionSchema(BaseModel):
     11     popular_library_name: List[str]
     14 async def web_operations():
     15     # Crawl
```
```output
Cell In[6], line 11, in ExtractionSchema()
     10 class ExtractionSchema(BaseModel):
---> 11     popular_library_name: List[str]
```
```output
NameError: name 'List' is not defined
```

## Agent 내에서 사용

다음은 agent 내에서 웹 도구를 사용하는 방법입니다:

```python
from langchain_hyperbrowser import HyperbrowserCrawlTool
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent


# Initialize the crawl tool
crawl_tool = HyperbrowserCrawlTool()

# Create the agent with the crawl tool
model = ChatOpenAI(temperature=0)

agent = create_agent(model, [crawl_tool])
user_input = "Crawl https://example.com and get content from up to 5 pages"
for step in agent.stream(
    {"messages": user_input},
    stream_mode="values",
):
    step["messages"][-1].pretty_print()
```

```output
================================ Human Message =================================

Crawl https://example.com and get content from up to 5 pages
================================== Ai Message ==================================
Tool Calls:
  hyperbrowser_crawl_data (call_G2ofdHOqjdnJUZu4hhbuga58)
 Call ID: call_G2ofdHOqjdnJUZu4hhbuga58
  Args:
    url: https://example.com
    max_pages: 5
    scrape_options: {'formats': ['markdown']}
================================= Tool Message =================================
Name: hyperbrowser_crawl_data

{'data': [CrawledPage(metadata={'url': 'https://www.example.com/', 'title': 'Example Domain', 'viewport': 'width=device-width, initial-scale=1', 'sourceURL': 'https://example.com'}, html=None, markdown='Example Domain\n\n# Example Domain\n\nThis domain is for use in illustrative examples in documents. You may use this\ndomain in literature without prior coordination or asking for permission.\n\n[More information...](https://www.iana.org/domains/example)', links=None, screenshot=None, url='https://example.com', status='completed', error=None)], 'error': None}
================================== Ai Message ==================================

I have crawled the website [https://example.com](https://example.com) and retrieved content from the first page. Here is the content in markdown format:

\`\`\`
Example Domain

# Example Domain

This domain is for use in illustrative examples in documents. You may use this
domain in literature without prior coordination or asking for permission.

[More information...](https://www.iana.org/domains/example)
\`\`\`

If you would like to crawl more pages or need additional information, please let me know!
```

## 구성 옵션

### 공통 옵션

모든 도구는 다음과 같은 기본 구성 옵션을 지원합니다:

- `url`: 처리할 URL
- `session_options`: 브라우저 세션 구성
  - `use_proxy`: 프록시 사용 여부
  - `solve_captchas`: CAPTCHA 자동 해결 여부
  - `accept_cookies`: 쿠키 수락 여부

### 도구별 옵션

#### Crawl Tool

- `max_pages`: 크롤링할 최대 페이지 수
- `scrape_options`: 각 페이지 스크래핑 옵션
  - `formats`: 출력 형식 목록 (markdown, html)

#### Scrape Tool

- `scrape_options`: 페이지 스크래핑 옵션
  - `formats`: 출력 형식 목록 (markdown, html)

#### Extract Tool

- `schema`: 추출할 구조를 정의하는 Pydantic 모델
- `extraction_prompt`: 추출을 위한 자연어 프롬프트

자세한 내용은 각 API 참조를 확인하세요:

- [Crawl API Reference](https://docs.hyperbrowser.ai/reference/api-reference/crawl)
- [Scrape API Reference](https://docs.hyperbrowser.ai/reference/api-reference/scrape)
- [Extract API Reference](https://docs.hyperbrowser.ai/reference/api-reference/extract)

## API reference

- [GitHub](https://github.com/hyperbrowserai/langchain-hyperbrowser/)
- [PyPi](https://pypi.org/project/langchain-hyperbrowser/)
- [Hyperbrowser Docs](https://docs.hyperbrowser.ai/)

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/tools/hyperbrowser_web_scraping_tools.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
