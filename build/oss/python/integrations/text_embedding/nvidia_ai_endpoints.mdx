---
title: NVIDIA NIMs
---

`langchain-nvidia-ai-endpoints` 패키지는 NVIDIA NIM inference microservice의 모델을 사용하여 애플리케이션을 구축하는 LangChain integration을 포함하고 있습니다. NIM은 커뮤니티와 NVIDIA의 chat, embedding, re-ranking 모델과 같은 다양한 도메인의 모델을 지원합니다. 이러한 모델들은 NVIDIA 가속 인프라에서 최고의 성능을 제공하도록 NVIDIA에 의해 최적화되었으며, NVIDIA 가속 인프라에서 단일 명령으로 어디서나 배포할 수 있는 사용하기 쉬운 사전 구축 컨테이너인 NIM으로 배포됩니다.

NVIDIA가 호스팅하는 NIM 배포는 [NVIDIA API catalog](https://build.nvidia.com/)에서 테스트할 수 있습니다. 테스트 후, NIM은 NVIDIA AI Enterprise 라이선스를 사용하여 NVIDIA의 API catalog에서 내보낼 수 있으며 온프레미스 또는 클라우드에서 실행할 수 있어, 기업이 자신의 IP와 AI 애플리케이션에 대한 소유권과 완전한 제어권을 가질 수 있습니다.

NIM은 모델별로 컨테이너 이미지로 패키징되며 NVIDIA NGC Catalog을 통해 NGC 컨테이너 이미지로 배포됩니다. 핵심적으로 NIM은 AI 모델에서 inference를 실행하기 위한 쉽고 일관되며 친숙한 API를 제공합니다.

이 예제는 `NVIDIAEmbeddings` 클래스를 통해 [retrieval-augmented generation](https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/)을 위한 지원되는 [NVIDIA Retrieval QA Embedding Model](https://build.nvidia.com/nvidia/embed-qa-4)과 상호작용하기 위해 LangChain을 사용하는 방법을 다룹니다.

이 API를 통해 chat 모델에 액세스하는 방법에 대한 자세한 내용은 [ChatNVIDIA](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/) 문서를 확인하세요.

## Installation

```python
pip install -qU  langchain-nvidia-ai-endpoints
```

## Setup

**시작하기:**

1. NVIDIA AI Foundation 모델을 호스팅하는 [NVIDIA](https://build.nvidia.com/)에서 무료 계정을 생성하세요.

2. `Retrieval` 탭을 선택한 다음 원하는 모델을 선택하세요.

3. `Input`에서 `Python` 탭을 선택하고 `Get API Key`를 클릭하세요. 그런 다음 `Generate Key`를 클릭하세요.

4. 생성된 키를 `NVIDIA_API_KEY`로 복사하고 저장하세요. 그러면 endpoint에 액세스할 수 있습니다.

```python
import getpass
import os

# del os.environ['NVIDIA_API_KEY']  ## delete key and reset
if os.environ.get("NVIDIA_API_KEY", "").startswith("nvapi-"):
    print("Valid NVIDIA_API_KEY already in environment. Delete to reset")
else:
    nvapi_key = getpass.getpass("NVAPI Key (starts with nvapi-): ")
    assert nvapi_key.startswith("nvapi-"), f"{nvapi_key[:5]}... is not a valid key"
    os.environ["NVIDIA_API_KEY"] = nvapi_key
```

해당 목록에서 효과적인 RAG 솔루션을 위해 LLM과 함께 사용할 수 있는 embedding 모델을 볼 수 있습니다. `NVIDIAEmbeddings` 클래스를 통해 이 모델과 NIM이 지원하는 다른 embedding 모델과 인터페이스할 수 있습니다.

## NVIDIA API Catalog의 NIM 작업하기

embedding 모델을 초기화할 때 아래의 `NV-Embed-QA`와 같이 모델을 전달하여 선택하거나, 인수를 전달하지 않고 기본값을 사용할 수 있습니다.

```python
from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings

embedder = NVIDIAEmbeddings(model="NV-Embed-QA")
```

이 모델은 다음과 같은 예상되는 [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) 메서드를 지원하는 fine-tuned E5-large 모델입니다:

- `embed_query`: query 샘플에 대한 query embedding을 생성합니다.

- `embed_documents`: 검색하려는 문서 목록에 대한 passage embedding을 생성합니다.

- `aembed_query`/`aembed_documents`: 위의 비동기 버전입니다.

## 자체 호스팅 NVIDIA NIM 작업하기

배포할 준비가 되면 NVIDIA AI Enterprise 소프트웨어 라이선스에 포함된 NVIDIA NIM으로 모델을 자체 호스팅하고 어디서나 실행할 수 있어, 커스터마이징에 대한 소유권과 지적 재산권(IP) 및 AI 애플리케이션에 대한 완전한 제어권을 가질 수 있습니다.

[NIM에 대해 자세히 알아보기](https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/)

```python
from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings

# connect to an embedding NIM running at localhost:8080
embedder = NVIDIAEmbeddings(base_url="http://localhost:8080/v1")
```

### **Similarity**

다음은 이러한 데이터 포인트에 대한 유사성의 간단한 테스트입니다:

**Queries:**

- What's the weather like in Komchatka?

- What kinds of food is Italy known for?

- What's my name? I bet you don't remember...

- What's the point of life anyways?

- The point of life is to have fun :D

**Documents:**

- Komchatka's weather is cold, with long, severe winters.

- Italy is famous for pasta, pizza, gelato, and espresso.

- I can't recall personal names, only provide information.

- Life's purpose varies, often seen as personal fulfillment.

- Enjoying life's moments is indeed a wonderful approach.

### Embedding Runtimes

```python
print("\nSequential Embedding: ")
q_embeddings = [
    embedder.embed_query("What's the weather like in Komchatka?"),
    embedder.embed_query("What kinds of food is Italy known for?"),
    embedder.embed_query("What's my name? I bet you don't remember..."),
    embedder.embed_query("What's the point of life anyways?"),
    embedder.embed_query("The point of life is to have fun :D"),
]
print("Shape:", (len(q_embeddings), len(q_embeddings[0])))
```

### Document Embedding

```python
print("\nBatch Document Embedding: ")
d_embeddings = embedder.embed_documents(
    [
        "Komchatka's weather is cold, with long, severe winters.",
        "Italy is famous for pasta, pizza, gelato, and espresso.",
        "I can't recall personal names, only provide information.",
        "Life's purpose varies, often seen as personal fulfillment.",
        "Enjoying life's moments is indeed a wonderful approach.",
    ]
)
print("Shape:", (len(q_embeddings), len(q_embeddings[0])))
```

이제 embedding을 생성했으므로 결과에 대한 간단한 유사성 검사를 수행하여 retrieval 작업에서 어떤 문서가 합리적인 답변으로 트리거되었을지 확인할 수 있습니다:

```python
pip install -qU  matplotlib scikit-learn
```

```python
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Compute the similarity matrix between q_embeddings and d_embeddings
cross_similarity_matrix = cosine_similarity(
    np.array(q_embeddings),
    np.array(d_embeddings),
)

# Plotting the cross-similarity matrix
plt.figure(figsize=(8, 6))
plt.imshow(cross_similarity_matrix, cmap="Greens", interpolation="nearest")
plt.colorbar()
plt.title("Cross-Similarity Matrix")
plt.xlabel("Query Embeddings")
plt.ylabel("Document Embeddings")
plt.grid(True)
plt.show()
```

참고로, 시스템에 전송된 query와 document는 다음과 같습니다:

**Queries:**

- What's the weather like in Komchatka?

- What kinds of food is Italy known for?

- What's my name? I bet you don't remember...

- What's the point of life anyways?

- The point of life is to have fun :D

**Documents:**

- Komchatka's weather is cold, with long, severe winters.

- Italy is famous for pasta, pizza, gelato, and espresso.

- I can't recall personal names, only provide information.

- Life's purpose varies, often seen as personal fulfillment.

- Enjoying life's moments is indeed a wonderful approach.

## Truncation

Embedding 모델은 일반적으로 embedding할 수 있는 최대 입력 token 수를 결정하는 고정된 context window를 가지고 있습니다. 이 제한은 모델의 최대 입력 token 길이와 동일한 하드 제한이거나, embedding의 정확도가 감소하는 효과적인 제한일 수 있습니다.

모델은 token으로 작동하고 애플리케이션은 일반적으로 텍스트로 작업하기 때문에, 애플리케이션이 입력이 모델의 token 제한 내에 있는지 확인하는 것이 어려울 수 있습니다. 기본적으로 입력이 너무 크면 예외가 발생합니다.

이를 지원하기 위해 NVIDIA의 NIM(API Catalog 또는 로컬)은 입력이 너무 큰 경우 서버 측에서 입력을 잘라내는 `truncate` 매개변수를 제공합니다.

`truncate` 매개변수에는 세 가지 옵션이 있습니다:

- "NONE": 기본 옵션입니다. 입력이 너무 크면 예외가 발생합니다.
- "START": 서버가 시작(왼쪽)부터 입력을 잘라내고 필요에 따라 token을 버립니다.
- "END": 서버가 끝(오른쪽)부터 입력을 잘라내고 필요에 따라 token을 버립니다.

```python
long_text = "AI is amazing, amazing is " * 100
```

```python
strict_embedder = NVIDIAEmbeddings()
try:
    strict_embedder.embed_query(long_text)
except Exception as e:
    print("Error:", e)
```

```python
truncating_embedder = NVIDIAEmbeddings(truncate="END")
truncating_embedder.embed_query(long_text)[:5]
```

## RAG Retrieval

다음은 [LangChain Expression Language Retrieval Cookbook entry](
https://python.langchain.com/docs/expression_language/cookbook/retrieval)의 초기 예제를 재구성한 것이지만, playground 환경에서 사용 가능한 AI Foundation Models의 [Mixtral 8x7B Instruct](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/mixtral-8x7b)와 [NVIDIA Retrieval QA Embedding](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/nvolve-40k) 모델로 실행됩니다. cookbook의 후속 예제도 예상대로 실행되며, 이러한 옵션을 탐색해 보시기 바랍니다.

**팁:** 내부 추론(즉, 데이터 추출, 도구 선택 등을 위한 instruction following)에는 Mixtral을 사용하고, "기록과 컨텍스트를 기반으로 이 사용자에게 적합한 간단한 응답을 만들어 마무리"하는 단일 최종 응답에는 Llama-Chat을 사용하는 것이 좋습니다.

```python
pip install -qU  langchain faiss-cpu tiktoken langchain-community

from operator import itemgetter

from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_nvidia_ai_endpoints import ChatNVIDIA
```

```python
vectorstore = FAISS.from_texts(
    ["harrison worked at kensho"],
    embedding=NVIDIAEmbeddings(model="NV-Embed-QA"),
)
retriever = vectorstore.as_retriever()

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "Answer solely based on the following context:\n<Documents>\n{context}\n</Documents>",
        ),
        ("user", "{question}"),
    ]
)

model = ChatNVIDIA(model="ai-mixtral-8x7b-instruct")

chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

chain.invoke("where did harrison work?")
```

```python
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "Answer using information solely based on the following context:\n<Documents>\n{context}\n</Documents>"
            "\nSpeak only in the following language: {language}",
        ),
        ("user", "{question}"),
    ]
)

chain = (
    {
        "context": itemgetter("question") | retriever,
        "question": itemgetter("question"),
        "language": itemgetter("language"),
    }
    | prompt
    | model
    | StrOutputParser()
)

chain.invoke({"question": "where did harrison work", "language": "italian"})
```

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/text_embedding/nvidia_ai_endpoints.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
