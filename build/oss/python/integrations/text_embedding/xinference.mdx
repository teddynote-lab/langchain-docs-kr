---
title: Xorbits inference (Xinference)
---

이 노트북은 LangChain 내에서 Xinference embedding을 사용하는 방법을 다룹니다

## Installation

PyPI를 통해 `Xinference`를 설치합니다:

```python
pip install -qU  "xinference[all]"
```

## 로컬 또는 분산 클러스터에 Xinference 배포하기

로컬 배포의 경우 `xinference`를 실행합니다.

클러스터에 Xinference를 배포하려면 먼저 `xinference-supervisor`를 사용하여 Xinference supervisor를 시작합니다. `-p` 옵션으로 포트를 지정하고 `-H` 옵션으로 호스트를 지정할 수 있습니다. 기본 포트는 9997입니다.

그런 다음 실행하려는 각 서버에서 `xinference-worker`를 사용하여 Xinference worker를 시작합니다.

자세한 내용은 [Xinference](https://github.com/xorbitsai/inference)의 README 파일을 참조하세요.

## Wrapper

LangChain과 함께 Xinference를 사용하려면 먼저 model을 실행해야 합니다. command line interface (CLI)를 사용하여 실행할 수 있습니다:

```python
!xinference launch -n vicuna-v1.3 -f ggmlv3 -q q4_0
```

```output
Model uid: 915845ee-2a04-11ee-8ed4-d29396a3f064
```

사용할 수 있는 model UID가 반환됩니다. 이제 LangChain과 함께 Xinference embedding을 사용할 수 있습니다:

```python
from langchain_community.embeddings import XinferenceEmbeddings

xinference = XinferenceEmbeddings(
    server_url="http://0.0.0.0:9997", model_uid="915845ee-2a04-11ee-8ed4-d29396a3f064"
)
```

```python
query_result = xinference.embed_query("This is a test query")
```

```python
doc_result = xinference.embed_documents(["text A", "text B"])
```

마지막으로 model을 더 이상 사용하지 않을 때 종료합니다:

```python
!xinference terminate --model-uid "915845ee-2a04-11ee-8ed4-d29396a3f064"
```

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/text_embedding/xinference.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
