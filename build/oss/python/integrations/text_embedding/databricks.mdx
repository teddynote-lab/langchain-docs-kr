---
title: DatabricksEmbeddings
---

> [Databricks](https://www.databricks.com/) Lakehouse Platform은 데이터, 분석 및 AI를 하나의 플랫폼에 통합합니다.

이 가이드는 Databricks [embedding models](/oss/python/integrations/text_embedding) 시작하기에 대한 간단한 개요를 제공합니다. 모든 `DatabricksEmbeddings` 기능 및 구성에 대한 자세한 문서는 [API reference](https://python.langchain.com/api_reference/community/embeddings/langchain_community.embeddings.databricks.DatabricksEmbeddings.html)를 참조하세요.

## Overview

### Integration details

| Class | Package |
| :--- | :--- |
| [DatabricksEmbeddings](https://python.langchain.com/api_reference/community/embeddings/langchain_community.embeddings.databricks.DatabricksEmbeddings.html) | [databricks-langchain](https://python.langchain.com/docs/integrations/providers/databricks/) |

### Supported Methods

`DatabricksEmbeddings`는 비동기 API를 포함한 [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) 클래스의 모든 메서드를 지원합니다.

### Endpoint Requirement

`DatabricksEmbeddings`가 래핑하는 serving endpoint는 OpenAI 호환 embedding 입력/출력 형식을 가져야 합니다([참조](https://mlflow.org/docs/latest/llms/deployments/index.html#embeddings)). 입력 형식이 호환되는 한, `DatabricksEmbeddings`는 [Databricks Model Serving](https://docs.databricks.com/en/machine-learning/model-serving/index.html)에서 호스팅되는 모든 endpoint 유형에 사용할 수 있습니다:

1. Foundation Models - BAAI General Embedding (BGE)과 같은 최첨단 foundation model의 큐레이션된 목록입니다. 이러한 endpoint는 별도의 설정 없이 Databricks workspace에서 바로 사용할 수 있습니다.
2. Custom Models - LangChain, Pytorch, Transformers 등 원하는 프레임워크를 사용하여 MLflow를 통해 커스텀 embedding model을 serving endpoint에 배포할 수도 있습니다.
3. External Models - Databricks endpoint는 OpenAI text-embedding-3과 같은 독점 모델 서비스처럼 Databricks 외부에서 호스팅되는 모델을 프록시로 제공할 수 있습니다.

## Setup

Databricks 모델에 액세스하려면 Databricks 계정을 생성하고, 자격 증명을 설정하고(Databricks workspace 외부에 있는 경우에만), 필요한 패키지를 설치해야 합니다.

### Credentials (Databricks 외부에 있는 경우에만)

Databricks 내부에서 LangChain 앱을 실행하는 경우 이 단계를 건너뛸 수 있습니다.

그렇지 않은 경우, Databricks workspace hostname과 personal access token을 각각 `DATABRICKS_HOST` 및 `DATABRICKS_TOKEN` 환경 변수에 수동으로 설정해야 합니다. access token을 얻는 방법은 [Authentication Documentation](https://docs.databricks.com/en/dev-tools/auth/index.html#databricks-personal-access-tokens)을 참조하세요.

```python
import getpass
import os

os.environ["DATABRICKS_HOST"] = "https://your-workspace.cloud.databricks.com"
if "DATABRICKS_TOKEN" not in os.environ:
    os.environ["DATABRICKS_TOKEN"] = getpass.getpass(
        "Enter your Databricks access token: "
    )
```

### Installation

LangChain Databricks integration은 `databricks-langchain` 패키지에 포함되어 있습니다:

```python
pip install -qU databricks-langchain
```

## Instantiation

```python
from databricks_langchain import DatabricksEmbeddings

embeddings = DatabricksEmbeddings(
    endpoint="databricks-bge-large-en",
    # Specify parameters for embedding queries and documents if needed
    # query_params={...},
    # document_params={...},
)
```

## Indexing and Retrieval

Embedding model은 데이터 인덱싱과 이후 검색 모두에서 retrieval-augmented generation (RAG) 플로우에 자주 사용됩니다. 자세한 지침은 [RAG tutorials](/oss/python/langchain/rag)를 참조하세요.

아래에서는 위에서 초기화한 `embeddings` 객체를 사용하여 데이터를 인덱싱하고 검색하는 방법을 확인할 수 있습니다. 이 예제에서는 `InMemoryVectorStore`에서 샘플 문서를 인덱싱하고 검색합니다.

```python
# Create a vector store with a sample text
from langchain_core.vectorstores import InMemoryVectorStore

text = "LangChain is the framework for building context-aware reasoning applications"

vectorstore = InMemoryVectorStore.from_texts(
    [text],
    embedding=embeddings,
)

# Use the vectorstore as a retriever
retriever = vectorstore.as_retriever()

# Retrieve the most similar text
retrieved_document = retriever.invoke("What is LangChain?")

# show the retrieved document's content
retrieved_document[0].page_content
```

## Direct Usage

내부적으로 vectorstore와 retriever 구현은 `embeddings.embed_documents(...)`와 `embeddings.embed_query(...)`를 호출하여 각각 `from_texts` 및 retrieval `invoke` 작업에 사용되는 텍스트에 대한 embedding을 생성합니다.

이러한 메서드를 직접 호출하여 자신의 사용 사례에 맞는 embedding을 얻을 수 있습니다.

### Embed single texts

`embed_query`를 사용하여 단일 텍스트 또는 문서를 embedding할 수 있습니다:

```python
single_vector = embeddings.embed_query(text)
print(str(single_vector)[:100])  # Show the first 100 characters of the vector
```

### Embed multiple texts

`embed_documents`를 사용하여 여러 텍스트를 embedding할 수 있습니다:

```python
text2 = (
    "LangGraph is a library for building stateful, multi-actor applications with LLMs"
)
two_vectors = embeddings.embed_documents([text, text2])
for vector in two_vectors:
    print(str(vector)[:100])  # Show the first 100 characters of the vector
```

### Async Usage

`aembed_query`와 `aembed_documents`를 사용하여 비동기적으로 embedding을 생성할 수도 있습니다:

```python
import asyncio


async def async_example():
    single_vector = await embeddings.aembed_query(text)
    print(str(single_vector)[:100])  # Show the first 100 characters of the vector


asyncio.run(async_example())
```

## API reference

`DatabricksEmbeddings` 기능 및 구성 옵션에 대한 자세한 문서는 [API reference](https://python.langchain.com/api_reference/community/embeddings/langchain_community.embeddings.databricks.DatabricksEmbeddings.html)를 참조하세요.

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/text_embedding/databricks.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
