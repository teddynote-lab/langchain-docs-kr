---
title: Oracle Cloud Infrastructure Generative AI
---

Oracle Cloud Infrastructure (OCI) Generative AI는 단일 API를 통해 제공되는 최첨단의 맞춤형 대규모 언어 모델(LLM) 세트를 제공하는 완전 관리형 서비스로, 광범위한 사용 사례를 다룹니다.
OCI Generative AI 서비스를 사용하면 바로 사용 가능한 사전 학습된 모델에 액세스하거나, 전용 AI 클러스터에서 자체 데이터를 기반으로 미세 조정된 커스텀 모델을 생성하고 호스팅할 수 있습니다. 서비스 및 API에 대한 자세한 문서는 __[여기](https://docs.oracle.com/en-us/iaas/Content/generative-ai/home.htm)__ 와 __[여기](https://docs.oracle.com/en-us/iaas/api/#/en/generative-ai/20231130/)__ 에서 확인할 수 있습니다.

이 노트북은 LangChain에서 OCI의 Generative AI 모델을 사용하는 방법을 설명합니다.

### 사전 요구사항

oci sdk를 설치해야 합니다

```python
!pip install -U oci
```

### OCI Generative AI API endpoint
[inference.generativeai.us-chicago-1.oci.oraclecloud.com](https://inference.generativeai.us-chicago-1.oci.oraclecloud.com)

## 인증

이 langchain 통합에서 지원되는 인증 방법은 다음과 같습니다:

1. API Key
2. Session token
3. Instance principal
4. Resource principal

이는 __[여기](https://docs.oracle.com/en-us/iaas/Content/API/Concepts/sdk_authentication_methods.htm)__ 에 자세히 설명된 표준 SDK 인증 방법을 따릅니다.

## 사용법

```python
from langchain_community.embeddings import OCIGenAIEmbeddings

# use default authN method API-key
embeddings = OCIGenAIEmbeddings(
    model_id="MY_EMBEDDING_MODEL",
    service_endpoint="https://inference.generativeai.us-chicago-1.oci.oraclecloud.com",
    compartment_id="MY_OCID",
)


query = "This is a query in English."
response = embeddings.embed_query(query)
print(response)

documents = ["This is a sample document", "and here is another one"]
response = embeddings.embed_documents(documents)
print(response)
```

```python
# Use Session Token to authN
embeddings = OCIGenAIEmbeddings(
    model_id="MY_EMBEDDING_MODEL",
    service_endpoint="https://inference.generativeai.us-chicago-1.oci.oraclecloud.com",
    compartment_id="MY_OCID",
    auth_type="SECURITY_TOKEN",
    auth_profile="MY_PROFILE",  # replace with your profile name
    auth_file_location="MY_CONFIG_FILE_LOCATION",  # replace with file location where profile name configs present
)


query = "This is a sample query"
response = embeddings.embed_query(query)
print(response)

documents = ["This is a sample document", "and here is another one"]
response = embeddings.embed_documents(documents)
print(response)
```

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/text_embedding/oci_generative_ai.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
