---
title: IBM watsonx.ai
---

>WatsonxEmbeddings는 IBM [watsonx.ai](https://www.ibm.com/products/watsonx-ai) foundation model을 위한 wrapper입니다.

이 예제는 `LangChain`을 사용하여 `watsonx.ai` model과 통신하는 방법을 보여줍니다.

## Overview

### Integration details

<ItemTable category="text_embedding" item="IBM" />

## Setup

IBM watsonx.ai model에 액세스하려면 IBM watsonx.ai 계정을 생성하고, API key를 받고, `langchain-ibm` integration package를 설치해야 합니다.

### Credentials

이 셀은 watsonx Embeddings와 작업하는 데 필요한 WML credential을 정의합니다.

**Action:** IBM Cloud 사용자 API key를 제공하세요. 자세한 내용은
[documentation](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui)을 참조하세요.

```python
import os
from getpass import getpass

watsonx_api_key = getpass()
os.environ["WATSONX_APIKEY"] = watsonx_api_key
```

추가로 환경 변수로 추가 secret을 전달할 수 있습니다.

```python
import os

os.environ["WATSONX_URL"] = "your service instance url"
os.environ["WATSONX_TOKEN"] = "your token for accessing the CLOUD or CPD cluster"
os.environ["WATSONX_PASSWORD"] = "your password for accessing the CPD cluster"
os.environ["WATSONX_USERNAME"] = "your username for accessing the CPD cluster"
os.environ["WATSONX_INSTANCE_ID"] = "your instance_id for accessing the CPD cluster"
```

### Installation

LangChain IBM integration은 `langchain-ibm` package에 있습니다:

```python
!pip install -qU langchain-ibm
```

## Instantiation

다른 model에 대해 model `parameters`를 조정해야 할 수 있습니다.

```python
from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames

embed_params = {
    EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,
    EmbedTextParamsMetaNames.RETURN_OPTIONS: {"input_text": True},
}
```

이전에 설정한 parameter로 `WatsonxEmbeddings` class를 초기화합니다.

**Note**:

- API 호출에 대한 context를 제공하려면 `project_id` 또는 `space_id`를 추가해야 합니다. 자세한 내용은 [documentation](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=projects)을 참조하세요.
- 프로비저닝된 서비스 인스턴스의 지역에 따라 [여기](https://ibm.github.io/watsonx-ai-python-sdk/setup_cloud.html#authentication)에 설명된 url 중 하나를 사용하세요.

이 예제에서는 `project_id`와 Dallas url을 사용합니다.

추론에 사용될 `model_id`를 지정해야 합니다.

```python
from langchain_ibm import WatsonxEmbeddings

watsonx_embedding = WatsonxEmbeddings(
    model_id="ibm/granite-embedding-107m-multilingual",
    url="https://us-south.ml.cloud.ibm.com",
    project_id="PASTE YOUR PROJECT_ID HERE",
    params=embed_params,
)
```

또는 Cloud Pak for Data credential을 사용할 수 있습니다. 자세한 내용은 [documentation](https://ibm.github.io/watsonx-ai-python-sdk/setup_cpd.html)을 참조하세요.

```python
watsonx_embedding = WatsonxEmbeddings(
    model_id="ibm/granite-embedding-107m-multilingual",
    url="PASTE YOUR URL HERE",
    username="PASTE YOUR USERNAME HERE",
    password="PASTE YOUR PASSWORD HERE",
    instance_id="openshift",
    version="4.8",
    project_id="PASTE YOUR PROJECT_ID HERE",
    params=embed_params,
)
```

특정 요구 사항의 경우, IBM의 [`APIClient`](https://ibm.github.io/watsonx-ai-python-sdk/base.html#apiclient) 객체를 `WatsonxEmbeddings` class에 전달하는 옵션이 있습니다.

```python
from ibm_watsonx_ai import APIClient

api_client = APIClient(...)

watsonx_embedding = WatsonxEmbeddings(
    model_id="ibm/granite-embedding-107m-multilingual",
    watsonx_client=api_client,
)
```

## Indexing and Retrieval

Embedding model은 데이터 인덱싱과 나중에 검색하는 과정 모두에서 retrieval-augmented generation (RAG) flow에 자주 사용됩니다. 자세한 지침은 [RAG tutorials](/oss/python/langchain/rag)을 참조하세요.

아래에서는 위에서 초기화한 `embeddings` 객체를 사용하여 데이터를 인덱싱하고 검색하는 방법을 확인할 수 있습니다. 이 예제에서는 `InMemoryVectorStore`에서 샘플 문서를 인덱싱하고 검색합니다.

```python
# Create a vector store with a sample text
from langchain_core.vectorstores import InMemoryVectorStore

text = "LangChain is the framework for building context-aware reasoning applications"

vectorstore = InMemoryVectorStore.from_texts(
    [text],
    embedding=watsonx_embedding,
)

# Use the vectorstore as a retriever
retriever = vectorstore.as_retriever()

# Retrieve the most similar text
retrieved_documents = retriever.invoke("What is LangChain?")

# show the retrieved document's content
retrieved_documents[0].page_content
```

```output
'LangChain is the framework for building context-aware reasoning applications'
```

## Direct Usage

내부적으로 vectorstore와 retriever 구현은 `embeddings.embed_documents(...)`와 `embeddings.embed_query(...)`를 호출하여 각각 `from_texts`와 retrieval `invoke` 작업에 사용되는 텍스트에 대한 embedding을 생성합니다.

이러한 method를 직접 호출하여 자신의 사용 사례에 맞는 embedding을 얻을 수 있습니다.

### Embed single texts

`embed_query`로 단일 텍스트나 문서를 embedding할 수 있습니다:

```python
text = "This is a test document."

query_result = watsonx_embedding.embed_query(text)
query_result[:5]
```

```output
[0.009447193, -0.024981951, -0.026013248, -0.040483937, -0.05780445]
```

### Embed multiple texts

`embed_documents`로 여러 텍스트를 embedding할 수 있습니다:

```python
texts = ["This is a content of the document", "This is another document"]

doc_result = watsonx_embedding.embed_documents(texts)
doc_result[0][:5]
```

```output
[0.009447167, -0.024981938, -0.02601326, -0.04048393, -0.05780444]
```

## API reference

모든 `WatsonxEmbeddings` feature와 configuration에 대한 자세한 문서는 [API reference](https://python.langchain.com/api_reference/ibm/embeddings/langchain_ibm.embeddings.WatsonxEmbeddings.html)를 참조하세요.

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/text_embedding/ibm_watsonx.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
