---
title: IPEX-LLM - Intel GPU에서 로컬 BGE Embeddings 사용하기
---

> [IPEX-LLM](https://github.com/intel-analytics/ipex-llm)은 Intel CPU 및 GPU(예: iGPU가 있는 로컬 PC, Arc, Flex, Max와 같은 discrete GPU)에서 매우 낮은 지연 시간으로 LLM을 실행하기 위한 PyTorch 라이브러리입니다.

이 예제는 Intel GPU에서 `ipex-llm` 최적화를 사용하여 LangChain으로 embedding 작업을 수행하는 방법을 다룹니다. 이는 RAG, 문서 QA 등과 같은 애플리케이션에서 유용합니다.

> **참고**
>
> Intel Arc A-Series GPU(Intel Arc A300-Series 또는 Pro A60 제외)를 사용하는 Windows 사용자만 이 Jupyter notebook을 직접 실행하는 것이 권장됩니다. 다른 경우(예: Linux 사용자, Intel iGPU 등)에는 최상의 경험을 위해 터미널에서 Python 스크립트로 코드를 실행하는 것이 권장됩니다.

## 사전 요구 사항 설치

Intel GPU에서 IPEX-LLM의 이점을 활용하려면 도구 설치 및 환경 준비를 위한 몇 가지 사전 요구 사항 단계가 있습니다.

Windows 사용자인 경우 [Intel GPU로 Windows에 IPEX-LLM 설치 가이드](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_windows_gpu.md)를 방문하여 [사전 요구 사항 설치](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_windows_gpu.md#install-prerequisites)를 따라 GPU 드라이버를 업데이트하고(선택 사항) Conda를 설치하세요.

Linux 사용자인 경우 [Intel GPU로 Linux에 IPEX-LLM 설치](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux_gpu.md)를 방문하여 [**사전 요구 사항 설치**](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux_gpu.md#install-prerequisites)를 따라 GPU 드라이버, Intel® oneAPI Base Toolkit 2024.0 및 Conda를 설치하세요.

## 설정

사전 요구 사항 설치 후 모든 사전 요구 사항이 설치된 conda 환경을 생성했어야 합니다. **이 conda 환경에서 jupyter 서비스를 시작하세요**:

```python
pip install -qU langchain langchain-community
```

Intel GPU에서 최적화를 위한 IPEX-LLM과 `sentence-transformers`를 설치하세요.

```python
pip install --pre --upgrade ipex-llm[xpu] --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/
pip install sentence-transformers
```

> **참고**
>
> extra-index-url로 `https://pytorch-extension.intel.com/release-whl/stable/xpu/cn/`을 사용할 수도 있습니다.

## Runtime 구성

최적의 성능을 위해 장치에 따라 여러 환경 변수를 설정하는 것이 권장됩니다:

### Intel Core Ultra 내장 GPU를 사용하는 Windows 사용자

```python
import os

os.environ["SYCL_CACHE_PERSISTENT"] = "1"
os.environ["BIGDL_LLM_XMX_DISABLED"] = "1"
```

### Intel Arc A-Series GPU를 사용하는 Windows 사용자

```python
import os

os.environ["SYCL_CACHE_PERSISTENT"] = "1"
```

> **참고**
>
> 각 모델이 Intel iGPU/Intel Arc A300-Series 또는 Pro A60에서 처음 실행될 때 컴파일하는 데 몇 분이 걸릴 수 있습니다.
>
> 다른 GPU 유형의 경우 Windows 사용자는 [여기](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Overview/install_gpu.md#runtime-configuration)를, Linux 사용자는 [여기](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Overview/install_gpu.md#runtime-configuration-1)를 참조하세요.

## 기본 사용법

`IpexLLMBgeEmbeddings`를 초기화할 때 `model_kwargs`에서 `device`를 `"xpu"`로 설정하면 embedding 모델이 Intel GPU에 배치되고 IPEX-LLM 최적화의 이점을 얻을 수 있습니다:

```python
from langchain_community.embeddings import IpexLLMBgeEmbeddings

embedding_model = IpexLLMBgeEmbeddings(
    model_name="BAAI/bge-large-en-v1.5",
    model_kwargs={"device": "xpu"},
    encode_kwargs={"normalize_embeddings": True},
)
```

## API reference

- [IpexLLMBgeEmbeddings](https://python.langchain.com/api_reference/community/embeddings/langchain_community.embeddings.ipex_llm.IpexLLMBgeEmbeddings.html)

```python
sentence = "IPEX-LLM is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency."
query = "What is IPEX-LLM?"

text_embeddings = embedding_model.embed_documents([sentence, query])
print(f"text_embeddings[0][:10]: {text_embeddings[0][:10]}")
print(f"text_embeddings[1][:10]: {text_embeddings[1][:10]}")

query_embedding = embedding_model.embed_query(query)
print(f"query_embedding[:10]: {query_embedding[:10]}")
```

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/text_embedding/ipex_llm_gpu.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
