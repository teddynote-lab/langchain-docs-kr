Python/TypeScript의 `wrap_openai`/`wrapOpenAI` 메서드를 사용하면 OpenAI client를 래핑하여 자동으로 trace를 로깅할 수 있습니다 -- decorator나 function 래핑이 필요하지 않습니다! wrapper를 사용하면 tool call과 multimodal content block을 포함한 message들이 LangSmith에서 깔끔하게 렌더링됩니다. 또한 wrapper는 `@traceable` decorator 또는 `traceable` function과 원활하게 작동하며, 동일한 애플리케이션에서 둘 다 사용할 수 있습니다.

<Note>
`wrap_openai` 또는 `wrapOpenAI`를 사용하는 경우에도 trace가 LangSmith에 로깅되려면 `LANGSMITH_TRACING` environment variable을 `'true'`로 설정해야 합니다. 이를 통해 코드를 변경하지 않고도 tracing을 켜고 끌 수 있습니다.

또한 `LANGSMITH_API_KEY` environment variable을 API key로 설정해야 합니다 (자세한 내용은 [Setup](/) 참조).

LangSmith API key가 여러 workspace에 연결되어 있는 경우, `LANGSMITH_WORKSPACE_ID` environment variable을 설정하여 사용할 workspace를 지정하세요.

기본적으로 trace는 `default`라는 이름의 project에 로깅됩니다. 다른 project에 trace를 로깅하려면 [이 섹션](/langsmith/log-traces-to-project)을 참조하세요.
</Note>

<CodeGroup>

```python Python
import openai
from langsmith import traceable
from langsmith.wrappers import wrap_openai

client = wrap_openai(openai.Client())

@traceable(run_type="tool", name="Retrieve Context")
def my_tool(question: str) -> str:
  return "During this morning's meeting, we solved all world conflict."

@traceable(name="Chat Pipeline")
def chat_pipeline(question: str):
  context = my_tool(question)
  messages = [
      { "role": "system", "content": "You are a helpful assistant. Please respond to the user's request only based on the given context." },
      { "role": "user", "content": f"Question: {question}\nContext: {context}"}
  ]
  chat_completion = client.chat.completions.create(
      model="gpt-4o-mini", messages=messages
  )
  return chat_completion.choices[0].message.content

chat_pipeline("Can you summarize this morning's meetings?")
```

```typescript TypeScript
import OpenAI from "openai";
import { traceable } from "langsmith/traceable";
import { wrapOpenAI } from "langsmith/wrappers";

const client = wrapOpenAI(new OpenAI());

const myTool = traceable(async (question: string) => {
  return "During this morning's meeting, we solved all world conflict.";
}, { name: "Retrieve Context", run_type: "tool" });

const chatPipeline = traceable(async (question: string) => {
  const context = await myTool(question);
  const messages = [
      {
          role: "system",
          content:
              "You are a helpful assistant. Please respond to the user's request only based on the given context.",
      },
      { role: "user", content: `Question: ${question} Context: ${context}` },
  ];
  const chatCompletion = await client.chat.completions.create({
      model: "gpt-4o-mini",
      messages: messages,
  });
  return chatCompletion.choices[0].message.content;
}, { name: "Chat Pipeline" });

await chatPipeline("Can you summarize this morning's meetings?");
```

</CodeGroup>