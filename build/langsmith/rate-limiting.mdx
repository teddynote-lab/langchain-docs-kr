---
title: 모델 속도 제한 처리 방법
sidebarTitle: 모델 속도 제한 처리
---

대규모 평가 작업을 실행할 때 흔히 발생하는 문제는 일반적으로 모델 제공업체의 타사 API 속도 제한에 도달하는 것입니다. 속도 제한을 처리하는 몇 가지 방법이 있습니다.

## `langchain` RateLimiters 사용하기 (Python 전용)

애플리케이션이나 evaluator에서 `langchain` Python ChatModels를 사용하는 경우, 모델에 rate limiter를 추가하여 모델 제공업체 API로 요청이 전송되는 빈도를 클라이언트 측에서 제어하고 속도 제한 오류를 방지할 수 있습니다.

```python
from langchain.chat_models import init_chat_model
from langchain_core.rate_limiters import InMemoryRateLimiter

rate_limiter = InMemoryRateLimiter(
    requests_per_second=0.1,  # <-- Super slow! We can only make a request once every 10 seconds!!
    check_every_n_seconds=0.1,  # Wake up every 100 ms to check whether allowed to make a request,
    max_bucket_size=10,  # Controls the maximum burst size.
)

model = init_chat_model("gpt-4o", rate_limiter=rate_limiter)

def app(inputs: dict) -> dict:
    response = model.invoke(...)
    ...

def evaluator(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:
    response = model.invoke(...)
    ...
```

rate limiter 구성 방법에 대한 자세한 내용은 [langchain](https://python.langchain.com/docs/how_to/chat_model_rate_limiting/) 문서를 참조하세요.

## 지수 백오프를 사용한 재시도

속도 제한 오류를 처리하는 매우 일반적인 방법은 지수 백오프를 사용한 재시도입니다. 지수 백오프를 사용한 재시도는 각 재시도 사이에 (지수적으로) 증가하는 대기 시간을 두고 실패한 요청을 반복적으로 재시도하는 것을 의미합니다. 이는 요청이 성공하거나 최대 요청 횟수에 도달할 때까지 계속됩니다.

#### `langchain` 사용 시

`langchain` 컴포넌트를 사용하는 경우 `.with_retry(...)` / `.withRetry()` 메서드를 사용하여 모든 모델 호출에 재시도를 추가할 수 있습니다:

<CodeGroup>

```python Python
from langchain import init_chat_model

model_with_retry = init_chat_model("gpt-4o-mini").with_retry(stop_after_attempt=6)
```

```typescript TypeScript
import { initChatModel } from "langchain";

const model = await initChatModel("gpt-4o", {
    modelProvider: "openai",
});

const modelWithRetry = model.withRetry({ stopAfterAttept: 2 });
```

</CodeGroup>

자세한 내용은 `langchain` [Python](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html#langchain_core.language_models.chat_models.BaseChatModel.with_retry) 및 [JS](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#withRetry) API 레퍼런스를 참조하세요.

#### `langchain` 미사용 시

`langchain`을 사용하지 않는 경우 `tenacity` (Python) 또는 `backoff` (Python)와 같은 다른 라이브러리를 사용하여 지수 백오프를 사용한 재시도를 구현하거나 처음부터 직접 구현할 수 있습니다. 이를 수행하는 방법의 예시는 [OpenAI 문서](https://platform.openai.com/docs/guides/rate-limits#retrying-with-exponential-backoff)를 참조하세요.

## max_concurrency 제한

애플리케이션과 evaluator에 대한 동시 호출 수를 제한하는 것은 모델 호출 빈도를 줄이고 속도 제한 오류를 방지하는 또 다른 방법입니다. `max_concurrency`는 [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) 함수에서 직접 설정할 수 있습니다. 이는 데이터셋을 스레드로 효과적으로 분할하여 평가를 병렬화합니다.

<CodeGroup>

```python Python
from langsmith import aevaluate

results = await aevaluate(
    ...
    max_concurrency=4,
)
```

```typescript TypeScript
import { evaluate } from "langsmith/evaluation";

await evaluate(..., {
  ...,
  maxConcurrency: 4,
});
```

</CodeGroup>
```

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/rate-limiting.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
