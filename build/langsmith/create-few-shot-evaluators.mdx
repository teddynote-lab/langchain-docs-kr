---
title: Few-shot 예제로 평가자 개선하기
sidebarTitle: Few-shot 예제로 평가자 개선하기
---

LLM-as-a-judge 평가자는 시스템을 프로그래밍 방식으로 평가할 수 없을 때 매우 유용합니다. 그러나 평가자의 효과는 품질과 인간 검토자 피드백과의 일치도에 따라 달라집니다. LangSmith는 few-shot 예제를 사용하여 LLM-as-a-judge 평가자를 인간 선호도에 맞게 개선하는 기능을 제공합니다.

인간의 수정 사항은 few-shot 예제를 사용하여 평가자 prompt에 자동으로 삽입됩니다. Few-shot 예제는 몇 가지 고품질 예제로 모델의 출력을 안내하는 [few-shot prompting](https://www.promptingguide.ai/techniques/fewshot)에서 영감을 받은 기법입니다.

이 가이드는 LLM-as-a-judge 평가자의 일부로 few-shot 예제를 설정하고 피드백 점수에 수정 사항을 적용하는 방법을 다룹니다.

## Few-shot 예제 작동 방식

* Few-shot 예제는 `{{Few-shot examples}}` variable을 사용하여 평가자 prompt에 추가됩니다
* Few-shot 예제로 평가자를 생성하면 자동으로 dataset이 생성되며, 수정 사항을 만들기 시작하면 few-shot 예제로 자동 채워집니다
* Runtime에 이러한 예제는 평가자에 삽입되어 출력에 대한 가이드 역할을 합니다 - 이를 통해 평가자가 인간 선호도에 더 잘 맞춰질 수 있습니다

## 평가자 구성하기

<Note>
Few-shot 예제는 현재 prompt hub를 사용하는 LLM-as-a-judge 평가자에서는 지원되지 않으며 mustache 형식을 사용하는 prompt와만 호환됩니다.
</Note>

Few-shot 예제를 활성화하기 전에 LLM-as-a-judge 평가자를 설정하세요. 아직 설정하지 않았다면 [LLM-as-a-judge 평가자 가이드](/langsmith/llm-as-judge)의 단계를 따르세요.

### 1. Variable mapping 구성하기

각 few-shot 예제는 구성에 지정된 variable mapping에 따라 형식이 지정됩니다. Few-shot 예제의 variable mapping은 메인 prompt와 동일한 variable을 포함해야 하며, 추가로 `few_shot_explanation`과 feedback key와 동일한 이름을 가진 `score` variable을 포함해야 합니다.

예를 들어, 메인 prompt에 `question`과 `response` variable이 있고 평가자가 `correctness` 점수를 출력한다면, few-shot prompt는 `question`, `response`, `few_shot_explanation`, `correctness` variable을 가져야 합니다.

### 2. 사용할 few-shot 예제 수 지정하기

사용할 few-shot 예제의 수를 지정할 수도 있습니다. 기본값은 5입니다. 예제가 매우 길다면 token을 절약하기 위해 이 숫자를 낮게 설정할 수 있으며, 예제가 짧은 경향이 있다면 평가자가 학습할 수 있는 더 많은 예제를 제공하기 위해 더 높은 숫자를 설정할 수 있습니다. Dataset에 이 숫자보다 많은 예제가 있는 경우 무작위로 선택됩니다.

## 수정 사항 만들기

<Info>
[평가자 점수 감사하기](/langsmith/audit-evaluator-scores)
</Info>

Trace를 로깅하거나 실험을 실행하기 시작하면 평가자가 부여한 일부 점수에 동의하지 않을 수 있습니다. [이러한 점수에 수정 사항을 만들면](/langsmith/audit-evaluator-scores) corrections dataset 내부에 예제가 채워지기 시작합니다. 수정 사항을 만들 때는 반드시 설명을 첨부하세요 - 이러한 설명은 `few_shot_explanation` variable 자리에 평가자 prompt에 채워집니다.

Few-shot 예제의 input은 chain/dataset의 input, output, reference(offline 평가자인 경우)에서 관련 필드가 됩니다. Output은 수정된 평가자 점수와 수정 사항을 남길 때 작성한 설명이 됩니다. 원하는 대로 자유롭게 편집하세요. 다음은 corrections dataset의 few-shot 예제 예시입니다:

![Few-shot example](/langsmith/images/few-shot-example.png)

수정 사항이 few-shot dataset에 채워지는 데 1~2분 정도 걸릴 수 있습니다. 채워지면 평가자의 향후 실행에 prompt에 포함됩니다!

## Corrections dataset 보기

Corrections dataset을 보려면:

* **Online 평가자**: Run rule을 선택하고 **Edit Rule**을 클릭합니다
* **Offline 평가자**: 평가자를 선택하고 **Edit Evaluator**를 클릭합니다

![Edit Evaluator](/langsmith/images/edit-evaluator.png)

**Improve evaluator accuracy using few-shot examples** 섹션에 링크된 corrections dataset으로 이동하세요. Dataset에서 few-shot 예제를 보고 업데이트할 수 있습니다.

![View few-shot dataset](/langsmith/images/view-few-shot-ds.png)

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/create-few-shot-evaluators.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
