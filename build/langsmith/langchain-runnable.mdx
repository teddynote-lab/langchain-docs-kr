---
title: runnable을 평가하는 방법
sidebarTitle: runnable 평가하기
---

<Info>
* `langchain`: [Python](https://python.langchain.com) 및 [JS/TS](https://js.langchain.com)
* Runnable: [Python](https://python.langchain.com/docs/concepts/runnables/) 및 [JS/TS](https://js.langchain.com/docs/concepts/runnables/)
</Info>

`langchain` [Runnable](https://python.langchain.com/docs/concepts/runnables/) 객체(chat model, retriever, chain 등)는 `evaluate()` / `aevaluate()`에 직접 전달할 수 있습니다.

## Setup

평가할 간단한 chain을 정의해 보겠습니다. 먼저 필요한 모든 패키지를 설치합니다:

<CodeGroup>

```bash Python
pip install -U langsmith langchain[openai]
```

```bash TypeScript
yarn add langsmith @langchain/openai
```

</CodeGroup>

이제 chain을 정의합니다:

<CodeGroup>

```python Python
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

instructions = (
    "Please review the user query below and determine if it contains any form "
    "of toxic behavior, such as insults, threats, or highly negative comments. "
    "Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."
)

prompt = ChatPromptTemplate(
    [("system", instructions), ("user", "{text}")],
)

model = init_chat_model("gpt-4o")
chain = prompt | model | StrOutputParser()
```

```typescript TypeScript
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."],
  ["user", "{text}"]
]);

const chatModel = new ChatOpenAI();
const outputParser = new StringOutputParser();
const chain = prompt.pipe(chatModel).pipe(outputParser);
```

</CodeGroup>

## Evaluate

chain을 평가하기 위해 `evaluate()` / `aevaluate()` 메서드에 직접 전달할 수 있습니다. chain의 입력 변수는 example input의 key와 일치해야 합니다. 이 경우 example input은 `{"text": "..."}` 형식이어야 합니다.

<CodeGroup>

```python Python
from langsmith import aevaluate, Client

client = Client()

# Clone a dataset of texts with toxicity labels.
# Each example input has a "text" key and each output has a "label" key.
dataset = client.clone_public_dataset(
    "https://smith.langchain.com/public/3d6831e6-1680-4c88-94df-618c8e01fc55/d"
)

def correct(outputs: dict, reference_outputs: dict) -> bool:
    # Since our chain outputs a string not a dict, this string
    # gets stored under the default "output" key in the outputs dict:
    actual = outputs["output"]
    expected = reference_outputs["label"]
    return actual == expected

results = await aevaluate(
    chain,
    data=dataset,
    evaluators=[correct],
    experiment_prefix="gpt-4o, baseline",
)
```

```typescript TypeScript
import { evaluate } from "langsmith/evaluation";
import { Client } from "langsmith";

const langsmith = new Client();

const dataset = await client.clonePublicDataset(
  "https://smith.langchain.com/public/3d6831e6-1680-4c88-94df-618c8e01fc55/d"
)

await evaluate(chain, {
  data: dataset.name,
  evaluators: [correct],
  experimentPrefix: "gpt-4o, baseline",
});
```

</CodeGroup>

runnable은 각 output에 대해 적절하게 추적됩니다.

![Runnable Evaluation](/langsmith/images/runnable-eval.png)

## Related

* [How to evaluate a `langgraph` graph](/langsmith/evaluate-on-intermediate-steps)

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langchain-runnable.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
