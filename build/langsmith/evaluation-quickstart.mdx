---
title: 평가 빠른 시작
sidebarTitle: 빠른 시작
---

import WorkspaceSecret from '/snippets/langsmith/set-workspace-secrets.mdx';

[_평가_](/langsmith/evaluation-concepts)는 LLM 애플리케이션의 성능을 정량적으로 측정하는 방법입니다. LLM은 예측 불가능하게 동작할 수 있으며, 프롬프트, 모델, 입력의 작은 변화도 결과에 큰 영향을 줄 수 있습니다. 평가는 실패를 식별하고, 버전을 비교하며, 더 신뢰할 수 있는 AI 애플리케이션을 구축할 수 있도록 구조화된 방법을 제공합니다.

LangSmith에서 평가를 실행하려면 세 가지 주요 구성 요소가 필요합니다:

- [_데이터셋_](/langsmith/evaluation-concepts#datasets): 테스트 입력(및 선택적으로 기대 출력) 집합입니다.
- [_타겟 함수_](/langsmith/define-target-function): 테스트하려는 애플리케이션의 부분—새 프롬프트가 적용된 단일 LLM 호출, 하나의 모듈, 또는 전체 워크플로우일 수 있습니다.
- [_평가자_](/langsmith/evaluation-concepts#evaluators): 타겟 함수의 출력을 점수화하는 함수입니다.

이 빠른 시작 가이드는 LangSmith SDK 또는 UI를 사용하여 LLM 응답의 정확성을 확인하는 기본 평가를 실행하는 방법을 안내합니다.

<Tip>
트레이싱 시작에 대한 동영상 시청을 원하신다면, 데이터셋 및 평가 [동영상 가이드](#video-guide)를 참고하세요.
</Tip>

## 사전 준비 사항

시작하기 전에 다음을 준비하세요:

- **LangSmith 계정**: [smith.langchain.com](https://smith.langchain.com)에서 가입하거나 로그인하세요.
- **LangSmith API 키**: [API 키 생성](/langsmith/create-account-api-key#create-an-api-key) 가이드를 따라주세요.
- **OpenAI API 키**: [OpenAI 대시보드](https://platform.openai.com/account/api-keys)에서 생성하세요.

**UI 또는 SDK 필터를 선택하여 안내를 확인하세요:**

<Tabs>
<Tab title="UI" icon="window">

## 1. 워크스페이스 시크릿 설정

<WorkspaceSecret/>

## 2. 프롬프트 생성

LangSmith의 [Prompt Playground](/langsmith/observability-concepts#prompt-playground)를 사용하면 다양한 프롬프트, 새로운 모델, 또는 다양한 모델 설정에 대해 평가를 실행할 수 있습니다.

1. [LangSmith UI](https://smith.langchain.com)에서 **Prompt Engineering** 아래의 **Playground**로 이동하세요.
1. **Prompts** 패널에서 **system** 프롬프트를 다음과 같이 수정하세요:

    ```
    Answer the following question accurately:
    ```

    **Human** 메시지는 그대로 두세요: `{question}`.

## 3. 데이터셋 생성

1. **Set up Evaluation**을 클릭하면 페이지 하단에 **New Experiment** 테이블이 열립니다.
1. **Select or create a new dataset** 드롭다운에서 **+ New** 버튼을 클릭하여 새 데이터셋을 만드세요.

    <div style={{ textAlign: 'center' }}>
    <img
        className="block dark:hidden"
        src="/langsmith/images/playground-system-prompt-light.png"
        alt="수정된 system 프롬프트와 새 실험, 새 데이터셋 생성 드롭다운이 있는 Playground 화면."
    />

    <img
        className="hidden dark:block"
        src="/langsmith/images/playground-system-prompt-dark.png"
        alt="수정된 system 프롬프트와 새 실험, 새 데이터셋 생성 드롭다운이 있는 Playground 화면."
    />
    </div>

1. 데이터셋에 다음 예시를 추가하세요:

    | Inputs                                                   | Reference Outputs                                 |
    | -------------------------------------------------------- | ------------------------------------------------- |
    | question: Which country is Mount Kilimanjaro located in? | output: Mount Kilimanjaro is located in Tanzania. |
    | question: What is Earth's lowest point?                  | output: Earth's lowest point is The Dead Sea.     |

1. **Save**를 클릭하고 이름을 입력하여 새로 만든 데이터셋을 저장하세요.

## 4. 평가자 추가

1. **+ Evaluator**를 클릭하고 **Pre-built Evaluator** 옵션에서 **Correctness**를 선택하세요.
1. **Correctness** 패널에서 **Save**를 클릭하세요.

## 5. 평가 실행

1. 오른쪽 상단에서 <Icon icon="circle-play" /> **Start**를 선택하여 평가를 실행하세요. 그러면 **New Experiment** 테이블에 미리보기가 있는 [_실험_](/langsmith/evaluation-concepts#experiment)이 생성됩니다. 실험 이름을 클릭하면 전체 결과를 볼 수 있습니다.

    <div style={{ textAlign: 'center' }}>
    <img
        className="block dark:hidden"
        src="/langsmith/images/full-experiment-view-light.png"
        alt="예시 데이터셋을 사용한 결과의 전체 실험 화면."
    />

    <img
        className="hidden dark:block"
        src="/langsmith/images/full-experiment-view-dark.png"
        alt="예시 데이터셋을 사용한 결과의 전체 실험 화면."
    />
    </div>

## 다음 단계

<Tip>
LangSmith에서 실험을 실행하는 방법에 대해 더 알고 싶다면 [평가 개념 가이드](/langsmith/evaluation-concepts)를 읽어보세요.
</Tip>

- 평가에 대한 자세한 내용은 [평가 문서](/langsmith/evaluation)를 참고하세요.
- UI에서 [데이터셋 생성 및 관리 방법](/langsmith/manage-datasets-in-application#set-up-your-dataset)을 알아보세요.
- [프롬프트 플레이그라운드에서 평가 실행 방법](/langsmith/run-evaluation-from-prompt-playground)을 알아보세요.

</Tab>

<Tab title="SDK" icon="code">

<Tip>
이 가이드에서는 오픈소스 [`openevals`](https://github.com/langchain-ai/openevals) 패키지의 사전 구축된 LLM-as-judge 평가자를 사용합니다. OpenEvals에는 자주 사용되는 평가자가 포함되어 있으며, 평가에 처음 입문하는 경우 좋은 시작점입니다. 앱 평가 방식을 더 유연하게 제어하고 싶다면 [완전히 커스텀 평가자 정의](/langsmith/code-evaluator)도 가능합니다.
</Tip>

## 1. 의존성 설치

터미널에서 프로젝트 디렉터리를 만들고 환경에 의존성을 설치하세요:

<CodeGroup>

```bash Python
mkdir ls-evaluation-quickstart && cd ls-evaluation-quickstart
python -m venv .venv && source .venv/bin/activate
python -m pip install --upgrade pip
pip install -U langsmith openevals openai
```

```bash TypeScript
mkdir ls-evaluation-quickstart-ts && cd ls-evaluation-quickstart-ts
npm init -y
npm install langsmith openevals openai
npx tsc --init
```

</CodeGroup>

<Info>
패키지 매니저로 `yarn`을 사용하는 경우, `openevals`의 peer dependency인 `@langchain/core`를 직접 설치해야 합니다. 일반적으로 LangSmith 평가에는 필요하지 않으며, [임의의 커스텀 코드로 평가자 정의](/langsmith/code-evaluator)도 가능합니다.
</Info>

## 2. 환경 변수 설정

다음 환경 변수를 설정하세요:

- `LANGSMITH_TRACING`
- `LANGSMITH_API_KEY`
- `OPENAI_API_KEY` (또는 사용하는 LLM 제공자의 API 키)
- (선택) `LANGSMITH_WORKSPACE_ID`: LangSmith API가 여러 [워크스페이스](/langsmith/administration-overview#workspaces)에 연결된 경우, 사용할 워크스페이스를 지정하려면 이 변수를 설정하세요.

``` bash
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY="<your-langsmith-api-key>"
export OPENAI_API_KEY="<your-openai-api-key>"
export LANGSMITH_WORKSPACE_ID="<your-workspace-id>"
```
<Note>
Anthropic을 사용하는 경우 [Anthropic 래퍼](/langsmith/annotate-code#wrap-the-anthropic-client-python-only)를 사용하여 호출을 추적하세요. 다른 제공자의 경우 [traceable 래퍼](/langsmith/annotate-code#use-%40traceable-%2F-traceable)를 사용하세요.
</Note>

## 3. 데이터셋 생성

1. 파일을 만들고 다음 코드를 추가하세요. 이 코드는 다음을 수행합니다:

    - LangSmith에 연결하기 위해 `Client`를 import합니다.
    - 데이터셋을 생성합니다.
    - 예시 [_입력_ 및 _출력_](/langsmith/evaluation-concepts#examples)을 정의합니다.
    - 입력과 출력 쌍을 LangSmith의 해당 데이터셋에 연결하여 평가에 사용할 수 있도록 합니다.

    <CodeGroup>

    ```python Python
    # dataset.py
    from langsmith import Client

    def main():
        client = Client()

        # Programmatically create a dataset in LangSmith
        dataset = client.create_dataset(
            dataset_name="Sample dataset",
            description="A sample dataset in LangSmith."
        )

        # Create examples
        examples = [
            {
                "inputs": {"question": "Which country is Mount Kilimanjaro located in?"},
                "outputs": {"answer": "Mount Kilimanjaro is located in Tanzania."},
            },
            {
                "inputs": {"question": "What is Earth's lowest point?"},
                "outputs": {"answer": "Earth's lowest point is The Dead Sea."},
            },
        ]

        # Add examples to the dataset
        client.create_examples(dataset_id=dataset.id, examples=examples)
        print("Created dataset:", dataset.name)

    if __name__ == "__main__":
        main()

    ```

    ```typescript TypeScript
    // dataset.ts
    import { Client } from "langsmith";

    async function main() {
    const client = new Client();

    const dataset = await client.createDataset(
        "Sample dataset",
        { description: "A sample dataset in LangSmith." }
    );

    // Define examples
    const inputs = [
        { question: "Which country is Mount Kilimanjaro located in?" },
        { question: "What is Earth's lowest point?" },
    ];
    const outputs = [
        { answer: "Mount Kilimanjaro is located in Tanzania." },
        { answer: "Earth's lowest point is The Dead Sea." },
    ];

    await client.createExamples({
        datasetId: dataset.id,
        inputs,
        outputs,
    });

    console.log("Created dataset:", dataset.name);
    }

    if (require.main === module) {
    main().catch((e) => {
        console.error(e);
        process.exit(1);
    });
    }
    ```

    </CodeGroup>

1. 터미널에서 `dataset` 파일을 실행하여 앱 평가에 사용할 데이터셋을 생성하세요:

    <CodeGroup>
    ```bash Python
    python dataset.py
    ```
    ```bash TypeScript
    npx ts-node dataset.ts
    ```

    </CodeGroup>

    다음과 같은 출력이 표시됩니다:

    ```bash
    Created dataset: Sample dataset
    ```

## 4. 타겟 함수 생성

[타겟 함수](/langsmith/define-target-function)를 정의하여 평가할 내용을 포함합니다. 이 가이드에서는 질문에 답하는 단일 LLM 호출을 포함하는 타겟 함수를 정의합니다.

`eval` 파일에 다음 코드를 추가하세요:

<CodeGroup>

```python Python
# eval.py
from langsmith import Client, wrappers
from openai import OpenAI

# Wrap the OpenAI client for LangSmith tracing
openai_client = wrappers.wrap_openai(OpenAI())

# Define the application logic you want to evaluate inside a target function
# The SDK will automatically send the inputs from the dataset to your target function
def target(inputs: dict) -> dict:
    response = openai_client.chat.completions.create(
        model="gpt-5-mini",
        messages=[
            {"role": "system", "content": "Answer the following question accurately"},
            {"role": "user", "content": inputs["question"]},
        ],
    )
    return {"answer": response.choices[0].message.content.strip()}
```

```typescript TypeScript
// eval.ts
import { evaluate } from "langsmith/evaluation";
import { wrapOpenAI } from "langsmith/wrappers/openai";
import OpenAI from "openai";

const openaiClient = wrapOpenAI(new OpenAI());

async function target(inputs: Record<string, any>): Promise<Record<string, any>> {
  const question = String(inputs.question ?? "");
  const resp = await openaiClient.chat.completions.create({
    model: "gpt-5-mini",
    messages: [
      { role: "system", content: "Answer the following question accurately" },
      { role: "user", content: question },
    ],
  });
  return { answer: resp.choices[0].message.content?.trim() ?? "" };
}
```

</CodeGroup>


## 5. 평가자 정의

이 단계에서는 앱이 생성한 답변을 LangSmith가 어떻게 채점할지 지정합니다.

[`openevals`](https://github.com/langchain-ai/openevals)에서 사전 구축된 평가 프롬프트(`CORRECTNESS_PROMPT`)와 이를 [_LLM-as-judge 평가자_](/langsmith/evaluation-concepts#llm-as-judge)로 래핑하는 헬퍼를 import합니다. 이 평가자는 애플리케이션의 출력을 점수화합니다.

<Info>
`CORRECTNESS_PROMPT`는 `"inputs"`, `"outputs"`, `"reference_outputs"` 변수를 가진 f-string입니다. OpenEvals 프롬프트 커스터마이징에 대한 자세한 내용은 [여기](https://github.com/langchain-ai/openevals#customizing-prompts)를 참고하세요.
</Info>

평가자는 다음을 비교합니다:

- `inputs`: 타겟 함수에 전달된 값(예: 질문 텍스트)
- `outputs`: 타겟 함수가 반환한 값(예: 모델의 답변)
- `reference_outputs`: [3단계](#3-create-a-dataset)에서 각 데이터셋 예시에 첨부한 정답

`eval` 파일에 다음 강조된 코드를 추가하세요:

<CodeGroup>

```python Python highlight={3,4,21-31}
from langsmith import Client, wrappers
from openai import OpenAI
from openevals.llm import create_llm_as_judge
from openevals.prompts import CORRECTNESS_PROMPT

# Wrap the OpenAI client for LangSmith tracing
openai_client = wrappers.wrap_openai(OpenAI())

# Define the application logic you want to evaluate inside a target function
# The SDK will automatically send the inputs from the dataset to your target function
def target(inputs: dict) -> dict:
    response = openai_client.chat.completions.create(
        model="gpt-5-mini",
        messages=[
            {"role": "system", "content": "Answer the following question accurately"},
            {"role": "user", "content": inputs["question"]},
        ],
    )
    return {"answer": response.choices[0].message.content.strip()}

def correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):
    evaluator = create_llm_as_judge(
        prompt=CORRECTNESS_PROMPT,
        model="openai:o3-mini",
        feedback_key="correctness",
    )
    return evaluator(
        inputs=inputs,
        outputs=outputs,
        reference_outputs=reference_outputs
    )
```

```typescript TypeScript highlight={4,20-37}
import { evaluate } from "langsmith/evaluation";
import { wrapOpenAI } from "langsmith/wrappers/openai";
import OpenAI from "openai";
import { createLLMAsJudge, CORRECTNESS_PROMPT } from "openevals";

const openaiClient = wrapOpenAI(new OpenAI());

async function target(inputs: Record<string, any>): Promise<Record<string, any>> {
  const question = String(inputs.question ?? "");
  const resp = await openaiClient.chat.completions.create({
    model: "gpt-5-mini",
    messages: [
      { role: "system", content: "Answer the following question accurately" },
      { role: "user", content: question },
    ],
  });
  return { answer: resp.choices[0].message.content?.trim() ?? "" };
}

const judge = createLLMAsJudge({
  prompt: CORRECTNESS_PROMPT,
  model: "openai:o3-mini",
  feedbackKey: "correctness",
});

async function correctnessEvaluator(run: {
  inputs: Record<string, any>;
  outputs: Record<string, any>;
  referenceOutputs?: Record<string, any>;
}) {
  return judge({
    inputs: run.inputs,
    outputs: run.outputs,
    // OpenEvals expects snake_case here:
    reference_outputs: run.referenceOutputs,
  });
}
```

</CodeGroup>

## 6. 실행 및 결과 확인

평가 실험을 실행하려면 `evaluate(...)`를 호출합니다. 이 함수는:

- [3단계](#3-create-a-dataset)에서 생성한 데이터셋에서 예시를 가져옵니다.
- 각 예시의 입력을 [4단계](#4-add-an-evaluator)의 타겟 함수에 전달합니다.
- 출력(모델의 답변)을 수집합니다.
- 출력과 `reference_outputs`를 [5단계](#5-define-an-evaluator)의 평가자에 전달합니다.
- 모든 결과를 LangSmith에 실험으로 기록하여 UI에서 볼 수 있도록 합니다.

1. `eval` 파일에 다음 강조된 코드를 추가하세요:

    <CodeGroup>

    ```python Python highlight={33-49}
    from langsmith import Client, wrappers
    from openai import OpenAI
    from openevals.llm import create_llm_as_judge
    from openevals.prompts import CORRECTNESS_PROMPT

    # Wrap the OpenAI client for LangSmith tracing
    openai_client = wrappers.wrap_openai(OpenAI())

    # Define the application logic you want to evaluate inside a target function
    # The SDK will automatically send the inputs from the dataset to your target function
    def target(inputs: dict) -> dict:
        response = openai_client.chat.completions.create(
            model="gpt-5-mini",
            messages=[
                {"role": "system", "content": "Answer the following question accurately"},
                {"role": "user", "content": inputs["question"]},
            ],
        )
        return {"answer": response.choices[0].message.content.strip()}

    def correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):
        evaluator = create_llm_as_judge(
            prompt=CORRECTNESS_PROMPT,
            model="openai:o3-mini",
            feedback_key="correctness",
        )
        return evaluator(
            inputs=inputs,
            outputs=outputs,
            reference_outputs=reference_outputs
        )

    # After running the evaluation, a link will be provided to view the results in langsmith
    def main():
        client = Client()
        experiment_results = client.evaluate(
            target,
            data="Sample dataset",
            evaluators=[
                correctness_evaluator,
                # can add multiple evaluators here
            ],
            experiment_prefix="first-eval-in-langsmith",
            max_concurrency=2,
        )
        print(experiment_results)

    if __name__ == "__main__":
        main()
    ```

    ```typescript TypeScript highlight={39-57}
    import { evaluate } from "langsmith/evaluation";
    import { wrapOpenAI } from "langsmith/wrappers/openai";   // helper to wrap OpenAI client
    import OpenAI from "openai";                              // model provider
    import { createLLMAsJudge, CORRECTNESS_PROMPT } from "openevals"; // evaluator tools

    const openaiClient = wrapOpenAI(new OpenAI());

    async function target(inputs: Record<string, any>): Promise<Record<string, any>> {
    const question = String(inputs.question ?? "");
    const resp = await openaiClient.chat.completions.create({
        model: "gpt-5-mini",
        messages: [
        { role: "system", content: "Answer the following question accurately" },
        { role: "user", content: question },
        ],
    });
    return { answer: resp.choices[0].message.content?.trim() ?? "" };
    }

    const judge = createLLMAsJudge({
    prompt: CORRECTNESS_PROMPT,
    model: "openai:o3-mini",
    feedbackKey: "correctness",
    });

    async function correctnessEvaluator(run: {
    inputs: Record<string, any>;
    outputs: Record<string, any>;
    referenceOutputs?: Record<string, any>;
    }) {
    return judge({
        inputs: run.inputs,
        outputs: run.outputs,
        // OpenEvals expects snake_case here:
        reference_outputs: run.referenceOutputs,
    });
    }

    async function main() {
    const datasetName = process.env.DATASET_NAME ?? "Sample dataset";

    const results = await evaluate(target, {
        data: datasetName,
        evaluators: [correctnessEvaluator],
        experimentPrefix: "first-eval-in-langsmith",
        maxConcurrency: 2,
    });

    console.log(results);
    }

    if (require.main === module) {
    main().catch((e) => {
        console.error(e);
        process.exit(1);
    });
    }
    ```

    </CodeGroup>

1. 평가자를 실행하세요:

    <CodeGroup>

    ```bash Python
    python eval.py
    ```

    ```bash TypeScript
    npx ts-node eval.ts
    ```

    </CodeGroup>

1. 평가 결과를 볼 수 있는 링크와 실험 결과의 메타데이터가 표시됩니다:

    ```
    View the evaluation results for experiment: 'first-eval-in-langsmith-00000000' at: https://smith.langchain.com/o/6551f9c4-2685-4a08-86b9-1b29643deb3d/datasets/e5fde557-c274-4e49-b39d-000000000000/compare?selectedSessions=70b11778-6a28-4cdb-be81-000000000000

    <ExperimentResults first-eval-in-langsmith-00000000>
    ```

1. 평가 실행 결과에 표시된 링크를 따라 [LangSmith UI](https://smith.langchain.com)의 **Datasets & Experiments** 페이지로 이동하여 실험 결과를 확인하세요. 생성된 실험에는 **Inputs**, **Reference Output**, **Outputs**가 표시된 테이블이 있습니다. 데이터셋을 선택하면 결과의 확장된 뷰를 볼 수 있습니다.

    <div style={{ textAlign: 'center' }}>
    <img
        className="block dark:hidden"
        src="/langsmith/images/experiment-results-link-light.png"
        alt="링크를 따라가면 UI에서 볼 수 있는 실험 결과 화면."
    />

    <img
        className="hidden dark:block"
        src="/langsmith/images/experiment-results-link-dark.png"
        alt="링크를 따라가면 UI에서 볼 수 있는 실험 결과 화면."
    />
    </div>

## 다음 단계

다음으로 살펴볼 수 있는 주제는 다음과 같습니다:

- [평가 개념](/langsmith/evaluation-concepts)에서는 LangSmith 평가의 주요 용어를 설명합니다.
- [OpenEvals README](https://github.com/langchain-ai/openevals)에서 사용 가능한 모든 사전 구축 평가자와 커스터마이징 방법을 확인하세요.
- [커스텀 평가자 정의](/langsmith/code-evaluator).
- [Python](https://docs.smith.langchain.com/reference/python/reference) 또는 [TypeScript](https://docs.smith.langchain.com/reference/js) SDK 레퍼런스에서 모든 클래스와 함수에 대한 자세한 설명을 확인하세요.

</Tab>
</Tabs>

## 동영상 가이드
<iframe
  className="w-full aspect-video rounded-xl"
  src="https://www.youtube.com/embed/iEgjJyk3aTw?si=C7BPKXPmdE1yAflv"
  title="YouTube 동영상 플레이어"
  frameBorder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen
></iframe>

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation-quickstart.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
