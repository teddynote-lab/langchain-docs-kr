---
title: 데이터셋 관리
sidebarTitle: 데이터셋 관리
---

LangSmith는 [_데이터셋_](/langsmith/evaluation-concepts#datasets)을 관리하고 작업하기 위한 도구를 제공합니다. 이 페이지에서는 다음을 포함한 데이터셋 작업에 대해 설명합니다:

- 시간 경과에 따른 변경 사항을 추적하기 위한 [데이터셋 버전 관리](#version-a-dataset).
- 평가를 위한 데이터셋 [필터링](#evaluate-on-a-filtered-view-of-a-dataset) 및 [분할](#evaluate-on-a-dataset-split).
- 데이터셋 [공개 공유](#share-a-dataset).
- 다양한 형식으로 데이터셋 [내보내기](#export-a-dataset).

또한 추가 분석 및 반복을 위해 [실험](/langsmith/evaluation-concepts#experiment)에서 [필터링된 trace를 데이터셋으로 내보내는](#export-filtered-traces-from-experiment-to-dataset) 방법도 배우게 됩니다.

## Version a dataset

LangSmith에서 데이터셋은 버전 관리됩니다. 즉, 데이터셋에서 예제를 추가, 업데이트 또는 삭제할 때마다 데이터셋의 새 버전이 생성됩니다.

### Create a new version of a dataset

데이터셋에서 예제를 추가, 업데이트 또는 삭제할 때마다 데이터셋의 새 [버전](/langsmith/evaluation-concepts#versions)이 생성됩니다. 이를 통해 시간 경과에 따른 데이터셋의 변경 사항을 추적하고 데이터셋이 어떻게 발전했는지 이해할 수 있습니다.

기본적으로 버전은 변경 시점의 timestamp로 정의됩니다. **Examples** 탭에서 데이터셋의 특정 버전(timestamp 기준)을 클릭하면 해당 시점의 데이터셋 상태를 확인할 수 있습니다.

![Version Datasets](/langsmith/images/version-dataset.png)

과거 버전의 데이터셋을 볼 때 예제는 읽기 전용입니다. 또한 이 버전의 데이터셋과 최신 버전의 데이터셋 사이에 수행된 작업도 확인할 수 있습니다.

<Note>
기본적으로 **Examples** 탭에는 데이터셋의 최신 버전이 표시되고 **Tests** 탭에는 모든 버전의 실험이 표시됩니다.
</Note>

**Tests** 탭에서는 다양한 버전의 데이터셋에서 실행된 테스트 결과를 확인할 수 있습니다.

![Version Datasets](/langsmith/images/version-dataset-tests.png)

### Tag a version

데이터셋의 버전에 태그를 지정하여 더 읽기 쉬운 이름을 부여할 수도 있습니다. 이는 데이터셋 히스토리에서 중요한 이정표를 표시하는 데 유용할 수 있습니다.

예를 들어, 데이터셋의 버전을 "prod"로 태그하고 이를 사용하여 LLM 파이프라인에 대한 테스트를 실행할 수 있습니다.

UI에서 **Examples** 탭의 **+ Tag this version**을 클릭하여 데이터셋의 버전에 태그를 지정할 수 있습니다.

![Tagging Datasets](/langsmith/images/tag-this-version.png)

SDK를 사용하여 데이터셋의 버전에 태그를 지정할 수도 있습니다. 다음은 [Python SDK](https://docs.smith.langchain.com/reference/python/reference)를 사용하여 데이터셋의 버전에 태그를 지정하는 예제입니다:

```python
from langsmith import Client
from datetime import datetime

client = Client()
initial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the version you want to tag

# You can tag a specific dataset version with a semantic name, like "prod"
client.update_dataset_tag(
    dataset_name=toxic_dataset_name, as_of=initial_time, tag="prod"
)
```

데이터셋의 특정 태그가 지정된 버전에서 평가를 실행하려면 [특정 데이터셋 버전에서 평가 섹션](#evaluate-on-specific-dataset-version)을 참조하세요.

## Evaluate on a specific dataset version

<Check>
이 섹션을 읽기 전에 다음 내용을 참조하는 것이 도움이 될 수 있습니다:

- [데이터셋 버전 관리](#version-a-dataset).
- [예제 가져오기](/langsmith/manage-datasets-programmatically#fetch-examples).
</Check>

### Use `list_examples`

`evaluate` / `aevaluate`를 사용하여 데이터셋의 특정 버전에서 평가할 예제의 iterable을 전달할 수 있습니다. `list_examples` / `listExamples`를 사용하여 `as_of` / `asOf`를 통해 특정 버전 태그에서 예제를 가져오고 이를 `data` 인수에 전달합니다.

<CodeGroup>

```python Python
from langsmith import Client

ls_client = Client()

# Assumes actual outputs have a 'class' key.
# Assumes example outputs have a 'label' key.
def correct(outputs: dict, reference_outputs: dict) -> bool:
  return outputs["class"] == reference_outputs["label"]

results = ls_client.evaluate(
    lambda inputs: {"class": "Not toxic"},
    # Pass in filtered data here:
    data=ls_client.list_examples(
      dataset_name="Toxic Queries",
      as_of="latest",  # specify version here
    ),
    evaluators=[correct],
)
```

```typescript TypeScript
import { evaluate } from "langsmith/evaluation";

await evaluate((inputs) => labelText(inputs["input"]), {
  data: langsmith.listExamples({
    datasetName: datasetName,
    asOf: "latest",
  }),
  evaluators: [correctLabel],
});
```

</CodeGroup>

데이터셋의 뷰를 가져오는 방법에 대한 자세한 내용은 [프로그래밍 방식으로 데이터셋 생성 및 관리](/langsmith/manage-datasets-programmatically#fetch-datasets) 페이지를 참조하세요.

## Evaluate on a split / filtered view of a dataset

<Check>
이 섹션을 읽기 전에 다음 내용을 참조하는 것이 도움이 될 수 있습니다:

- [예제 가져오기](/langsmith/manage-datasets-programmatically#fetch-examples).
- [데이터셋 분할 생성 및 관리](/langsmith/manage-datasets-in-application#create-and-manage-dataset-splits).
</Check>

### Evaluate on a filtered view of a dataset

`list_examples` / `listExamples` 메서드를 사용하여 데이터셋에서 평가할 예제의 하위 집합을 [가져올](/langsmith/manage-datasets-programmatically#fetch-examples) 수 있습니다.

일반적인 워크플로우 중 하나는 특정 metadata key-value 쌍을 가진 예제를 가져오는 것입니다.

<CodeGroup>

```python Python
from langsmith import evaluate

results = evaluate(
    lambda inputs: label_text(inputs["text"]),
    data=client.list_examples(dataset_name=dataset_name, metadata={"desired_key": "desired_value"}),
    evaluators=[correct_label],
    experiment_prefix="Toxic Queries",
)
```

```typescript TypeScript
import { evaluate } from "langsmith/evaluation";

await evaluate((inputs) => labelText(inputs["input"]), {
  data: langsmith.listExamples({
    datasetName: datasetName,
    metadata: {"desired_key": "desired_value"},
  }),
  evaluators: [correctLabel],
  experimentPrefix: "Toxic Queries",
});
```

</CodeGroup>

더 많은 필터링 기능은 이 [how-to 가이드](/langsmith/manage-datasets-programmatically#list-examples-by-structured-filter)를 참조하세요.

### Evaluate on a dataset split

`list_examples` / `listExamples` 메서드를 사용하여 데이터셋의 하나 또는 여러 [분할](/langsmith/evaluation-concepts#splits)에서 평가할 수 있습니다. `splits` 매개변수는 평가하려는 분할 목록을 받습니다.

<CodeGroup>

```python Python
from langsmith import evaluate

results = evaluate(
    lambda inputs: label_text(inputs["text"]),
    data=client.list_examples(dataset_name=dataset_name, splits=["test", "training"]),
    evaluators=[correct_label],
    experiment_prefix="Toxic Queries",
)
```

```typescript TypeScript
import { evaluate } from "langsmith/evaluation";

await evaluate((inputs) => labelText(inputs["input"]), {
  data: langsmith.listExamples({
    datasetName: datasetName,
    splits: ["test", "training"],
  }),
  evaluators: [correctLabel],
  experimentPrefix: "Toxic Queries",
});
```

</CodeGroup>

데이터셋의 뷰를 가져오는 방법에 대한 자세한 내용은 [데이터셋 가져오기](/langsmith/manage-datasets-programmatically#fetch-datasets) 가이드를 참조하세요.

## Share a dataset

### Share a dataset publicly

<Warning>
데이터셋을 공개적으로 공유하면 **데이터셋 예제, 실험 및 관련 run, 그리고 이 데이터셋에 대한 피드백이 LangSmith 계정이 없는 사람을 포함하여 링크를 가진 모든 사람이 액세스할 수 있게 됩니다**. 민감한 정보를 공유하지 않도록 주의하세요.

이 기능은 클라우드 호스팅 버전의 LangSmith에서만 사용할 수 있습니다.
</Warning>

**Dataset & Experiments** 탭에서 데이터셋을 선택하고 **⋮**(페이지 오른쪽 상단)를 클릭한 다음 **Share Dataset**을 클릭합니다. 그러면 데이터셋 링크를 복사할 수 있는 대화 상자가 열립니다.

![Share Dataset](/langsmith/images/share-dataset.gif)

### Unshare a dataset

1. 공개적으로 공유된 데이터셋의 오른쪽 상단에 있는 **Public**을 클릭한 다음 대화 상자에서 **Unshare**를 클릭하여 **Unshare**합니다. ![Unshare Dataset](/langsmith/images/unshare-dataset.png)

2. **Settings** -> **Shared URLs** 또는 [이 링크](https://smith.langchain.com/settings/shared)를 클릭하여 조직의 공개 공유 데이터셋 목록으로 이동한 다음 공유를 해제하려는 데이터셋 옆의 **Unshare**를 클릭합니다.

![Unshare Trace List](/langsmith/images/unshare-trace-list.png)

## Export a dataset

LangSmith UI에서 LangSmith 데이터셋을 CSV, JSONL 또는 [OpenAI의 fine tuning 형식](https://platform.openai.com/docs/guides/fine-tuning#example-format)으로 내보낼 수 있습니다.

**Dataset & Experiments** 탭에서 데이터셋을 선택하고 **⋮**(페이지 오른쪽 상단)를 클릭한 다음 **Download Dataset**을 클릭합니다.

![Export Dataset Button](/langsmith/images/export-dataset-button.gif)

## Export filtered traces from experiment to dataset

LangSmith에서 [오프라인 평가](/langsmith/evaluation-concepts#offline-evaluation)를 실행한 후 일부 평가 기준을 충족하는 [trace](/langsmith/observability-concepts#traces)를 데이터셋으로 내보낼 수 있습니다.

### View experiment traces

![Export filtered traces](/langsmith/images/export-filtered-trace-to-dataset.png)

이렇게 하려면 먼저 실험 이름 옆의 화살표를 클릭합니다. 그러면 실험에서 생성된 trace가 포함된 프로젝트로 이동합니다.

![Export filtered traces](/langsmith/images/experiment-tracing-project.png)

여기에서 평가 기준에 따라 trace를 필터링할 수 있습니다. 이 예제에서는 0.5보다 큰 accuracy 점수를 받은 모든 trace를 필터링합니다.

![Export filtered traces](/langsmith/images/filtered-traces-from-experiment.png)

프로젝트에 필터를 적용한 후 데이터셋에 추가할 run을 다중 선택하고 **Add to Dataset**을 클릭할 수 있습니다.

![Export filtered traces](/langsmith/images/add-filtered-traces-to-dataset.png)

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-datasets.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
