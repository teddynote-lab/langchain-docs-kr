---
title: Code evaluator 정의하는 방법
sidebarTitle: Code evaluator
---

<Info>
* [Evaluators](/langsmith/evaluation-concepts#evaluators)
</Info>

Code evaluator는 데이터셋 example과 결과 애플리케이션 output을 받아서 하나 이상의 metric을 반환하는 함수입니다. 이러한 함수들은 [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate)에 직접 전달될 수 있습니다.

## 기본 예제

<CodeGroup>

```python Python
from langsmith import evaluate

def correct(outputs: dict, reference_outputs: dict) -> bool:
    """Check if the answer exactly matches the expected answer."""
    return outputs["answer"] == reference_outputs["answer"]

def dummy_app(inputs: dict) -> dict:
    return {"answer": "hmm i'm not sure", "reasoning": "i didn't understand the question"}

results = evaluate(
    dummy_app,
    data="dataset_name",
    evaluators=[correct]
)
```

```typescript TypeScript
import type { EvaluationResult } from "langsmith/evaluation";

const correct = async ({ outputs, referenceOutputs }: {
  outputs: Record<string, any>;
  referenceOutputs?: Record<string, any>;
}): Promise<EvaluationResult> => {
  const score = outputs?.answer === referenceOutputs?.answer;
  return { key: "correct", score };
}
```

</CodeGroup>

## Evaluator 인자

code evaluator 함수는 특정 인자 이름을 가져야 합니다. 다음 인자들 중 일부를 사용할 수 있습니다:

* `run: Run`: 주어진 example에 대해 애플리케이션이 생성한 전체 [Run](/langsmith/run-data-format) 객체입니다.
* `example: Example`: example input, output(사용 가능한 경우), metadata(사용 가능한 경우)를 포함한 전체 데이터셋 [Example](/langsmith/example-data-format)입니다.
* `inputs: dict`: 데이터셋의 단일 example에 해당하는 input의 dictionary입니다.
* `outputs: dict`: 주어진 `inputs`에 대해 애플리케이션이 생성한 output의 dictionary입니다.
* `reference_outputs/referenceOutputs: dict`: example과 연관된 reference output의 dictionary입니다(사용 가능한 경우).

대부분의 사용 사례에서는 `inputs`, `outputs`, `reference_outputs`만 필요합니다. `run`과 `example`은 애플리케이션의 실제 input과 output 외에 추가적인 trace나 example metadata가 필요한 경우에만 유용합니다.

JS/TS를 사용할 때는 이들을 모두 단일 객체 인자의 일부로 전달해야 합니다.

## Evaluator output

Code evaluator는 다음 타입 중 하나를 반환해야 합니다:

Python과 JS/TS

* `dict`: `{"score" | "value": ..., "key": ...}` 형태의 dict를 사용하면 metric 타입("score"는 수치형, "value"는 범주형)과 metric 이름을 커스터마이즈할 수 있습니다. 예를 들어, 정수를 범주형 metric으로 기록하고 싶을 때 유용합니다.

Python만 해당

* `int | float | bool`: 평균을 구하거나 정렬할 수 있는 연속형 metric으로 해석됩니다. 함수 이름이 metric의 이름으로 사용됩니다.
* `str`: 범주형 metric으로 해석됩니다. 함수 이름이 metric의 이름으로 사용됩니다.
* `list[dict]`: 단일 함수를 사용하여 여러 metric을 반환합니다.

## 추가 예제

`langsmith>=0.2.0` 필요

<CodeGroup>

```python Python
from langsmith import evaluate, wrappers
from langsmith.schemas import Run, Example
from openai import AsyncOpenAI
# Assumes you've installed pydantic.
from pydantic import BaseModel

# We can still pass in Run and Example objects if we'd like
def correct_old_signature(run: Run, example: Example) -> dict:
    """Check if the answer exactly matches the expected answer."""
    return {"key": "correct", "score": run.outputs["answer"] == example.outputs["answer"]}

# Just evaluate actual outputs
def concision(outputs: dict) -> int:
    """Score how concise the answer is. 1 is the most concise, 5 is the least concise."""
    return min(len(outputs["answer"]) // 1000, 4) + 1

# Use an LLM-as-a-judge
oai_client = wrappers.wrap_openai(AsyncOpenAI())

async def valid_reasoning(inputs: dict, outputs: dict) -> bool:
    """Use an LLM to judge if the reasoning and the answer are consistent."""
    instructions = """
Given the following question, answer, and reasoning, determine if the reasoning for the
answer is logically valid and consistent with question and the answer."""

    class Response(BaseModel):
        reasoning_is_valid: bool

    msg = f"Question: {inputs['question']}\nAnswer: {outputs['answer']}\nReasoning: {outputs['reasoning']}"
    response = await oai_client.beta.chat.completions.parse(
        model="gpt-4o-mini",
        messages=[{"role": "system", "content": instructions,}, {"role": "user", "content": msg}],
        response_format=Response
    )
    return response.choices[0].message.parsed.reasoning_is_valid

def dummy_app(inputs: dict) -> dict:
    return {"answer": "hmm i'm not sure", "reasoning": "i didn't understand the question"}

results = evaluate(
    dummy_app,
    data="dataset_name",
    evaluators=[correct_old_signature, concision, valid_reasoning]
)
```

```typescript TypeScript
import { Client } from "langsmith";
import { evaluate } from "langsmith/evaluation";
import { Run, Example } from "langsmith/schemas";
import OpenAI from "openai";

// Type definitions
interface AppInputs {
    question: string;
}

interface AppOutputs {
    answer: string;
    reasoning: string;
}

interface Response {
    reasoning_is_valid: boolean;
}

// Old signature evaluator
function correctOldSignature(run: Run, example: Example) {
    return {
        key: "correct",
        score: run.outputs?.["answer"] === example.outputs?.["answer"],
    };
}

// Output-only evaluator
function concision({ outputs }: { outputs: AppOutputs }) {
    return {
        key: "concision",
        score: Math.min(Math.floor(outputs.answer.length / 1000), 4) + 1,
    };
}

// LLM-as-judge evaluator
const openai = new OpenAI();

async function validReasoning({
    inputs,
    outputs
}: {
    inputs: AppInputs;
    outputs: AppOutputs;
}) {
    const instructions = `\
  Given the following question, answer, and reasoning, determine if the reasoning for the \
  answer is logically valid and consistent with question and the answer.`;

    const msg = `Question: ${inputs.question}
Answer: ${outputs.answer}
Reasoning: ${outputs.reasoning}`;

    const response = await openai.chat.completions.create({
        model: "gpt-4",
        messages: [
            { role: "system", content: instructions },
            { role: "user", content: msg }
        ],
        response_format: { type: "json_object" },
        functions: [{
            name: "parse_response",
            parameters: {
                type: "object",
                properties: {
                    reasoning_is_valid: {
                        type: "boolean",
                        description: "Whether the reasoning is valid"
                    }
                },
                required: ["reasoning_is_valid"]
            }
        }]
    });

    const parsed = JSON.parse(response.choices[0].message.content ?? "{}") as Response;
    return {
        key: "valid_reasoning",
        score: parsed.reasoning_is_valid ? 1 : 0
    };
}

// Example application
function dummyApp(inputs: AppInputs): AppOutputs {
    return {
        answer: "hmm i'm not sure",
        reasoning: "i didn't understand the question"
    };
}

const results = await evaluate(dummyApp, {
    data: "dataset_name",
    evaluators: [correctOldSignature, concision, validReasoning],
    client: new Client()
});
```

</CodeGroup>

## 관련 항목

* [전체 experiment 결과 평가하기](/langsmith/summary): 전체 experiment에 대한 metric을 계산하는 summary evaluator를 정의합니다.
* [두 experiment를 비교하는 평가 실행하기](/langsmith/evaluate-pairwise): 두 개(또는 그 이상)의 experiment를 서로 비교하여 metric을 계산하는 pairwise evaluator를 정의합니다.