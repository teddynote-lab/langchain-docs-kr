---
title: Semantic Kernel로 추적하기
sidebarTitle: Semantic Kernel
---

LangSmith는 OpenInference의 OpenAI instrumentation을 사용하여 [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/)에서 생성된 trace를 캡처할 수 있습니다. 이 가이드는 Semantic Kernel 애플리케이션에서 자동으로 trace를 캡처하고 모니터링 및 분석을 위해 LangSmith로 전송하는 방법을 보여줍니다.

## Installation

선호하는 패키지 매니저를 사용하여 필요한 패키지를 설치하세요:

<CodeGroup>

```bash pip
pip install langsmith semantic-kernel openinference-instrumentation-openai
```

```bash uv
uv add langsmith semantic-kernel openinference-instrumentation-openai
```

</CodeGroup>

<Info>
최적의 OpenTelemetry 지원을 위해 LangSmith Python SDK 버전 `langsmith>=0.4.26`이 필요합니다.
</Info>

## Setup

### 1. 환경 변수 구성

API key와 프로젝트 이름을 설정하세요:

<CodeGroup>

```bash Shell
export LANGSMITH_API_KEY=<your_langsmith_api_key>
export LANGSMITH_PROJECT=<your_project_name>
export OPENAI_API_KEY=<your_openai_api_key>
```

</CodeGroup>

### 2. OpenTelemetry integration 구성

Semantic Kernel 애플리케이션에서 LangSmith OpenTelemetry integration과 OpenAI instrumentor를 import하고 구성하세요:

```python
from langsmith.integrations.otel import configure
from openinference.instrumentation.openai import OpenAIInstrumentor

# Configure LangSmith tracing
configure(project_name="semantic-kernel-demo")

# Instrument OpenAI calls
OpenAIInstrumentor().instrument()
```

<Note>
OpenTelemetry 환경 변수를 설정하거나 exporter를 수동으로 구성할 필요가 없습니다—`configure()`가 모든 것을 자동으로 처리합니다.
</Note>

### 3. Semantic Kernel 애플리케이션 생성 및 실행

구성이 완료되면 Semantic Kernel 애플리케이션이 자동으로 LangSmith로 trace를 전송합니다:

이 예제는 kernel을 구성하고, prompt 기반 function을 정의하며, 이를 호출하여 추적된 활동을 생성하는 최소한의 앱을 포함합니다.

```python
import os
import asyncio
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
from semantic_kernel.prompt_template import InputVariable, PromptTemplateConfig
from openinference.instrumentation.openai import OpenAIInstrumentor
from langsmith.integrations.otel import configure
import dotenv

# Load environment variables
dotenv.load_dotenv(".env.local")

# Configure LangSmith tracing
configure(project_name="semantic-kernel-assistant")

# Instrument OpenAI calls
OpenAIInstrumentor().instrument()

# Configure Semantic Kernel
kernel = Kernel()
kernel.add_service(OpenAIChatCompletion())

# Create a code analysis prompt template
code_analysis_prompt = """
Analyze the following code and provide insights:

Code: {{$code}}

Please provide:
1. A brief summary of what the code does
2. Any potential improvements
3. Code quality assessment
"""

prompt_template_config = PromptTemplateConfig(
    template=code_analysis_prompt,
    name="code_analyzer",
    template_format="semantic-kernel",
    input_variables=[
        InputVariable(name="code", description="The code to analyze", is_required=True),
    ],
)

# Add the function to the kernel
code_analyzer = kernel.add_function(
    function_name="analyzeCode",
    plugin_name="codeAnalysisPlugin",
    prompt_template_config=prompt_template_config,
)

# Create a documentation generator
doc_prompt = """
Generate comprehensive documentation for the following function:

{{$function_code}}

Include:
- Purpose and functionality
- Parameters and return values
- Usage examples
- Any important notes
"""

doc_template_config = PromptTemplateConfig(
    template=doc_prompt,
    name="doc_generator",
    template_format="semantic-kernel",
    input_variables=[
        InputVariable(name="function_code", description="The function code to document", is_required=True),
    ],
)

doc_generator = kernel.add_function(
    function_name="generateDocs",
    plugin_name="documentationPlugin",
    prompt_template_config=doc_template_config,
)

async def main():
    # Example code to analyze
    sample_code = """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
    """

    # Analyze the code
    analysis_result = await kernel.invoke(code_analyzer, code=sample_code)
    print("Code Analysis:")
    print(analysis_result)
    print("\n" + "="*50 + "\n")

    # Generate documentation
    doc_result = await kernel.invoke(doc_generator, function_code=sample_code)
    print("Generated Documentation:")
    print(doc_result)

    return {"analysis": str(analysis_result), "documentation": str(doc_result)}

if __name__ == "__main__":
    asyncio.run(main())
```

## Advanced usage

### Custom metadata 및 tag

span attribute를 설정하여 trace에 custom metadata를 추가할 수 있습니다:

```python
from opentelemetry import trace

# Get the current tracer
tracer = trace.get_tracer(__name__)

async def main():
    with tracer.start_as_current_span("semantic_kernel_workflow") as span:
        # Add custom metadata
        span.set_attribute("langsmith.metadata.workflow_type", "code_analysis")
        span.set_attribute("langsmith.metadata.user_id", "developer_123")
        span.set_attribute("langsmith.span.tags", "semantic-kernel,code-analysis")

        # Your Semantic Kernel code here
        result = await kernel.invoke(code_analyzer, code=sample_code)
        return result
```

### 다른 instrumentor와 결합

Semantic Kernel instrumentation을 다른 instrumentor(예: DSPy, AutoGen)와 결합하려면 이들을 추가하고 instrumentor로 초기화하세요:

```python
from langsmith.integrations.otel import configure
from openinference.instrumentation.openai import OpenAIInstrumentor
from openinference.instrumentation.dspy import DSPyInstrumentor

# Configure LangSmith tracing
configure(project_name="multi-framework-app")

# Initialize multiple instrumentors
OpenAIInstrumentor().instrument()
DSPyInstrumentor().instrument()

# Your application code using multiple frameworks
```