---
title: 비동기적으로 평가 실행하는 방법
sidebarTitle: 비동기적으로 평가 실행하기
---

<Info>
[Evaluations](/langsmith/evaluation-concepts#applying-evaluations) | [Evaluators](/langsmith/evaluation-concepts#evaluators) | [Datasets](/langsmith/evaluation-concepts#datasets) | [Experiments](/langsmith/evaluation-concepts#experiments)
</Info>

SDK를 사용하여 [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate)를 통해 비동기적으로 평가를 실행할 수 있습니다. 이 함수는 [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate)와 동일한 인자를 받지만 application function이 비동기여야 합니다. `evaluate()` 함수 사용 방법에 대한 자세한 내용은 [여기](/langsmith/evaluate-llm-application)에서 확인할 수 있습니다.

<Info>
이 가이드는 Python SDK를 사용할 때만 관련이 있습니다. JS/TS에서는 `evaluate()` 함수가 이미 async입니다. 사용 방법은 [여기](/langsmith/evaluate-llm-application)에서 확인할 수 있습니다.
</Info>

## `aevaluate()` 사용하기

* Python

`langsmith>=0.3.13` 필요

```python
from langsmith import wrappers, Client
from openai import AsyncOpenAI

# Optionally wrap the OpenAI client to trace all model calls.
oai_client = wrappers.wrap_openai(AsyncOpenAI())

# Optionally add the 'traceable' decorator to trace the inputs/outputs of this function.
@traceable
async def researcher_app(inputs: dict) -> str:
    instructions = """You are an excellent researcher. Given a high-level research idea, \
list 5 concrete questions that should be investigated to determine if the idea is worth pursuing."""

    response = await oai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": instructions},
            {"role": "user", "content": inputs["idea"]},
        ],
    )
    return response.choices[0].message.content

# Evaluator functions can be sync or async
def concise(inputs: dict, outputs: dict) -> bool:
    return len(outputs["output"]) < 3 * len(inputs["idea"])

ls_client = Client()
ideas = [
    "universal basic income",
    "nuclear fusion",
    "hyperloop",
    "nuclear powered rockets",
]
dataset = ls_client.create_dataset("research ideas")
ls_client.create_examples(
    dataset_name=dataset.name,
    examples=[{"inputs": {"idea": i}} for i in ideas],
)

# Can equivalently use the 'aevaluate' function directly:
# from langsmith import aevaluate
# await aevaluate(...)
results = await ls_client.aevaluate(
    researcher_app,
    data=dataset,
    evaluators=[concise],
    # Optional, add concurrency.
    max_concurrency=2,  # Optional, add concurrency.
    experiment_prefix="gpt-4o-mini-baseline"  # Optional, random by default.
)
```

## 관련 문서

* [평가 실행하기 (동기적으로)](/langsmith/evaluate-llm-application)
* [모델 rate limit 처리하기](/langsmith/rate-limiting)