---
title: trace에 대한 token 기반 비용 계산
sidebarTitle: trace에 대한 token 기반 비용 계산
---

<Info>
* [LLM run(span)에 대한 token 수 제공하기](/langsmith/log-llm-trace#provide-token-and-cost-information)
</Info>

LangSmith를 사용하면 LLM run에 대한 token 기반 비용을 추적할 수 있습니다. 비용은 trace 및 project 수준으로 집계됩니다.

비용을 추적하는 방법은 두 가지가 있습니다:

1. token 수와 model 가격으로부터 도출
2. run data의 일부로 직접 지정

대부분의 경우 run data에 token 수를 포함하고 LangSmith에서 model 가격을 지정하는 것이 더 쉽습니다. LangSmith는 비용이 token 유형별로 분류된 token 수에 선형적이라고 가정합니다. 비선형 가격 책정을 사용하는 소수의 model(예: X개 이상의 input token에서 token당 가격이 변경되는 경우)의 경우, 클라이언트 측에서 비용을 계산하고 run data의 일부로 전송하는 것을 권장합니다.

## token 수 전송

LangSmith가 LLM run에 대한 비용을 정확하게 도출하려면 token 수를 제공해야 합니다:

* OpenAI 또는 Anthropic model과 함께 LangSmith Python 또는 TS/JS SDK를 사용하는 경우, [내장 wrapper](/langsmith/annotate-code#wrap-the-openai-client)가 자동으로 token 수, model provider 및 model name data를 LangSmith로 전송합니다.
* 다른 model provider와 함께 LangSmith SDK를 사용하는 경우, [이 가이드](/langsmith/log-llm-trace#provide-token-and-cost-information)를 주의 깊게 읽어야 합니다.
* LangChain Python 또는 TS/JS를 사용하는 경우, 대부분의 chat model integration에 대해 token 수, model provider 및 model name이 자동으로 LangSmith로 전송됩니다. token 수가 누락된 chat model integration이 있고 기본 API가 model response에 token 수를 포함하는 경우, [LangChain repo](https://github.com/langchain-ai/langchain)에 GitHub issue를 열어주세요.

정확한 비용 추적을 위해서는 token 수를 명시적으로 제공해야 합니다. token 수를 포함하는 방법에 대한 자세한 내용은 [token 및 비용 정보 제공](/langsmith/log-llm-trace#provide-token-and-cost-information) 가이드를 참조하세요.

## model name 지정

LangSmith는 [run metadata](/langsmith/add-metadata-tags)의 `ls_model_name` 필드에서 LLM model name을 읽습니다. [SDK 내장 wrapper](/langsmith/annotate-code#wrap-the-openai-client) 및 모든 LangChain integration은 이 metadata를 자동으로 지정합니다.

## model 가격 설정

token 수와 model name으로부터 비용을 계산하려면 사용 중인 model의 token당 가격을 알아야 합니다. LangSmith에는 이를 위한 [model pricing table](https://smith.langchain.com/settings/workspaces/models)이 있습니다. 이 table에는 대부분의 OpenAI, Anthropic 및 Gemini model에 대한 가격 정보가 포함되어 있습니다. 다른 model에 대한 가격을 추가하거나 기본 model의 가격을 덮어쓸 수 있습니다.

prompt(input) 및 completion(output) token에 대한 가격을 지정할 수 있습니다. 필요한 경우 더 자세한 가격 분류를 제공할 수 있습니다. 예를 들어, 일부 model provider는 multimodal 또는 cached token에 대해 다른 가격을 책정합니다.

![](/langsmith/images/model-price-map.png)

prompt/completion 가격 옆의 `...`에 마우스를 올리면 token 유형별 가격 분류를 볼 수 있습니다. 예를 들어, `audio` 및 `image` prompt token이 기본 text prompt token과 다른 가격을 가지는지 확인할 수 있습니다.

model pricing map에 *새 항목*을 생성하려면 오른쪽 상단의 `Add new model` 버튼을 클릭하세요.

![New price map entry interface](/langsmith/images/new-price-map-entry.png)

여기에서 다음 필드를 지정할 수 있습니다:

* **Model Name**: model의 사람이 읽을 수 있는 이름입니다.
* **Match Pattern**: model name과 일치시킬 regex pattern입니다. 이는 run metadata의 `ls_model_name` 값과 일치시키는 데 사용됩니다.
* **Prompt (Input) Price**: model의 1M input token당 비용입니다. 이 숫자는 prompt의 token 수를 곱하여 prompt 비용을 계산합니다.
* **Completion (Output) Price**: model의 1M output token당 비용입니다. 이 숫자는 completion의 token 수를 곱하여 completion 비용을 계산합니다.
* **Prompt (Input) Price Breakdown** (선택사항): 각 다른 유형의 prompt token에 대한 가격 분류입니다. 예: `cache_read`, `video`, `audio` 등.
* **Completion (Output) Price Breakdown** (선택사항): 각 다른 유형의 completion token에 대한 가격 분류입니다. 예: `reasoning`, `image` 등.
* **Model Activation Date** (선택사항): 가격이 적용되는 날짜입니다. 이 날짜 이후의 run만 이 model 가격을 적용합니다.
* **Provider** (선택사항): model의 provider입니다. 지정된 경우 run metadata의 `ls_provider`와 일치합니다.

model pricing map을 설정하면 LangSmith는 LLM invocation에서 제공된 token 수를 기반으로 trace에 대한 token 기반 비용을 자동으로 계산하고 집계합니다.

<Note>
model pricing map에 대한 업데이트는 이미 로깅된 trace의 비용에 반영되지 않습니다. 현재 model 가격 변경에 대한 backfill을 지원하지 않습니다.
</Note>

가격 분류를 지정하기 위해 LangChain chat model integration 및 LangSmith SDK wrapper에서 사용하는 자세한 token count 유형은 다음과 같습니다:

```python
# Standardized
cache_read
cache_creation
reasoning
audio
image
video
# Anthropic-only
ephemeral_1h_input_tokens
ephemeral_5m_input_tokens
```

## 비용 계산 공식

run에 대한 비용은 가장 구체적인 token 유형에서 가장 덜 구체적인 token 유형으로 탐욕적으로 계산됩니다. 1M prompt token당 $2의 가격을 설정하고 `cache_read` prompt token 1M당 $1의 세부 가격, completion token 1M당 $3의 가격을 설정했다고 가정합니다. 다음과 같은 usage metadata를 업로드한 경우:

```python
{
  "input_tokens": 20,
  "input_token_details": {"cache_read": 5},
  "output_tokens": 10,
  "output_token_details": {},
  "total_tokens": 30,
}
```

token 비용은 다음과 같이 계산됩니다:

```python
# A.K.A. prompt_cost
# Notice that we compute the cache_read cost and then for any
# remaining input_tokens we apply the default input price.
input_cost = 5 * 1e-6 + (20 - 5) * 2e-6  # 3.5e-5
# A.K.A. completion_cost
output_cost = 10 * 3e-6  # 3e-5
total_cost = input_cost + output_cost  # 6.5e-5
```

## 비용 직접 전송

token 비용 정보를 반환하는 LLM 호출을 추적하거나, token 기반이 아닌 가격 책정 방식을 사용하는 API를 추적하거나, 런타임에 비용에 대한 정확한 정보를 가지고 있는 경우, LangSmith의 내장 비용 계산에 의존하는 대신 추적하는 동안 `usage_metadata` dict를 채울 수 있습니다.

run에 대한 비용 정보를 수동으로 제공하는 방법을 알아보려면 [이 가이드](/langsmith/log-llm-trace#provide-token-and-cost-information)를 참조하세요.