---
title: LLM 애플리케이션 평가 방법
sidebarTitle: SDK 사용
---

이 가이드는 LangSmith SDK를 사용하여 LLM 애플리케이션에 대한 평가를 실행하는 방법을 보여줍니다.

<Info>
[Evaluations](/langsmith/evaluation-concepts#applying-evaluations) | [Evaluators](/langsmith/evaluation-concepts#evaluators) | [Datasets](/langsmith/evaluation-concepts#datasets)
</Info>

이 가이드에서는 LangSmith SDK의 [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) 메서드를 사용하여 애플리케이션을 평가하는 방법을 다룹니다.

<Check>
Python에서 대규모 평가 작업의 경우 [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate)의 비동기 버전인 [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) 사용을 권장합니다. 두 메서드는 동일한 인터페이스를 가지고 있으므로 [비동기 평가 실행](/langsmith/evaluation-async) 가이드를 읽기 전에 이 가이드를 먼저 읽는 것이 좋습니다.

JS/TS에서는 evaluate()가 이미 비동기이므로 별도의 메서드가 필요하지 않습니다.

대규모 작업을 실행할 때는 `max_concurrency`/`maxConcurrency` 인자를 구성하는 것도 중요합니다. 이는 데이터셋을 스레드로 효과적으로 분할하여 평가를 병렬화합니다.
</Check>

## 애플리케이션 정의

먼저 평가할 애플리케이션이 필요합니다. 이 예제에서는 간단한 독성 분류기를 만들어 보겠습니다.

<CodeGroup>

```python Python
from langsmith import traceable, wrappers
from openai import OpenAI

# Optionally wrap the OpenAI client to trace all model calls.
oai_client = wrappers.wrap_openai(OpenAI())

# Optionally add the 'traceable' decorator to trace the inputs/outputs of this function.
@traceable
def toxicity_classifier(inputs: dict) -> dict:
    instructions = (
      "Please review the user query below and determine if it contains any form of toxic behavior, "
      "such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does "
      "and 'Not toxic' if it doesn't."
    )
    messages = [
        {"role": "system", "content": instructions},
        {"role": "user", "content": inputs["text"]},
    ]
    result = oai_client.chat.completions.create(
        messages=messages, model="gpt-4o-mini", temperature=0
    )
    return {"class": result.choices[0].message.content}
```

```typescript TypeScript
import { OpenAI } from "openai";
import { wrapOpenAI } from "langsmith/wrappers";
import { traceable } from "langsmith/traceable";

// Optionally wrap the OpenAI client to trace all model calls.
const oaiClient = wrapOpenAI(new OpenAI());

// Optionally add the 'traceable' wrapper to trace the inputs/outputs of this function.
const toxicityClassifier = traceable(
  async (text: string) => {
    const result = await oaiClient.chat.completions.create({
      messages: [
        {
           role: "system",
          content: "Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.",
        },
        { role: "user", content: text },
      ],
      model: "gpt-4o-mini",
      temperature: 0,
    });

    return result.choices[0].message.content;
  },
  { name: "toxicityClassifier" }
);
```

</CodeGroup>

파이프라인의 각 단계의 입력과 출력을 캡처하기 위해 선택적으로 tracing을 활성화했습니다. tracing을 위해 코드에 주석을 추가하는 방법을 이해하려면 [이 가이드](/langsmith/annotate-code)를 참조하세요.

## Dataset 생성 또는 선택

애플리케이션을 평가할 [Dataset](/langsmith/evaluation-concepts#datasets)이 필요합니다. 우리의 데이터셋에는 독성 및 비독성 텍스트의 레이블이 지정된 [examples](/langsmith/evaluation-concepts#examples)가 포함됩니다.


`langsmith>=0.3.13` 필요

<CodeGroup>

```python Python
from langsmith import Client
ls_client = Client()

examples = [
  {
    "inputs": {"text": "Shut up, idiot"},
    "outputs": {"label": "Toxic"},
  },
  {
    "inputs": {"text": "You're a wonderful person"},
    "outputs": {"label": "Not toxic"},
  },
  {
    "inputs": {"text": "This is the worst thing ever"},
    "outputs": {"label": "Toxic"},
  },
  {
    "inputs": {"text": "I had a great day today"},
    "outputs": {"label": "Not toxic"},
  },
  {
    "inputs": {"text": "Nobody likes you"},
    "outputs": {"label": "Toxic"},
  },
  {
    "inputs": {"text": "This is unacceptable. I want to speak to the manager."},
    "outputs": {"label": "Not toxic"},
  },
]

dataset = ls_client.create_dataset(dataset_name="Toxic Queries")
ls_client.create_examples(
  dataset_id=dataset.id,
  examples=examples,
)
```

```typescript TypeScript
import { Client } from "langsmith";

const langsmith = new Client();

// create a dataset
const labeledTexts = [
  ["Shut up, idiot", "Toxic"],
  ["You're a wonderful person", "Not toxic"],
  ["This is the worst thing ever", "Toxic"],
  ["I had a great day today", "Not toxic"],
  ["Nobody likes you", "Toxic"],
  ["This is unacceptable. I want to speak to the manager.", "Not toxic"],
];

const [inputs, outputs] = labeledTexts.reduce<
  [Array<{ input: string }>, Array<{ outputs: string }>]
>(
  ([inputs, outputs], item) => [
    [...inputs, { input: item[0] }],
    [...outputs, { outputs: item[1] }],
  ],
  [[], []]
);

const datasetName = "Toxic Queries";
const toxicDataset = await langsmith.createDataset(datasetName);
await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });
```

</CodeGroup>

데이터셋에 대한 자세한 내용은 [Manage datasets](/langsmith/manage-datasets) 페이지를 참조하세요.

## Evaluator 정의

<Check>
일반적인 사전 구축된 evaluator를 위해 LangChain의 오픈 소스 평가 패키지 [openevals](https://github.com/langchain-ai/openevals)도 확인할 수 있습니다.
</Check>

[Evaluators](/langsmith/evaluation-concepts#evaluators)는 애플리케이션의 출력을 채점하는 함수입니다. 이들은 예제 입력, 실제 출력, 그리고 존재하는 경우 참조 출력을 받습니다. 이 작업에 대한 레이블이 있으므로 evaluator는 실제 출력이 참조 출력과 일치하는지 직접 확인할 수 있습니다.

- Python: `langsmith>=0.3.13` 필요
- TypeScript: `langsmith>=0.2.9` 필요

<CodeGroup>

```python Python
def correct(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:
    return outputs["class"] == reference_outputs["label"]
```

```typescript TypeScript
import type { EvaluationResult } from "langsmith/evaluation";

function correct({
  outputs,
  referenceOutputs,
}: {
  outputs: Record<string, any>;
  referenceOutputs?: Record<string, any>;
}): EvaluationResult {
  const score = outputs.output === referenceOutputs?.outputs;
  return { key: "correct", score };
}
```

</CodeGroup>

## 평가 실행

[evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) 메서드를 사용하여 평가를 실행합니다.

주요 인자는 다음과 같습니다:

* 입력 dictionary를 받아 출력 dictionary를 반환하는 target function. 각 [Example](/langsmith/example-data-format)의 `example.inputs` 필드가 target function에 전달됩니다. 이 경우 우리의 `toxicity_classifier`는 이미 예제 입력을 받도록 설정되어 있으므로 직접 사용할 수 있습니다.
* `data` - 평가할 LangSmith 데이터셋의 이름 또는 UUID, 또는 예제의 iterator
* `evaluators` - 함수의 출력을 채점할 evaluator 목록


Python: `langsmith>=0.3.13` 필요

<CodeGroup>

```python Python
# Can equivalently use the 'evaluate' function directly:
# from langsmith import evaluate; evaluate(...)
results = ls_client.evaluate(
    toxicity_classifier,
    data=dataset.name,
    evaluators=[correct],
    experiment_prefix="gpt-4o-mini, baseline",  # optional, experiment name prefix
    description="Testing the baseline system.",  # optional, experiment description
    max_concurrency=4, # optional, add concurrency
)
```

```typescript TypeScript
import { evaluate } from "langsmith/evaluation";

await evaluate((inputs) => toxicityClassifier(inputs["input"]), {
  data: datasetName,
  evaluators: [correct],
  experimentPrefix: "gpt-4o-mini, baseline",  // optional, experiment name prefix
  maxConcurrency: 4, // optional, add concurrency
});
```

</CodeGroup>

## 결과 탐색[​](#explore-the-results "Direct link to Explore the results")

`evaluate()`의 각 호출은 LangSmith UI에서 보거나 SDK를 통해 쿼리할 수 있는 [Experiment](/langsmith/evaluation-concepts#experiments)를 생성합니다. 평가 점수는 각 실제 출력에 대해 feedback으로 저장됩니다.

*tracing을 위해 코드에 주석을 추가한 경우, 사이드 패널 뷰에서 각 행의 trace를 열 수 있습니다.*

![](/langsmith/images/view-experiment.gif)

## 참조 코드[​](#reference-code "Direct link to Reference code")

<Accordion title="통합된 코드 스니펫을 보려면 클릭하세요">
    <CodeGroup>

```python Python
from langsmith import Client, traceable, wrappers
from openai import OpenAI

# Step 1. Define an application
oai_client = wrappers.wrap_openai(OpenAI())

@traceable
def toxicity_classifier(inputs: dict) -> str:
    system = (
      "Please review the user query below and determine if it contains any form of toxic behavior, "
      "such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does "
      "and 'Not toxic' if it doesn't."
    )
    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": inputs["text"]},
    ]
    result = oai_client.chat.completions.create(
        messages=messages, model="gpt-4o-mini", temperature=0
    )
    return result.choices[0].message.content

# Step 2. Create a dataset
ls_client = Client()
dataset = ls_client.create_dataset(dataset_name="Toxic Queries")
examples = [
  {
    "inputs": {"text": "Shut up, idiot"},
    "outputs": {"label": "Toxic"},
  },
  {
    "inputs": {"text": "You're a wonderful person"},
    "outputs": {"label": "Not toxic"},
  },
  {
    "inputs": {"text": "This is the worst thing ever"},
    "outputs": {"label": "Toxic"},
  },
  {
    "inputs": {"text": "I had a great day today"},
    "outputs": {"label": "Not toxic"},
  },
  {
    "inputs": {"text": "Nobody likes you"},
    "outputs": {"label": "Toxic"},
  },
  {
    "inputs": {"text": "This is unacceptable. I want to speak to the manager."},
    "outputs": {"label": "Not toxic"},
  },
]
ls_client.create_examples(
  dataset_id=dataset.id,
  examples=examples,
)

# Step 3. Define an evaluator
def correct(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:
    return outputs["output"] == reference_outputs["label"]

# Step 4. Run the evaluation
# Client.evaluate() and evaluate() behave the same.
results = ls_client.evaluate(
    toxicity_classifier,
    data=dataset.name,
    evaluators=[correct],
    experiment_prefix="gpt-4o-mini, simple",  # optional, experiment name prefix
    description="Testing the baseline system.",  # optional, experiment description
    max_concurrency=4,  # optional, add concurrency
)
```

```typescript TypeScript
import { OpenAI } from "openai";
import { Client } from "langsmith";
import { evaluate, EvaluationResult } from "langsmith/evaluation";
import type { Run, Example } from "langsmith/schemas";
import { traceable } from "langsmith/traceable";
import { wrapOpenAI } from "langsmith/wrappers";

const oaiClient = wrapOpenAI(new OpenAI());

const toxicityClassifier = traceable(
  async (text: string) => {
    const result = await oaiClient.chat.completions.create({
      messages: [
        {
          role: "system",
          content: "Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.",
        },
        { role: "user", content: text },
      ],
      model: "gpt-4o-mini",
      temperature: 0,
    });
    return result.choices[0].message.content;
  },
  { name: "toxicityClassifier" }
);

const langsmith = new Client();

// create a dataset
const labeledTexts = [
  ["Shut up, idiot", "Toxic"],
  ["You're a wonderful person", "Not toxic"],
  ["This is the worst thing ever", "Toxic"],
  ["I had a great day today", "Not toxic"],
  ["Nobody likes you", "Toxic"],
  ["This is unacceptable. I want to speak to the manager.", "Not toxic"],
];

const [inputs, outputs] = labeledTexts.reduce<
  [Array<{ input: string }>, Array<{ outputs: string }>]
>(
  ([inputs, outputs], item) => [
    [...inputs, { input: item[0] }],
    [...outputs, { outputs: item[1] }],
  ],
  [[], []]
);

const datasetName = "Toxic Queries";
const toxicDataset = await langsmith.createDataset(datasetName);
await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });

// Row-level evaluator
function correct({
  outputs,
  referenceOutputs,
}: {
  outputs: Record<string, any>;
  referenceOutputs?: Record<string, any>;
}): EvaluationResult {
  const score = outputs.output === referenceOutputs?.outputs;
  return { key: "correct", score };
}

await evaluate((inputs) => toxicityClassifier(inputs["input"]), {
  data: datasetName,
  evaluators: [correct],
  experimentPrefix: "gpt-4o-mini, simple",  // optional, experiment name prefix
  maxConcurrency: 4, // optional, add concurrency
});
```

</CodeGroup>
</Accordion>

## 관련 항목[​](#related "Direct link to Related")

* [비동기로 평가 실행](/langsmith/evaluation-async)
* [REST API를 통한 평가 실행](/langsmith/run-evals-api-only)
* [프롬프트 playground에서 평가 실행](/langsmith/run-evaluation-from-prompt-playground)