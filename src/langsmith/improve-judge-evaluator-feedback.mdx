---
title: 인간 피드백을 사용하여 LLM-as-judge evaluator 개선하기
sidebarTitle: 인간 피드백을 사용하여 LLM-as-judge evaluator 개선하기
---

<Check>
이 페이지를 진행하기 전에 다음 내용을 읽어보시면 도움이 됩니다:

* [Evaluation 개념](/langsmith/evaluation-concepts#evaluators)
* [LLM-as-a-judge evaluator 생성하기](/langsmith/llm-as-judge)
</Check>

신뢰할 수 있는 [_LLM-as-a-judge evaluator_](/langsmith/evaluation-concepts#llm-as-judge)는 AI 애플리케이션에 대한 정보에 입각한 결정(예: prompt, model, architecture 변경)을 내리는 데 매우 중요합니다. evaluator prompt를 올바르게 정의하는 것은 어려울 수 있지만, 평가의 신뢰성에 직접적인 영향을 미칩니다.

이 가이드는 인간 피드백을 사용하여 LLM-as-a-judge evaluator를 정렬하고 evaluator의 품질을 개선하여 신뢰할 수 있는 AI 애플리케이션을 구축하는 방법을 설명합니다.

## 작동 방식

LangSmith의 **Align Evaluator** 기능은 인간 전문가 피드백과 LLM-as-a-judge evaluator를 정렬하는 데 도움이 되는 일련의 단계를 제공합니다. 이 기능을 사용하여 [offline evaluation](/langsmith/evaluation-concepts#offline-evaluation)을 위해 dataset에서 실행되는 evaluator 또는 [online evaluation](/langsmith/evaluation-concepts#online-evaluation)을 위한 evaluator를 정렬할 수 있습니다. 두 경우 모두 단계는 유사합니다:

1. 애플리케이션의 출력을 포함하는 **experiment 또는 run을 선택**합니다.
2. 선택한 experiment 또는 run을 인간 전문가가 데이터에 레이블을 지정할 수 있는 **annotation queue**에 추가합니다.
3. 레이블이 지정된 예제에 대해 **LLM-as-a-judge evaluator prompt를 테스트**합니다. evaluator 결과가 레이블이 지정된 데이터와 일치하지 않는 경우를 확인합니다. 이는 evaluator prompt를 개선해야 하는 영역을 나타냅니다.
4. evaluator 정렬을 개선하기 위해 **개선하고 반복**합니다. LLM-as-a-judge evaluator prompt를 업데이트하고 다시 테스트합니다.

## 사전 요구 사항

[offline evaluation](#offline-evaluations) 또는 [online evaluation](#online-evaluations)을 위해 이 가이드를 시작하기 전에 다음이 필요합니다:

### Offline evaluations

- 최소 하나의 [experiment](/langsmith/evaluation-concepts#experiment)가 있는 [dataset](/langsmith/evaluation-concepts#datasets).
- [SDK](/langsmith/manage-datasets-programmatically#create-a-dataset) 또는 [UI](/langsmith/manage-datasets-in-application#set-up-your-dataset)를 통해 dataset을 업로드하거나 생성하고 [SDK](/langsmith/evaluate-llm-application#run-the-evaluation) 또는 [Playground](/langsmith/run-evaluation-from-prompt-playground#5-run-your-evaluation)를 통해 experiment를 실행해야 합니다.

### Online evaluations

- 이미 LangSmith에 trace를 전송하고 있는 애플리케이션.
- 시작하려면 [tracing integration](/langsmith/observability-concepts#integrations) 중 하나로 구성하세요.

## 시작하기

dataset 및 tracing project에서 새 evaluator와 기존 evaluator 모두에 대해 정렬 플로우를 시작할 수 있습니다.

|                                              | Dataset Evaluator                                                                                                                                                                         | Tracing Project Evaluator                                                                                                                                                             |
| -------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **처음부터 정렬된 evaluator 생성** | 1. **Datasets & Experiments**를 선택하고 dataset을 선택합니다<br></br>2. **+ Evaluator** > **Create from labeled data**를 클릭합니다<br></br>3. 설명적인 feedback key 이름을 입력합니다 (예: `correctness`, `hallucination`) | 1. **Projects**를 선택하고 project를 선택합니다<br></br>2. **+ New** > **Evaluator** > **Create from labeled data**를 클릭합니다<br></br>3. 설명적인 feedback‑key 이름을 입력합니다 (예: `correctness`, `hallucination`) |
| **기존 evaluator 정렬**              | 1. **Datasets & Experiments** > dataset 선택 > **Evaluators** 탭<br></br>2. **Align Evaluator with experiment data** 박스에서 **Select Experiments**를 클릭합니다                              | 1. **Projects** > project 선택 > **Evaluators** 탭<br></br>2. **Align Evaluator with experiment data** 박스에서 **Select Experiments**를 클릭합니다                                         |

## 1. experiment 또는 run 선택

인간 레이블링을 위해 전송할 하나 이상의 experiment(또는 run)를 선택합니다. 이렇게 하면 [annotation queue](/langsmith/evaluation-concepts#annotation-queues)에 run이 추가됩니다.

![Add to evaluator queue](/langsmith/images/add-to-evaluator-queue.gif)

기존 annotation queue에 새 experiment/run을 추가하려면 **Evaluators** 탭으로 이동하여 정렬 중인 evaluator를 선택하고 **Add to Queue**를 클릭합니다.

<Check>
Dataset은 프로덕션에서 볼 것으로 예상되는 입력 및 출력을 대표해야 합니다.

모든 가능한 시나리오를 다룰 필요는 없지만, 예상되는 전체 사용 사례 범위에 걸쳐 예제를 포함하는 것이 중요합니다. 예를 들어, 야구, 농구, 축구에 대한 질문에 답하는 스포츠 봇을 구축하는 경우 dataset에는 각 스포츠에서 최소 하나의 레이블이 지정된 예제가 포함되어야 합니다.
</Check>

## 2. 예제 레이블 지정

feedback score를 추가하여 annotation queue의 예제에 레이블을 지정합니다. 예제에 레이블을 지정한 후 **Add to Reference Dataset**을 클릭합니다.

<Check>
experiment에 많은 수의 예제가 있는 경우 시작하기 위해 모든 예제에 레이블을 지정할 필요는 없습니다. 최소 20개의 예제로 시작하는 것을 권장하며, 나중에 언제든지 더 추가할 수 있습니다. 레이블을 지정하는 예제는 균형 잡힌 evaluator prompt를 구축할 수 있도록 다양해야 합니다(0과 1 레이블 모두 균형 있게).
</Check>

## 3. 레이블이 지정된 예제에 대해 evaluator prompt 테스트

레이블이 지정된 예제가 있으면 다음 단계는 레이블이 지정된 데이터를 가능한 한 잘 모방하도록 evaluator prompt를 반복하는 것입니다. 이 반복은 **Evaluator Playground**에서 수행됩니다.

evaluator playground로 이동하려면: evaluator queue의 오른쪽 상단에 있는 **View evaluator** 버튼을 클릭합니다. 그러면 정렬 중인 evaluator의 세부 정보 페이지로 이동합니다. **Evaluator Playground** 버튼을 클릭하여 playground에 액세스합니다.

![Evaluator Playground](/langsmith/images/evaluator-pg.gif)

evaluator playground에서 evaluator prompt를 생성하거나 편집하고 **Start Alignment**를 클릭하여 2단계에서 생성한 레이블이 지정된 예제 세트에 대해 실행할 수 있습니다. evaluator를 실행한 후 생성된 점수가 인간 레이블과 어떻게 비교되는지 확인할 수 있습니다. alignment score는 evaluator의 판단이 인간 전문가의 판단과 일치하는 예제의 백분율입니다.

![Evaluator Playground](/langsmith/images/alignment-evaluator-pg.gif)

## 4. evaluator 정렬을 개선하기 위해 반복

prompt를 업데이트하고 다시 테스트하여 evaluator 정렬을 개선합니다.

<Check>
evaluator prompt에 대한 업데이트는 **기본적으로 저장되지 않습니다**. evaluator prompt를 정기적으로 저장하고 특히 alignment score가 개선된 후에 저장하는 것을 권장합니다.

evaluator playground는 prompt를 반복할 때 비교를 위해 가장 최근에 저장된 버전의 evaluator prompt에 대한 alignment score를 표시합니다.
</Check>

evaluator의 alignment score를 개선하는 것은 정확한 과학은 아니지만 alignment score를 높이는 데 도움이 되는 몇 가지 전략이 있습니다.

### evaluator 정렬 개선을 위한 팁

**1. 정렬되지 않은 예제 조사**

정렬되지 않은 예제를 파헤치고 공통 실패 모드로 그룹화하려고 시도하는 것은 evaluator 정렬을 개선하기 위한 훌륭한 첫 번째 단계입니다.

공통 실패 모드를 식별한 후 LLM이 이를 알 수 있도록 evaluator prompt에 지침을 추가합니다. 예를 들어, 특정 약어를 이해하지 못하는 것을 발견하면 "MFA는 'multi-factor authentication'을 의미합니다"라고 설명할 수 있습니다. 또는 evaluator의 컨텍스트에서 좋음/나쁨이 무엇을 의미하는지 혼란스러워하는 경우 "좋은 응답에는 항상 예약할 수 있는 최소 3개의 잠재적 호텔이 포함됩니다"라고 알려줄 수 있습니다.

**2. LLM score 뒤의 reasoning 검사**

LLM이 예제를 특정 방식으로 점수를 매긴 이유를 이해하려면 LLM-as-a-judge evaluator에 대해 reasoning을 활성화할 수 있습니다. Reasoning은 LLM의 사고 과정을 이해하는 데 도움이 되며 evaluator prompt에 통합할 공통 실패 모드를 식별하는 데도 도움이 될 수 있습니다.

evaluator playground에서 reasoning을 보려면 LLM score 위에 마우스를 올립니다.

![Enable reasoning](/langsmith/images/enable-reasoning.gif)

이렇게 하면 evaluator playground에서 LLM의 점수 뒤에 있는 reasoning이 표시됩니다.

**3. 더 많은 레이블이 지정된 예제 추가 및 성능 검증**

레이블이 지정된 예제에 과적합되는 것을 방지하려면 더 많은 레이블이 지정된 예제를 추가하고 성능을 테스트하는 것이 중요합니다. 특히 적은 수의 예제로 시작한 경우 더욱 그렇습니다.

## 비디오 가이드
<iframe
  className="w-full aspect-video rounded-xl"
  src="https://www.youtube.com/embed/-9o94oj4x0A?si=wfv9cN3L4DalMD2e"
  title="YouTube video player"
  frameBorder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen
></iframe>