---
title: Tracing 빠른 시작
sidebarTitle: 빠른 시작
---

[_관찰 가능성(Observability)_](/langsmith/observability-concepts)은 대규모 언어 모델(LLM)로 구축된 애플리케이션에 필수적인 요구사항입니다. LLM은 비결정적이므로 동일한 프롬프트가 다른 응답을 생성할 수 있습니다. 이러한 동작은 기존 소프트웨어보다 디버깅과 모니터링을 더 어렵게 만듭니다.

LangSmith는 애플리케이션이 요청을 처리하는 방법에 대한 엔드투엔드 가시성을 제공하여 이 문제를 해결합니다. 각 요청은 발생한 모든 내용의 전체 기록을 캡처하는 [_trace_](/langsmith/observability-concepts#traces)를 생성합니다. trace 내에는 LLM 호출이나 검색 단계와 같이 애플리케이션이 수행한 특정 작업인 개별 [_run_](/langsmith/observability-concepts#runs)이 있습니다. run을 추적하면 애플리케이션의 동작을 검사하고, 디버그하고, 검증할 수 있습니다.

이 빠른 시작에서는 최소한의 [_Retrieval Augmented Generation (RAG)_](https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-retrieval-augmented-generation-rag) 애플리케이션을 설정하고 LangSmith로 tracing을 추가합니다. 다음을 수행합니다:

1. 환경을 구성합니다.
1. 컨텍스트를 검색하고 LLM을 호출하는 애플리케이션을 생성합니다.
1. 검색 단계와 LLM 호출을 모두 캡처하도록 tracing을 활성화합니다.
1. LangSmith UI에서 결과 trace를 확인합니다.

<Tip>
tracing 시작에 대한 비디오를 선호하는 경우 빠른 시작 [비디오 가이드](#video-guide)를 참조하세요.
</Tip>

## 사전 요구사항

시작하기 전에 다음이 있는지 확인하세요:

- **LangSmith 계정**: [smith.langchain.com](https://smith.langchain.com)에서 가입하거나 로그인하세요.
- **LangSmith API key**: [API key 생성](/langsmith/create-account-api-key#create-an-api-key) 가이드를 따르세요.
- **OpenAI API key**: [OpenAI 대시보드](https://platform.openai.com/account/api-keys)에서 생성하세요.

이 빠른 시작의 예제 앱은 OpenAI를 LLM provider로 사용합니다. 앱의 LLM provider에 맞게 예제를 조정할 수 있습니다.

<Tip>
[LangChain](https://python.langchain.com/docs/introduction/) 또는 [LangGraph](https://langchain-ai.github.io/langgraph/)로 애플리케이션을 구축하는 경우 단일 환경 변수로 LangSmith tracing을 활성화할 수 있습니다. [LangChain으로 tracing](/langsmith/trace-with-langchain) 또는 [LangGraph로 tracing](/langsmith/trace-with-langgraph) 가이드를 읽고 시작하세요.
</Tip>

## 1. 디렉토리 생성 및 종속성 설치

터미널에서 프로젝트용 디렉토리를 생성하고 환경에 종속성을 설치합니다:

<CodeGroup>

```bash Python
mkdir ls-observability-quickstart && cd ls-observability-quickstart
python -m venv .venv && source .venv/bin/activate
python -m pip install --upgrade pip
pip install -U langsmith openai
```

```bash TypeScript
mkdir ls-observability-quickstart-ts && cd ls-observability-quickstart-ts
npm init -y
npm install langsmith openai typescript ts-node
npx tsc --init
```

</CodeGroup>

## 2. 환경 변수 설정

다음 환경 변수를 설정합니다:

- `LANGSMITH_TRACING`
- `LANGSMITH_API_KEY`
- `OPENAI_API_KEY` (또는 LLM provider의 API key)
- (선택사항) `LANGSMITH_WORKSPACE_ID`: LangSmith API가 여러 workspace에 연결된 경우 사용할 workspace를 지정하려면 이 변수를 설정하세요.

``` bash
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY="<your-langsmith-api-key>"
export OPENAI_API_KEY="<your-openai-api-key>"
export LANGSMITH_WORKSPACE_ID="<your-workspace-id>"
```

Anthropic을 사용하는 경우 [Anthropic wrapper](/langsmith/annotate-code#wrap-the-anthropic-client-python-only)를 사용하여 호출을 추적하세요. 다른 provider의 경우 [traceable wrapper](/langsmith/annotate-code#use-%40traceable-%2F-traceable)를 사용하세요.

## 3. 애플리케이션 정의

이 단계에 설명된 예제 앱 코드를 사용하여 RAG 애플리케이션을 계측할 수 있습니다. 또는 LLM 호출을 포함하는 자체 애플리케이션 코드를 사용할 수 있습니다.

이것은 아직 LangSmith tracing이 추가되지 않은 OpenAI SDK를 직접 사용하는 최소한의 RAG 앱입니다. 세 가지 주요 부분이 있습니다:

- **Retriever function**: 항상 동일한 문자열을 반환하는 문서 검색을 시뮬레이션합니다.
- **OpenAI client**: chat completion 요청을 보내기 위해 일반 OpenAI client를 인스턴스화합니다.
- **RAG function**: 검색된 문서를 사용자의 질문과 결합하여 system prompt를 형성하고, `gpt-4o-mini`로 `chat.completions.create()` endpoint를 호출하고, assistant의 응답을 반환합니다.

앱 파일(예: `app.py` 또는 `app.ts`)에 다음 코드를 추가합니다:

<CodeGroup>

```python Python
from openai import OpenAI

def retriever(query: str):
    # Minimal example retriever
    return ["Harrison worked at Kensho"]

# OpenAI client call (no wrapping yet)
client = OpenAI()

def rag(question: str) -> str:
    docs = retriever(question)
    system_message = (
        "Answer the user's question using only the provided information below:\n"
        + "\n".join(docs)
    )

    # This call is not traced yet
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": question},
        ],
    )
    return resp.choices[0].message.content

if __name__ == "__main__":
    print(rag("Where did Harrison work?"))
```

```typescript TypeScript
import "dotenv/config";
import OpenAI from "openai";

// Minimal example retriever
function retriever(query: string): string[] {
  return ["Harrison worked at Kensho"];
}

// OpenAI client call (no wrapping yet)
const client = new OpenAI();

async function rag(question: string) {
  const docs = retriever(question);
  const systemMessage =
    "Answer the user's question using only the provided information below:\n" +
    docs.join("\n");

  // This call is not traced yet
  const resp = await client.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [
      { role: "system", content: systemMessage },
      { role: "user", content: question },
    ],
  });

  return resp.choices[0].message?.content;
}

(async () => {
  console.log(await rag("Where did Harrison work?"));
})();
```

</CodeGroup>

## 4. LLM 호출 추적

시작하려면 모든 OpenAI 호출을 추적합니다. LangSmith는 wrapper를 제공합니다:

- Python: [`wrap_openai`](https://docs.smith.langchain.com/reference/python/wrappers/langsmith.wrappers._openai.wrap_openai)
- TypeScript: [`wrapOpenAI`](https://docs.smith.langchain.com/reference/js/functions/wrappers_openai.wrapOpenAI)

이 스니펫은 OpenAI client를 래핑하여 이후의 모든 모델 호출이 LangSmith에서 추적된 child run으로 자동으로 로깅되도록 합니다.

1. 앱 파일에 강조 표시된 줄을 포함합니다:

    <CodeGroup>

    ```python Python highlight={2,7}
    from openai import OpenAI
    from langsmith.wrappers import wrap_openai  # traces openai calls

    def retriever(query: str):
        return ["Harrison worked at Kensho"]

    client = wrap_openai(OpenAI())  # log traces by wrapping the model calls

    def rag(question: str) -> str:
        docs = retriever(question)
        system_message = (
            "Answer the user's question using only the provided information below:\n"
            + "\n".join(docs)
        )
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": question},
            ],
        )
        return resp.choices[0].message.content

    if __name__ == "__main__":
        print(rag("Where did Harrison work?"))
    ```

    ```typescript TypeScript highlight={3,9}
    import "dotenv/config";
    import OpenAI from "openai";
    import { wrapOpenAI } from "langsmith/wrappers"; // traces openai calls

    function retriever(query: string): string[] {
    return ["Harrison worked at Kensho"];
    }

    const client = wrapOpenAI(new OpenAI()); // log traces by wrapping the model calls

    async function rag(question: string) {
    const docs = retriever(question);
    const systemMessage =
        "Answer the user's question using only the provided information below:\n" +
        docs.join("\n");

    const resp = await client.chat.completions.create({
        model: "gpt-4o-mini",
        messages: [
        { role: "system", content: systemMessage },
        { role: "user", content: question },
        ],
    });

    return resp.choices[0].message?.content;
    }

    (async () => {
    console.log(await rag("Where did Harrison work?"));
    })();
    ```

    </CodeGroup>

1. 애플리케이션을 호출합니다:

    <CodeGroup>

    ```bash Python
    python app.py
    ```

    ```bash TypeScript
    npx ts-node app.ts
    ```

    </CodeGroup>


    다음 출력을 받게 됩니다:

    ```
    Harrison worked at Kensho.
    ```

1. [LangSmith UI](https://smith.langchain.com)에서 workspace의 **default** Tracing Project([2단계](#2-set-up-environment-variables)에서 지정한 workspace)로 이동합니다. 방금 계측한 OpenAI 호출을 볼 수 있습니다.

<div style={{ textAlign: 'center' }}>
<img
    className="block dark:hidden"
    src="/langsmith/images/trace-quickstart-llm-call.png"
    alt="system 및 human input과 AI Output이 있는 ChatOpenAI라는 LLM 호출 trace를 보여주는 LangSmith UI."
/>

<img
    className="hidden dark:block"
    src="/langsmith/images/trace-quickstart-llm-call-dark.png"
    alt="system 및 human input과 AI Output이 있는 ChatOpenAI라는 LLM 호출 trace를 보여주는 LangSmith UI."
/>
</div>

## 5. 전체 애플리케이션 추적

LLM 호출만이 아닌 전체 애플리케이션을 추적하기 위해 [Python](https://docs.smith.langchain.com/reference/python/run_helpers/langsmith.run_helpers.traceable) 또는 [TypeScript](https://langsmith-docs-bdk0fivr6-langchain.vercel.app/reference/js/functions/traceable.traceable)용 `traceable` decorator를 사용할 수도 있습니다.

1. 앱 파일에 강조 표시된 코드를 포함합니다:

    <CodeGroup>

    ```python Python highlight={3,10}
    from openai import OpenAI
    from langsmith.wrappers import wrap_openai
    from langsmith import traceable

    def retriever(query: str):
        return ["Harrison worked at Kensho"]

    client = wrap_openai(OpenAI())  # keep this to capture the prompt and response from the LLM

    @traceable
    def rag(question: str) -> str:
        docs = retriever(question)
        system_message = (
            "Answer the user's question using only the provided information below:\n"
            + "\n".join(docs)
        )
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": question},
            ],
        )
        return resp.choices[0].message.content

    if __name__ == "__main__":
        print(rag("Where did Harrison work?"))
    ```

    ```typescript TypeScript highlight={3,11}
    import "dotenv/config";
    import OpenAI from "openai";
    import { wrapOpenAI, traceable } from "langsmith/wrappers";

    function retriever(query: string): string[] {
    return ["Harrison worked at Kensho"];
    }

    const client = wrapOpenAI(new OpenAI()); // keep this to capture the prompt and response from the LLM

    const rag = traceable(async (question: string) => {
    const docs = retriever(question);
    const systemMessage =
        "Answer the user's question using only the provided information below:\n" +
        docs.join("\n");

    const resp = await client.chat.completions.create({
        model: "gpt-4o-mini",
        messages: [
        { role: "system", content: systemMessage },
        { role: "user", content: question },
        ],
    });

    return resp.choices[0].message?.content;
    });

    (async () => {
    console.log(await rag("Where did Harrison work?"));
    })();
    ```

    </CodeGroup>

1. run을 생성하기 위해 애플리케이션을 다시 호출합니다:

    <CodeGroup>

    ```bash Python
    python app.py
    ```

    ```bash TypeScript
    npx ts-node app.ts
    ```

    </CodeGroup>

1. [LangSmith UI](https://smith.langchain.com)로 돌아가서 workspace의 **default** Tracing Project([2단계](#2-set-up-environment-variables)에서 지정한 workspace)로 이동합니다. **rag** 단계와 **ChatOpenAI** LLM 호출이 포함된 전체 앱 파이프라인의 trace를 찾을 수 있습니다.

<div style={{ textAlign: 'center' }}>
<img
    className="block dark:hidden"
    src="/langsmith/images/trace-quickstart-app.png"
    alt="input과 output이 있는 rag라는 전체 애플리케이션의 trace를 보여주는 LangSmith UI."
/>

<img
    className="hidden dark:block"
    src="/langsmith/images/trace-quickstart-app-dark.png"
    alt="input과 output이 있는 rag라는 전체 애플리케이션의 trace를 보여주는 LangSmith UI."
/>
</div>

## 다음 단계

다음으로 탐색할 수 있는 몇 가지 주제는 다음과 같습니다:

- [Tracing 통합](/langsmith/trace-with-langchain)은 다양한 LLM provider 및 agent framework에 대한 지원을 제공합니다.
- [Trace 필터링](/langsmith/filter-traces-in-application)은 상당한 양의 데이터가 포함된 tracing project에서 데이터를 효과적으로 탐색하고 분석하는 데 도움이 될 수 있습니다.
- [RAG 애플리케이션 추적](/langsmith/observability-llm-tutorial)은 개발부터 프로덕션까지 애플리케이션에 관찰 가능성을 추가하는 전체 튜토리얼입니다.
- [특정 project로 trace 전송](/langsmith/log-traces-to-project)은 trace의 대상 project를 변경합니다.

## 비디오 가이드
<iframe
  className="w-full aspect-video rounded-xl"
  src="https://www.youtube.com/embed/fA9b4D8IsPQ?si=0eBb1vzw5AxUtplS"
  title="YouTube video player"
  frameBorder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen
></iframe>