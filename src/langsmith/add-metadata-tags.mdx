```mdx
---
title: trace에 metadata와 tag 추가하기
sidebarTitle: trace에 metadata와 tag 추가하기
---

LangSmith는 trace와 함께 임의의 metadata와 tag를 전송하는 것을 지원합니다.

Tag는 trace를 분류하거나 레이블링하는 데 사용할 수 있는 문자열입니다. Metadata는 trace에 대한 추가 정보를 저장하는 데 사용할 수 있는 key-value 쌍의 dictionary입니다.

둘 다 trace와 관련된 추가 정보를 연결하는 데 유용합니다. 예를 들어 실행된 환경, 시작한 사용자 또는 내부 correlation ID 등을 저장할 수 있습니다. tag와 metadata에 대한 자세한 내용은 [Concepts](/langsmith/observability-concepts#tags) 페이지를 참조하세요. metadata와 tag로 trace와 run을 쿼리하는 방법에 대한 정보는 [애플리케이션에서 trace 필터링하기](/langsmith/filter-traces-in-application) 페이지를 참조하세요.

<CodeGroup>

```python Python
import openai
import langsmith as ls
from langsmith.wrappers import wrap_openai

client = openai.Client()
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
]

    # You can set metadata & tags **statically** when decorating a function
    # Use the @traceable decorator with tags and metadata
    # Ensure that the LANGSMITH_TRACING environment variables are set for @traceable to work
    @ls.traceable(
        run_type="llm",
        name="OpenAI Call Decorator",
        tags=["my-tag"],
        metadata={"my-key": "my-value"}
    )
    def call_openai(
        messages: list[dict], model: str = "gpt-4o-mini"
    ) -> str:
        # You can also dynamically set metadata on the parent run:
        rt = ls.get_current_run_tree()
        rt.metadata["some-conditional-key"] = "some-val"
        rt.tags.extend(["another-tag"])
        return client.chat.completions.create(
            model=model,
            messages=messages,
        ).choices[0].message.content

    call_openai(
        messages,
        # To add at **invocation time**, when calling the function.
        # via the langsmith_extra parameter
        langsmith_extra={"tags": ["my-other-tag"], "metadata": {"my-other-key": "my-value"}}
    )

    # Alternatively, you can use the context manager
    with ls.trace(
        name="OpenAI Call Trace",
        run_type="llm",
        inputs={"messages": messages},
        tags=["my-tag"],
        metadata={"my-key": "my-value"},
    ) as rt:
        chat_completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
        )
        rt.metadata["some-conditional-key"] = "some-val"
        rt.end(outputs={"output": chat_completion})

# You can use the same techniques with the wrapped client
patched_client = wrap_openai(
    client, tracing_extra={"metadata": {"my-key": "my-value"}, "tags": ["a-tag"]}
)
chat_completion = patched_client.chat.completions.create(
    model="gpt-4o-mini",
    messages=messages,
    langsmith_extra={
        "tags": ["my-other-tag"],
        "metadata": {"my-other-key": "my-value"},
    },
)
```

```typescript TypeScript
import OpenAI from "openai";
import { traceable, getCurrentRunTree } from "langsmith/traceable";
import { wrapOpenAI } from "langsmith/wrappers";

    const client = wrapOpenAI(new OpenAI());
    const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [
        { role: "system", content: "You are a helpful assistant." },
        { role: "user", content: "Hello!" },
    ];

    const traceableCallOpenAI = traceable(
        async (messages: OpenAI.Chat.ChatCompletionMessageParam[]) => {
            const completion = await client.chat.completions.create({
                model: "gpt-4o-mini",
                messages,
            });
            const runTree = getCurrentRunTree();
            runTree.extra.metadata = {
                ...runTree.extra.metadata,
                someKey: "someValue",
            };
            runTree.tags = [...(runTree.tags ?? []), "runtime-tag"];
            return completion.choices[0].message.content;
        },
        {
            run_type: "llm",
            name: "OpenAI Call Traceable",
            tags: ["my-tag"],
            metadata: { "my-key": "my-value" },
        }
    );

// Call the traceable function
await traceableCallOpenAI(messages);
```

</CodeGroup>