---
title: LangSmith로 SWE-bench 실행하기
sidebarTitle: SWE-bench 실행
---

SWE-bench는 개발자들이 코딩 에이전트를 테스트하기 위해 사용하는 가장 인기 있는 (그리고 어려운!) 벤치마크 중 하나입니다. 이 가이드에서는 SWE-bench 데이터셋을 LangSmith에 로드하고 쉽게 평가를 실행하는 방법을 보여드립니다. 이를 통해 기본 제공되는 SWE-bench 평가 도구를 사용하는 것보다 에이전트의 동작을 훨씬 더 잘 파악할 수 있습니다. 이를 통해 특정 문제를 더 빠르게 찾아내고 에이전트를 신속하게 반복 개선하여 성능을 향상시킬 수 있습니다!

## 데이터 로드하기

데이터를 로드하기 위해 Hugging Face에서 `dev` split을 가져오겠지만, 사용 사례에 따라 `test` 또는 `train` split 중 하나를 가져올 수 있으며, 여러 split을 결합하려면 `pd.concat`을 사용할 수 있습니다.

```python
import pandas as pd

splits = {
    'dev': 'data/dev-00000-of-00001.parquet',
    'test': 'data/test-00000-of-00001.parquet',
    'train': 'data/train-00000-of-00001.parquet'
}

df = pd.read_parquet("hf://datasets/princeton-nlp/SWE-bench/" + splits["dev"])
```

### 'version' 컬럼 수정하기

<Note>
이것은 매우 중요한 단계입니다! 건너뛰면 나머지 코드가 작동하지 않습니다!
</Note>

`version` 컬럼은 모든 문자열 값을 포함하지만 모두 float 형식이므로 CSV를 업로드하여 LangSmith 데이터셋을 생성할 때 float로 변환됩니다. 실험 중에 값을 문자열로 변환할 수 있지만, `"0.10"`과 같은 값에서 문제가 발생합니다. float로 변환되면 `0.1` 값을 얻게 되고, 이를 문자열로 변환하면 `"0.1"`이 되어 제안된 패치를 실행하는 동안 키 오류가 발생합니다.

이 문제를 해결하려면 LangSmith가 `version` 컬럼을 float로 변환하지 않도록 해야 합니다. 이를 위해 float와 호환되지 않는 문자열 접두사를 각 값에 추가할 수 있습니다. 그런 다음 평가를 수행할 때 이 접두사를 분리하여 실제 `version` 값을 가져와야 합니다. 여기서 선택한 접두사는 문자열 `"version:"`입니다.

<Note>
이 해결 방법을 사용하지 않아도 되도록, CSV를 LangSmith에 업로드할 때 컬럼 타입을 선택하는 기능이 향후 추가될 예정입니다.
</Note>

```python
df['version'] = df['version'].apply(lambda x: f"version:{x}")
```

## LangSmith에 데이터 업로드하기

### CSV로 저장하기

데이터를 LangSmith에 업로드하려면 먼저 CSV로 저장해야 하며, pandas에서 제공하는 `to_csv` 함수를 사용하여 저장할 수 있습니다. 쉽게 접근할 수 있는 위치에 이 파일을 저장하세요.

```python
df.to_csv("./../SWE-bench.csv",index=False)
```

### LangSmith에 수동으로 CSV 업로드하기

이제 CSV를 LangSmith에 업로드할 준비가 되었습니다. [LangSmith 웹사이트](https://smith.langchain.com)에 접속한 후, 왼쪽 사이드 네비게이션 바에서 `Datasets & Testing` 탭으로 이동한 다음 오른쪽 상단의 `+ New Dataset` 버튼을 클릭하세요.

그런 다음 상단의 `Upload CSV` 버튼을 클릭하고 이전 단계에서 저장한 CSV 파일을 선택하세요. 그런 다음 데이터셋에 이름과 설명을 지정할 수 있습니다.

다음으로 데이터셋 타입으로 `Key-Value`를 선택하세요. 마지막으로 `Create Schema` 섹션으로 이동하여 모든 키를 `Input fields`로 추가하세요. 이 예제에는 `Output fields`가 없습니다. 평가자가 참조와 비교하는 대신 실험의 출력을 docker 컨테이너에서 실행하여 코드가 실제로 PR 이슈를 해결하는지 확인하기 때문입니다.

`Input fields`를 채우고 (`Output fields`는 비워둔 상태로!) 오른쪽 상단의 파란색 `Create` 버튼을 클릭하면 데이터셋이 생성됩니다!

### 프로그래밍 방식으로 LangSmith에 CSV 업로드하기

또는 아래 코드 블록에 표시된 것처럼 SDK를 사용하여 csv를 LangSmith에 업로드할 수 있습니다:

```python
dataset = client.upload_csv(
    csv_file="./../SWE-bench-dev.csv",
    input_keys=list(df.columns),
    output_keys=[],
    name="swe-bench-programatic-upload",
    description="SWE-bench dataset",
    data_type="kv"
)
```

### 빠른 테스트를 위한 데이터셋 split 생성하기

모든 예제에서 SWE-bench 평가자를 실행하는 데 시간이 오래 걸리므로, 평가자와 코드를 빠르게 테스트하기 위한 "test" split을 생성할 수 있습니다. 데이터셋 split 관리에 대한 자세한 내용은 [이 가이드](/langsmith/manage-datasets-in-application#create-and-manage-dataset-splits)를 참조하세요.

## prediction 함수 실행하기

SWE-bench에 대한 평가 실행은 참조 출력이 없기 때문에 LangSmith에서 일반적으로 실행하는 대부분의 평가와는 조금 다르게 작동합니다. 따라서 먼저 평가자를 실행하지 않고 모든 출력을 생성합니다 (`evaluate` 호출에 `evaluators` 매개변수가 설정되지 않은 것을 확인하세요). 이 경우 더미 predict 함수를 반환했지만, `predict` 함수 내부에 에이전트 로직을 삽입하여 의도한 대로 작동하도록 만들 수 있습니다.

```python
from langsmith import evaluate
from langsmith import Client

client = Client()

def predict(inputs: dict):
    return {
        "instance_id": inputs['instance_id'],
        "model_patch": "None",
        "model_name_or_path": "test-model"
    }

result = evaluate(
    predict,
    data=client.list_examples(
        dataset_id="a9bffcdf-1dfe-4aef-8805-8806f0110067",
        splits=["test"]
    ),
)
```

실험에 대한 평가 결과 보기: ['perfect-lip-22'](https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/a9bffcdf-1dfe-4aef-8805-8806f0110067/compare?selectedSessions=182de5dc-fc9d-4065-a3e1-34527f952fd8)

## SWE-bench를 사용하여 예측 평가하기

이제 다음 코드를 실행하여 위에서 생성한 예측 패치를 Docker에서 실행할 수 있습니다. 이 코드는 `SWE-bench` [run\_evaluation.py](https://github.com/princeton-nlp/SWE-bench/blob/main/swebench/harness/run_evaluation.py) 파일에서 약간 수정되었습니다.

기본적으로 이 코드는 예측을 병렬로 실행하기 위해 docker 이미지를 설정하여 평가에 필요한 시간을 크게 줄입니다. 이 스크린샷은 `SWE-bench`가 내부적으로 평가를 수행하는 방법의 기본 사항을 설명합니다. 전체적으로 이해하려면 [GitHub 저장소](https://github.com/princeton-nlp/SWE-bench)의 코드를 읽어보세요.

![Eval Diagram](/langsmith/images/swebench-evaluation.png)

`convert_runs_to_langsmith_feedback` 함수는 docker 파일에서 생성된 로그를 LangSmith의 일반적인 key/score 방식의 피드백을 포함하는 깔끔한 .json 파일로 변환합니다.

```python
from swebench.harness.run_evaluation import run_instances
import resource
import docker
from swebench.harness.docker_utils import list_images, clean_images
from swebench.harness.docker_build import build_env_images
from pathlib import Path
import json
import os

RUN_EVALUATION_LOG_DIR = Path("logs/run_evaluation")
LANGSMITH_EVALUATION_DIR = './langsmith_feedback/feedback.json'

def convert_runs_to_langsmith_feedback(
    predictions: dict,
    full_dataset: list,
    run_id: str
) -> float:
    """
    Convert logs from docker containers into LangSmith feedback.
    Args:
        predictions (dict): Predictions dict generated by the model
        full_dataset (list): List of all instances
        run_id (str): Run ID
    """
    feedback_for_all_instances = {}
    for instance in full_dataset:
        feedback_for_instance = []
        instance_id = instance['instance_id']
        prediction = predictions[instance_id]

        if prediction.get("model_patch", None) in ["", None]:
            # Prediction returned an empty patch
            feedback_for_all_instances[prediction['run_id']] = [
                {"key": "non-empty-patch", "score": 0},
                {"key": "completed-patch", "score": 0},
                {"key": "resolved-patch", "score": 0}
            ]
            continue

        feedback_for_instance.append({"key": "non-empty-patch", "score": 1})
        report_file = (
            RUN_EVALUATION_LOG_DIR
            / run_id
            / prediction["model_name_or_path"].replace("/", "__")
            / prediction['instance_id']
            / "report.json"
        )

        if report_file.exists():
            # If report file exists, then the instance has been run
            feedback_for_instance.append({"key": "completed-patch", "score": 1})
            report = json.loads(report_file.read_text())
            # Check if instance actually resolved the PR
            if report[instance_id]["resolved"]:
                feedback_for_instance.append({"key": "resolved-patch", "score": 1})
            else:
                feedback_for_instance.append({"key": "resolved-patch", "score": 0})
        else:
            # The instance did not run successfully
            feedback_for_instance += [
                {"key": "completed-patch", "score": 0},
                {"key": "resolved-patch", "score": 0}
            ]
        feedback_for_all_instances[prediction['run_id']] = feedback_for_instance

    os.makedirs(os.path.dirname(LANGSMITH_EVALUATION_DIR), exist_ok=True)
    with open(LANGSMITH_EVALUATION_DIR, 'w') as json_file:
        json.dump(feedback_for_all_instances, json_file)

def evaluate_predictions(
    dataset: list,
    predictions: list,
    max_workers: int,
    force_rebuild: bool,
    cache_level: str,
    clean: bool,
    open_file_limit: int,
    run_id: str,
    timeout: int,
):
    """
    Run evaluation harness for the given dataset and predictions.
    """
    # set open file limit
    assert len(run_id) > 0, "Run ID must be provided"
    resource.setrlimit(resource.RLIMIT_NOFILE, (open_file_limit, open_file_limit))
    client = docker.from_env()
    existing_images = list_images(client)
    print(f"Running {len(dataset)} unevaluated instances...")

    # build environment images + run instances
    build_env_images(client, dataset, force_rebuild, max_workers)
    run_instances(predictions, dataset, cache_level, clean, force_rebuild, max_workers, run_id, timeout)

    # clean images + make final report
    clean_images(client, existing_images, cache_level, clean)
    convert_runs_to_langsmith_feedback(predictions, dataset, run_id)
```

```python
dataset = []
predictions = {}

for res in result:
    predictions[res['run'].outputs['instance_id']] = {
        **res['run'].outputs,
        **{"run_id": str(res['run'].id)}
    }
    dataset.append(res['run'].inputs['inputs'])

for d in dataset:
    d['version'] = d['version'].split(":")[1]
```

```python
evaluate_predictions(
    dataset,
    predictions,
    max_workers=8,
    force_rebuild=False,
    cache_level="env",
    clean=False,
    open_file_limit=4096,
    run_id="test",
    timeout=1_800
)
```

```bash
    Running 3 unevaluated instances...
    Base image sweb.base.arm64:latest already exists, skipping build.
    Base images built successfully.
    Total environment images to build: 2
    Building environment images: 100%|██████████| 2/2 [00:47<00:00, 23.94s/it]
    All environment images built successfully.
    Running 3 instances...
      0%|          | 0/3 [00:00<?, ?it/s]
    Evaluation error for sqlfluff__sqlfluff-884: >>>>> Patch Apply Failed:
    patch unexpectedly ends in middle of line
    patch: **** Only garbage was found in the patch input.
    Check (logs/run_evaluation/test/test-model/sqlfluff__sqlfluff-884/run_instance.log) for more information.
    Evaluation error for sqlfluff__sqlfluff-4151: >>>>> Patch Apply Failed:
    patch unexpectedly ends in middle of line
    patch: **** Only garbage was found in the patch input.
    Check (logs/run_evaluation/test/test-model/sqlfluff__sqlfluff-4151/run_instance.log) for more information.
    Evaluation error for sqlfluff__sqlfluff-2849: >>>>> Patch Apply Failed:
    patch: **** Only garbage was found in the patch input.
    patch unexpectedly ends in middle of line
    Check (logs/run_evaluation/test/test-model/sqlfluff__sqlfluff-2849/run_instance.log) for more information.
    100%|██████████| 3/3 [00:30<00:00, 10.04s/it]
    All instances run.
    Cleaning cached images...
    Removed 0 images.
```

## LangSmith로 평가 전송하기

이제 `evaluate_existing` 함수를 사용하여 평가 피드백을 LangSmith로 실제로 전송할 수 있습니다. 위의 `convert_runs_to_langsmith_feedback` 함수가 모든 피드백을 단일 파일에 저장하여 작업을 매우 쉽게 만들었기 때문에 이 경우 평가 함수는 매우 간단합니다.

```python
from langsmith import evaluate_existing
from langsmith.schemas import Example, Run

def swe_bench_evaluator(run: Run, example: Example):
    with open(LANGSMITH_EVALUATION_DIR, 'r') as json_file:
        langsmith_eval = json.load(json_file)
    return {"results": langsmith_eval[str(run.id)]}

experiment_name = result.experiment_name
evaluate_existing(experiment_name, evaluators=[swe_bench_evaluator])
```

```bash
    View the evaluation results for experiment: 'perfect-lip-22' at:
    https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/a9bffcdf-1dfe-4aef-8805-8806f0110067/compare?selectedSessions=182de5dc-fc9d-4065-a3e1-34527f952fd8
    3it [00:01,  1.52it/s]
    <ExperimentResults perfect-lip-22>
```

실행 후 데이터셋의 experiments 탭으로 이동하여 피드백 키가 제대로 할당되었는지 확인할 수 있습니다. 제대로 할당되었다면 다음 이미지와 유사한 내용이 표시됩니다:

![LangSmith feedback](/langsmith/images/swebench-langsmith-feedback.png)