```mdx
---
title: Zep Cloud
---

> [Zep](https://www.getzep.com)은 AI Assistant 앱을 위한 장기 메모리 서비스입니다.
> Zep을 사용하면 AI 어시스턴트가 아무리 오래된 대화라도 기억할 수 있도록 하면서
> 환각, 지연 시간 및 비용을 줄일 수 있습니다.

**참고:** `ZepCloudVectorStore`는 `Documents`와 함께 작동하며 `Retriever`로 사용하도록 설계되었습니다.
이는 사용자의 채팅 기록을 유지, 강화 및 검색하도록 설계된 Zep의 `ZepCloudMemory` 클래스와는 별도의 기능을 제공합니다.

## 왜 Zep의 VectorStore를 사용해야 할까요? 🤖🚀

Zep은 Zep 서버에 로컬로 있는 저지연 모델을 사용하여 Zep Vector Store에 추가된 문서를 자동으로 임베딩합니다.
Zep TS/JS 클라이언트는 Node가 아닌 엣지 환경에서도 사용할 수 있습니다. 이 두 가지와 Zep의 채팅 메모리 기능을
결합하면 지연 시간과 성능이 중요한 대화형 LLM 앱을 구축하는 데 Zep이 이상적입니다.

### 지원되는 검색 유형

Zep은 유사도 검색과 Maximal Marginal Relevance (MMR) 검색을 모두 지원합니다. MMR 검색은
반환된 문서의 다양성을 보장하기 위해 결과를 재순위화하므로 Retrieval Augmented Generation 애플리케이션에 특히 유용합니다.

## 설치

[Zep Cloud](https://app.getzep.com/)에 가입하고 프로젝트를 생성하세요.

[Zep Cloud Typescript SDK 설치 가이드](https://help.getzep.com/sdks)를 따라 Zep을 설치하고 시작하세요.

## 사용법

Zep VectorStore를 사용하려면 Zep Cloud Project API Key가 필요합니다. 자세한 내용은 [Zep Cloud 문서](https://help.getzep.com/projects)를 참조하세요.

Zep은 기본적으로 모든 문서를 자동으로 임베딩하며, 사용자로부터 임베딩을 받을 것으로 예상하지 않습니다.
LangChain은 @[`Embeddings`] 인스턴스를 전달해야 하므로 `FakeEmbeddings`를 전달합니다.

<Tip>
[LangChain 패키지 설치에 대한 일반 지침은 이 섹션을 참조하세요](/oss/langchain/install).
</Tip>

### 예제: Documents로부터 ZepVectorStore 생성 및 쿼리하기

```bash npm
npm install @getzep/zep-cloud @langchain/openai @langchain/community @langchain/core
```

```typescript
import { ZepCloudVectorStore } from "@langchain/community/vectorstores/zep_cloud";
import { FakeEmbeddings } from "@langchain/core/utils/testing";
import { TextLoader } from "@langchain/classic/document_loaders/fs/text";
import { randomUUID } from "crypto";

const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();
const collectionName = `collection${randomUUID().split("-")[0]}`;

const zepConfig = {
  // Your Zep Cloud Project API key https://help.getzep.com/projects
  apiKey: "<Zep Api Key>",
  collectionName,
};

// We're using fake embeddings here, because Zep Cloud handles embedding for you
const embeddings = new FakeEmbeddings();

const vectorStore = await ZepCloudVectorStore.fromDocuments(
  docs,
  embeddings,
  zepConfig
);

// Wait for the documents to be embedded
// eslint-disable-next-line no-constant-condition
while (true) {
  const c = await vectorStore.client.document.getCollection(collectionName);
  console.log(
    `Embedding status: ${c.documentEmbeddedCount}/${c.documentCount} documents embedded`
  );
  // eslint-disable-next-line no-promise-executor-return
  await new Promise((resolve) => setTimeout(resolve, 1000));
  if (c.documentEmbeddedCount === c.documentCount) {
    break;
  }
}

const results = await vectorStore.similaritySearchWithScore("bar", 3);

console.log("Similarity Results:");
console.log(JSON.stringify(results));

const results2 = await vectorStore.maxMarginalRelevanceSearch("bar", {
  k: 3,
});

console.log("MMR Results:");
console.log(JSON.stringify(results2));
```

### 예제: Expression Language와 함께 ZepCloudVectorStore 사용하기

```typescript
import { ZepClient } from "@getzep/zep-cloud";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ConsoleCallbackHandler } from "@langchain/core/tracers/console";
import { ChatOpenAI } from "@langchain/openai";
import { Document } from "@langchain/core/documents";
import {
  RunnableLambda,
  RunnableMap,
  RunnablePassthrough,
} from "@langchain/core/runnables";
import { ZepCloudVectorStore } from "@langchain/community/vectorstores/zep_cloud";
import { StringOutputParser } from "@langchain/core/output_parsers";

async function combineDocuments(docs: Document[], documentSeparator = "\n\n") {
  const docStrings: string[] = await Promise.all(
    docs.map((doc) => doc.pageContent)
  );
  return docStrings.join(documentSeparator);
}

// Your Zep Collection Name
const collectionName = "<Zep Collection Name>";

const zepClient = new ZepClient({
  // Your Zep Cloud Project API key https://help.getzep.com/projects
  apiKey: "<Zep Api Key>",
});

const vectorStore = await ZepCloudVectorStore.init({
  client: zepClient,
  collectionName,
});

const prompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    `Answer the question based only on the following context: {context}`,
  ],
  ["human", "{question}"],
]);

const model = new ChatOpenAI({
  temperature: 0.8,
  model: "gpt-3.5-turbo-1106",
});
const retriever = vectorStore.asRetriever();

const setupAndRetrieval = RunnableMap.from({
  context: new RunnableLambda({
    func: (input: string) => retriever.invoke(input).then(combineDocuments),
  }),
  question: new RunnablePassthrough(),
});
const outputParser = new StringOutputParser();

const chain = setupAndRetrieval
  .pipe(prompt)
  .pipe(model)
  .pipe(outputParser)
  .withConfig({
    callbacks: [new ConsoleCallbackHandler()],
  });

const result = await chain.invoke("Project Gutenberg?");

console.log("result", result);
```

## 관련 문서

- Vector store [개념 가이드](/oss/integrations/vectorstores)
- Vector store [사용 방법 가이드](/oss/integrations/vectorstores)
```