---
title: ChatLlamaCpp
---

<Tip>
**호환성**

Node.js에서만 사용 가능합니다.
</Tip>

이 모듈은 [llama.cpp](https://github.com/ggerganov/llama.cpp)를 위한 [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) Node.js 바인딩을 기반으로 하며, 로컬에서 실행되는 LLM과 작업할 수 있게 해줍니다. 이를 통해 노트북 환경에서 실행할 수 있는 훨씬 작은 양자화된 모델을 사용할 수 있으며, 비용 부담 없이 테스트하고 아이디어를 스케치하는 데 이상적입니다!

## Setup

로컬 모델과 통신하려면 [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) 모듈의 메이저 버전 `3`을 설치해야 합니다.

<Tip>
LangChain 패키지 설치에 대한 일반적인 지침은 [이 섹션](/oss/langchain/install)을 참조하세요.
</Tip>

```bash npm
npm install -S node-llama-cpp@3 @langchain/community @langchain/core
```

또한 로컬 Llama 3 모델(또는 [node-llama-cpp](https://github.com/withcatai/node-llama-cpp)에서 지원하는 모델)이 필요합니다. 이 모델의 경로를 매개변수의 일부로 LlamaCpp 모듈에 전달해야 합니다(예제 참조).

기본적으로 `node-llama-cpp`는 Apple M 시리즈 프로세서의 Metal GPU를 지원하는 MacOS 플랫폼에서 실행되도록 조정되어 있습니다. 이 기능을 끄거나 CUDA 아키텍처 지원이 필요한 경우 [node-llama-cpp](https://withcatai.github.io/node-llama-cpp/)의 문서를 참조하세요.

`llama3` 획득 및 준비에 대한 조언은 이 모듈의 LLM 버전 문서를 참조하세요.

LangChain.js 기여자를 위한 참고사항: 이 모듈과 관련된 테스트를 실행하려면 로컬 모델의 경로를 환경 변수 `LLAMA_PATH`에 넣어야 합니다.

## Usage

### 기본 사용법

이 경우 메시지로 래핑된 프롬프트를 전달하고 응답을 기대합니다.

```typescript
import { ChatLlamaCpp } from "@langchain/community/chat_models/llama_cpp";
import { HumanMessage } from "@langchain/core/messages";

const llamaPath = "/Replace/with/path/to/your/model/gguf-llama3-Q4_0.bin";

const model = await ChatLlamaCpp.initialize({ modelPath: llamaPath });

const response = await model.invoke([
  new HumanMessage({ content: "My name is John." }),
]);
console.log({ response });

/*
  AIMessage {
    lc_serializable: true,
    lc_kwargs: {
      content: 'Hello John.',
      additional_kwargs: {}
    },
    lc_namespace: [ 'langchain', 'schema' ],
    content: 'Hello John.',
    name: undefined,
    additional_kwargs: {}
  }
*/
```

### System messages

system message도 제공할 수 있으며, `llama_cpp` 모듈에서 system message는 새 세션 생성을 유발합니다.

```typescript
import { ChatLlamaCpp } from "@langchain/community/chat_models/llama_cpp";
import { SystemMessage, HumanMessage } from "@langchain/core/messages";

const llamaPath = "/Replace/with/path/to/your/model/gguf-llama3-Q4_0.bin";

const model = await ChatLlamaCpp.initialize({ modelPath: llamaPath });

const response = await model.invoke([
  new SystemMessage(
    "You are a pirate, responses must be very verbose and in pirate dialect, add 'Arr, m'hearty!' to each sentence."
  ),
  new HumanMessage("Tell me where Llamas come from?"),
]);
console.log({ response });

/*
  AIMessage {
    lc_serializable: true,
    lc_kwargs: {
      content: "Arr, m'hearty! Llamas come from the land of Peru.",
      additional_kwargs: {}
    },
    lc_namespace: [ 'langchain', 'schema' ],
    content: "Arr, m'hearty! Llamas come from the land of Peru.",
    name: undefined,
    additional_kwargs: {}
  }
*/
```

### Chains

이 모듈은 chain과 함께 사용할 수도 있습니다. 더 복잡한 chain을 사용하려면 70B 버전과 같이 충분히 강력한 `llama3` 버전이 필요합니다.

```typescript
import { ChatLlamaCpp } from "@langchain/community/chat_models/llama_cpp";
import { LLMChain } from "@langchain/classic/chains";
import { PromptTemplate } from "@langchain/core/prompts";

const llamaPath = "/Replace/with/path/to/your/model/gguf-llama3-Q4_0.bin";

const model = await ChatLlamaCpp.initialize({
  modelPath: llamaPath,
  temperature: 0.5,
});

const prompt = PromptTemplate.fromTemplate(
  "What is a good name for a company that makes {product}?"
);
const chain = new LLMChain({ llm: model, prompt });

const response = await chain.invoke({ product: "colorful socks" });

console.log({ response });

/*
  {
  text: `I'm not sure what you mean by "colorful socks" but here are some ideas:\n` +
    '\n' +
    '- Sock-it to me!\n' +
    '- Socks Away\n' +
    '- Fancy Footwear'
  }
*/
```

### Streaming

Llama CPP로 스트리밍할 수도 있습니다. 원시 '단일 프롬프트' 문자열을 사용할 수 있습니다:

```typescript
import { ChatLlamaCpp } from "@langchain/community/chat_models/llama_cpp";

const llamaPath = "/Replace/with/path/to/your/model/gguf-llama3-Q4_0.bin";

const model = await ChatLlamaCpp.initialize({
  modelPath: llamaPath,
  temperature: 0.7,
});

const stream = await model.stream("Tell me a short story about a happy Llama.");

for await (const chunk of stream) {
  console.log(chunk.content);
}

/*

  Once
   upon
   a
   time
  ,
   in
   a
   green
   and
   sunny
   field
  ...
*/
```

또는 여러 메시지를 제공할 수 있으며, 이는 입력을 받아 Llama3 형식의 프롬프트를 모델에 제출합니다.

```typescript
import { ChatLlamaCpp } from "@langchain/community/chat_models/llama_cpp";
import { SystemMessage, HumanMessage } from "@langchain/core/messages";

const llamaPath = "/Replace/with/path/to/your/model/gguf-llama3-Q4_0.bin";

const llamaCpp = await ChatLlamaCpp.initialize({
  modelPath: llamaPath,
  temperature: 0.7,
});

const stream = await llamaCpp.stream([
  new SystemMessage(
    "You are a pirate, responses must be very verbose and in pirate dialect."
  ),
  new HumanMessage("Tell me about Llamas?"),
]);

for await (const chunk of stream) {
  console.log(chunk.content);
}

/*

  Ar
  rr
  r
  ,
   me
   heart
  y
  !

   Ye
   be
   ask
  in
  '
   about
   llam
  as
  ,
   e
  h
  ?
  ...
*/
```

`invoke` 메서드를 사용하여 스트림 생성을 수행하고 `signal`을 사용하여 생성을 중단할 수도 있습니다.

```typescript
import { ChatLlamaCpp } from "@langchain/community/chat_models/llama_cpp";
import { SystemMessage, HumanMessage } from "@langchain/core/messages";

const llamaPath = "/Replace/with/path/to/your/model/gguf-llama3-Q4_0.bin";

const model = await ChatLlamaCpp.initialize({
  modelPath: llamaPath,
  temperature: 0.7,
});

const controller = new AbortController();

setTimeout(() => {
  controller.abort();
  console.log("Aborted");
}, 5000);

await model.invoke(
  [
    new SystemMessage(
      "You are a pirate, responses must be very verbose and in pirate dialect."
    ),
    new HumanMessage("Tell me about Llamas?"),
  ],
  {
    signal: controller.signal,
    callbacks: [
      {
        handleLLMNewToken(token) {
          console.log(token);
        },
      },
    ],
  }
);
/*

  Once
   upon
   a
   time
  ,
   in
   a
   green
   and
   sunny
   field
  ...
  Aborted

  AbortError

*/
```

## Related

- Chat model [개념 가이드](/oss/langchain/models)
- Chat model [사용 방법 가이드](/oss/langchain/models)