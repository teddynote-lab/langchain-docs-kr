---
title: Llama CPP
---

<Tip>
**호환성**

Node.js에서만 사용 가능합니다.
</Tip>

이 모듈은 [llama.cpp](https://github.com/ggerganov/llama.cpp)를 위한 [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) Node.js 바인딩을 기반으로 하며, 로컬에서 실행되는 LLM과 작업할 수 있게 해줍니다. 이를 통해 노트북 환경에서 실행할 수 있는 훨씬 작은 양자화된 모델로 작업할 수 있으며, 비용 부담 없이 테스트하고 아이디어를 스케치하는 데 이상적입니다!

## Setup

로컬 모델과 통신하려면 [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) 모듈의 메이저 버전 `3`을 설치해야 합니다.

```bash npm
npm install -S node-llama-cpp@3
```
<Tip>
[LangChain 패키지 설치에 대한 일반 지침은 이 섹션을 참조하세요](/oss/langchain/install).
</Tip>

```bash npm
npm install @langchain/community @langchain/core
```
또한 로컬 Llama 3 모델(또는 [node-llama-cpp](https://github.com/withcatai/node-llama-cpp)에서 지원하는 모델)이 필요합니다. 이 모델의 경로를 매개변수의 일부로 LlamaCpp 모듈에 전달해야 합니다(예제 참조).

기본적으로 `node-llama-cpp`는 Apple M 시리즈 프로세서의 Metal GPU를 지원하는 MacOS 플랫폼에서 실행되도록 조정되어 있습니다. 이 기능을 끄거나 CUDA 아키텍처에 대한 지원이 필요한 경우 [node-llama-cpp](https://withcatai.github.io/node-llama-cpp/)의 문서를 참조하세요.

LangChain.js 기여자를 위한 참고사항: 이 모듈과 관련된 테스트를 실행하려면 로컬 모델의 경로를 환경 변수 `LLAMA_PATH`에 넣어야 합니다.

## Llama3 설치 가이드

로컬 Llama3 모델을 머신에서 실행하는 것이 전제 조건이므로, 가장 작은 Llama 3.1-8B를 가져와서 빌드한 다음 노트북에서 편안하게 실행될 수 있도록 양자화하는 빠른 가이드입니다. 이를 위해서는 머신에 `python3`(3.11 권장)과 `llama.cpp`를 빌드할 수 있도록 `gcc` 및 `make`가 필요합니다.

### Llama3 모델 가져오기

Llama3 사본을 얻으려면 [Meta AI](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)를 방문하여 모델에 대한 액세스를 요청해야 합니다. Meta AI가 액세스 권한을 부여하면 파일에 액세스할 수 있는 고유 URL이 포함된 이메일을 받게 되며, 이는 다음 단계에서 필요합니다.
이제 작업할 디렉토리를 만듭니다. 예를 들어:

```
mkdir llama3
cd llama3
```
이제 [여기](https://github.com/meta-llama/llama-models)에서 찾을 수 있는 Meta AI `llama-models` 저장소로 이동해야 합니다. 저장소에는 원하는 모델을 다운로드하는 지침이 있으며, 이메일에서 받은 고유 URL을 사용해야 합니다.
나머지 튜토리얼에서는 `Llama3.1-8B`를 다운로드했다고 가정하지만, 여기서부터는 모든 모델이 작동해야 합니다. 모델을 다운로드할 때 모델 다운로드 경로를 저장해 두세요. 나중에 사용됩니다.

### 모델 변환 및 양자화

이 단계에서는 `llama.cpp`를 사용해야 하므로 해당 저장소를 다운로드해야 합니다.

```
cd ..
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
```
이제 `llama.cpp` 도구를 빌드하고 `python` 환경을 설정해야 합니다. 이 단계에서는 python 설치가 `python3`를 사용하여 실행될 수 있고 가상 환경이 `llama3`로 호출될 수 있다고 가정합니다. 자신의 상황에 맞게 조정하세요.

```
cmake -B build
cmake --build build --config Release
python3 -m venv llama3
source llama3/bin/activate
```
llama3 환경을 활성화한 후 명령 프롬프트 앞에 `(llama3)`가 표시되어 이것이 활성 환경임을 알려줍니다. 참고: 다른 모델을 빌드하거나 모델을 다시 양자화하기 위해 돌아와야 하는 경우 환경을 다시 활성화하는 것을 잊지 마세요. 또한 `llama.cpp`를 업데이트하는 경우 도구를 다시 빌드하고 새로운 또는 업데이트된 종속성을 설치해야 할 수 있습니다! 이제 활성 python 환경이 있으므로 python 종속성을 설치해야 합니다.

```
python3 -m pip install -r requirements.txt
```
이 작업을 완료하면 `llama.cpp`를 통해 로컬에서 사용할 수 있도록 Llama3 모델을 변환하고 양자화할 수 있습니다. Hugging Face 모델로의 변환이 필요하고, 그 다음 GGUF 모델로의 변환이 필요합니다.
먼저 다음 스크립트 `convert_llama_weights_to_hf.py`가 있는 경로를 찾아야 합니다. 이 스크립트를 현재 작업 디렉토리에 복사하여 붙여넣으세요. 스크립트를 사용하려면 추가 종속성을 pip install해야 할 수 있으니 필요에 따라 설치하세요.
그런 다음 모델을 변환해야 합니다. 변환하기 전에 Hugging Face 변환과 최종 모델을 저장할 디렉토리를 만들어 보겠습니다.

```
mkdir models/8B
mkdir models/8B-GGUF
python3 convert_llama_weights_to_hf.py --model_size 8B --input_dir <dir-to-your-model> --output_dir models/8B --llama_version 3
python3 convert_hf_to_gguf.py --outtype f16 --outfile models/8B-GGUF/gguf-llama3-f16.bin models/8B
```
이렇게 하면 생성한 디렉토리에 변환된 Hugging Face 모델과 최종 GGUF 모델이 생성됩니다. 이것은 단지 변환된 모델이므로 크기도 약 16Gb입니다. 다음 단계에서는 약 4Gb로 양자화합니다.

```
./build/bin/llama-quantize ./models/8B-GGUF/gguf-llama3-f16.bin ./models/8B-GGUF/gguf-llama3-Q4_0.bin Q4_0
```
이것을 실행하면 `models\8B-GGUF` 디렉토리에 새 모델이 생성되며, 이름은 `gguf-llama3-Q4_0.bin`입니다. 이것이 langchain과 함께 사용할 수 있는 모델입니다. `llama.cpp` 도구를 사용하여 테스트하여 이 모델이 작동하는지 확인할 수 있습니다.

```
./build/bin/llama-cli -m ./models/8B-GGUF/gguf-llama3-Q4_0.bin -cnv -p "You are a helpful assistant"
```

이 명령을 실행하면 채팅 세션을 위한 모델이 시작됩니다. 참고로 디스크 공간이 부족한 경우 이 작은 모델만 필요하므로 원본 및 변환된 13.5Gb 모델을 백업 및/또는 삭제할 수 있습니다.

## Usage

```typescript
import { LlamaCpp } from "@langchain/community/llms/llama_cpp";

const llamaPath = "/Replace/with/path/to/your/model/gguf-llama3-Q4_0.bin";
const question = "Where do Llamas come from?";

const model = await LlamaCpp.initialize({ modelPath: llamaPath });

console.log(`You: ${question}`);
const response = await model.invoke(question);
console.log(`AI : ${response}`);
```

## Streaming

```typescript
import { LlamaCpp } from "@langchain/community/llms/llama_cpp";

const llamaPath = "/Replace/with/path/to/your/model/gguf-llama3-Q4_0.bin";

const model = await LlamaCpp.initialize({
  modelPath: llamaPath,
  temperature: 0.7,
});

const prompt = "Tell me a short story about a happy Llama.";

const stream = await model.stream(prompt);

for await (const chunk of stream) {
  console.log(chunk);
}

/*


 Once
  upon
  a
  time
 ,
  in
  the
  rolling
  hills
  of
  Peru
 ...
 */
```;

## Related


- [Models 가이드](/oss/langchain/models)


CRITICAL: Provide ONLY the translated MDX content without any additional explanation, comments, or questions. If the content is empty or contains only frontmatter, return it exactly as provided.