---
title: NVIDIA
---

이 문서는 NVIDIA 모델을 시작하는 데 도움이 됩니다. 모든 `NVIDIA` 기능 및 구성에 대한 자세한 문서는 [API reference](https://python.langchain.com/api_reference/nvidia_ai_endpoints/llms/langchain_nvidia_ai_endpoints.chat_models.NVIDIA.html)를 참조하세요.

## Overview

`langchain-nvidia-ai-endpoints` 패키지는 NVIDIA NIM inference microservice의 모델을 사용하여 애플리케이션을 구축하는 LangChain integration을 포함하고 있습니다. 이러한 모델은 NVIDIA 가속 인프라에서 최고의 성능을 제공하도록 NVIDIA에 의해 최적화되었으며, NVIDIA 가속 인프라에서 단일 명령으로 어디서나 배포할 수 있는 사용하기 쉬운 사전 구축 컨테이너인 NIM으로 배포됩니다.

NVIDIA가 호스팅하는 NIM 배포는 [NVIDIA API catalog](https://build.nvidia.com/)에서 테스트할 수 있습니다. 테스트 후, NIM은 NVIDIA AI Enterprise 라이선스를 사용하여 NVIDIA의 API catalog에서 내보낼 수 있으며 온프레미스 또는 클라우드에서 실행할 수 있어, 기업이 IP 및 AI 애플리케이션에 대한 소유권과 완전한 제어권을 가질 수 있습니다.

NIM은 모델별로 컨테이너 이미지로 패키징되며 NVIDIA NGC Catalog을 통해 NGC 컨테이너 이미지로 배포됩니다. 핵심적으로 NIM은 AI 모델에서 inference를 실행하기 위한 쉽고 일관되며 친숙한 API를 제공합니다.

이 예제는 `NVIDIA` 클래스를 통해 지원되는 NVIDIA와 상호 작용하기 위해 LangChain을 사용하는 방법을 다룹니다.

이 API를 통해 llm 모델에 액세스하는 방법에 대한 자세한 내용은 [NVIDIA](https://python.langchain.com/docs/integrations/llms/nvidia_ai_endpoints/) 문서를 확인하세요.

### Integration details

| Class | Package | Local | Serializable | JS support | Downloads | Version |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [NVIDIA](https://python.langchain.com/api_reference/nvidia_ai_endpoints/llms/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html) | [langchain-nvidia-ai-endpoints](https://python.langchain.com/api_reference/nvidia_ai_endpoints/index.html) | ✅ | beta | ❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain_nvidia_ai_endpoints?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain_nvidia_ai_endpoints?style=flat-square&label=%20) |

### Model features

| JSON mode | [Image input](/oss/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/langchain/streaming/) | Native async | [Token usage](/oss/langchain/models#token-usage) | [Logprobs](/oss/langchain/models#log-probabilities) |
| :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |
| ❌ | ✅ | ❌ | ❌ | ✅ | ❌ | ❌ | ❌ |

## Setup

**시작하기:**

1. NVIDIA AI Foundation 모델을 호스팅하는 [NVIDIA](https://build.nvidia.com/)에서 무료 계정을 생성합니다.

2. 원하는 모델을 클릭합니다.

3. `Input` 아래에서 `Python` 탭을 선택하고 `Get API Key`를 클릭합니다. 그런 다음 `Generate Key`를 클릭합니다.

4. 생성된 키를 `NVIDIA_API_KEY`로 복사하고 저장합니다. 그러면 endpoint에 액세스할 수 있습니다.

### Credentials

```python
import getpass
import os

if not os.getenv("NVIDIA_API_KEY"):
    # Note: the API key should start with "nvapi-"
    os.environ["NVIDIA_API_KEY"] = getpass.getpass("Enter your NVIDIA API key: ")
```

### Installation

LangChain NVIDIA AI Endpoints integration은 `langchain-nvidia-ai-endpoints` 패키지에 있습니다:

```python
pip install -qU langchain-nvidia-ai-endpoints
```

## Instantiation

전체 기능은 [LLM](/oss/langchain/models)을 참조하세요.

```python
from langchain_nvidia_ai_endpoints import NVIDIA
```

```python
llm = NVIDIA().bind(max_tokens=256)
llm
```

## Invocation

```python
prompt = "# Function that does quicksort written in Rust without comments:"
```

```python
print(llm.invoke(prompt))
```

## Stream, Batch, and Async

이러한 모델은 기본적으로 streaming을 지원하며, 모든 LangChain LLM과 마찬가지로 동시 요청을 처리하기 위한 batch 메서드와 invoke, stream, batch를 위한 async 메서드를 제공합니다. 다음은 몇 가지 예제입니다.

```python
for chunk in llm.stream(prompt):
    print(chunk, end="", flush=True)
```

```python
llm.batch([prompt])
```

```python
await llm.ainvoke(prompt)
```

```python
async for chunk in llm.astream(prompt):
    print(chunk, end="", flush=True)
```

```python
await llm.abatch([prompt])
```

```python
async for chunk in llm.astream_log(prompt):
    print(chunk)
```

```python
response = llm.invoke(
    "X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.1) #Train a logistic regression model, predict the labels on the test set and compute the accuracy score"
)
print(response)
```

## Supported models

`available_models`를 쿼리하면 API 자격 증명으로 제공되는 다른 모든 모델을 확인할 수 있습니다.

```python
NVIDIA.get_available_models()
# llm.get_available_models()
```

## API reference

모든 `NVIDIA` 기능 및 구성에 대한 자세한 문서는 API reference를 참조하세요: [python.langchain.com/api_reference/nvidia_ai_endpoints/llms/langchain_nvidia_ai_endpoints.llms.NVIDIA.html](https://python.langchain.com/api_reference/nvidia_ai_endpoints/llms/langchain_nvidia_ai_endpoints.llms.NVIDIA.html)