---
title: Fireworks
---

<Warning>
**현재 Fireworks 모델을 text completion 모델로 사용하는 방법을 설명하는 페이지를 보고 계십니다. 많은 인기 있는 Fireworks 모델은 [chat completion 모델](/oss/langchain/models)입니다.**

[이 페이지](/oss/integrations/chat/fireworks/)를 찾고 계실 수 있습니다.
</Warning>

>[Fireworks](https://app.fireworks.ai/)는 혁신적인 AI 실험 및 프로덕션 플랫폼을 만들어 생성형 AI 제품 개발을 가속화합니다.

이 예제는 LangChain을 사용하여 `Fireworks` 모델과 상호작용하는 방법을 다룹니다.

## Overview

### Integration details

| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/v0.1/docs/integrations/llms/fireworks/) | Downloads | Version |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [Fireworks](https://python.langchain.com/api_reference/fireworks/llms/langchain_fireworks.llms.Fireworks.html#langchain_fireworks.llms.Fireworks) | [langchain-fireworks](https://python.langchain.com/api_reference/fireworks/index.html) | ❌ | ❌ | ✅ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain_fireworks?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain_fireworks?style=flat-square&label=%20) |

## Setup

### Credentials

[Fireworks AI](http://fireworks.ai)에 로그인하여 모델에 액세스할 수 있는 API Key를 받고, `FIREWORKS_API_KEY` environment variable로 설정되어 있는지 확인하세요.
3. model id를 사용하여 모델을 설정하세요. 모델이 설정되지 않은 경우 기본 모델은 fireworks-llama-v2-7b-chat입니다. [fireworks.ai](https://fireworks.ai)에서 전체 최신 모델 목록을 확인하세요.

```python
import getpass
import os

if "FIREWORKS_API_KEY" not in os.environ:
    os.environ["FIREWORKS_API_KEY"] = getpass.getpass("Fireworks API Key:")
```

### Installation

나머지 노트북이 작동하려면 `langchain-fireworks` python package를 설치해야 합니다.

```python
pip install -qU langchain-fireworks
```

```output
Note: you may need to restart the kernel to use updated packages.
```

## Instantiation

```python
from langchain_fireworks import Fireworks

# Initialize a Fireworks model
llm = Fireworks(
    model="accounts/fireworks/models/llama-v3p1-8b-instruct",
    base_url="https://api.fireworks.ai/inference/v1/completions",
)
```

## Invocation

문자열 prompt로 모델을 직접 호출하여 completion을 얻을 수 있습니다.

```python
output = llm.invoke("Who's the best quarterback in the NFL?")
print(output)
```

```output
 If Manningville Station, Lions rookie EJ Manuel's
```

### 여러 prompt로 호출하기

```python
# Calling multiple prompts
output = llm.generate(
    [
        "Who's the best cricket player in 2016?",
        "Who's the best basketball player in the league?",
    ]
)
print(output.generations)
```

```output
[[Generation(text=" We're not just asking, we've done some research. We'")], [Generation(text=' The conversation is dominated by Kobe Bryant, Dwyane Wade,')]]
```

### 추가 parameter와 함께 호출하기

```python
# Setting additional parameters: temperature, max_tokens, top_p
llm = Fireworks(
    model="accounts/fireworks/models/llama-v3p1-8b-instruct",
    temperature=0.7,
    max_tokens=15,
    top_p=1.0,
)
print(llm.invoke("What's the weather like in Kansas City in December?"))
```

```output
December is a cold month in Kansas City, with temperatures of
```

## Chaining

LangChain Expression Language를 사용하여 non-chat 모델로 간단한 chain을 만들 수 있습니다.

```python
from langchain_core.prompts import PromptTemplate
from langchain_fireworks import Fireworks

llm = Fireworks(
    model="accounts/fireworks/models/llama-v3p1-8b-instruct",
    temperature=0.7,
    max_tokens=15,
    top_p=1.0,
)
prompt = PromptTemplate.from_template("Tell me a joke about {topic}?")
chain = prompt | llm

print(chain.invoke({"topic": "bears"}))
```

```output
 What do you call a bear with no teeth? A gummy bear!
```

## Streaming

원하는 경우 출력을 stream할 수 있습니다.

```python
for token in chain.stream({"topic": "bears"}):
    print(token, end="", flush=True)
```

```output
 Why do bears hate shoes so much? They like to run around in their
```

## API reference

모든 `Fireworks` LLM 기능 및 구성에 대한 자세한 문서는 API reference를 참조하세요: [python.langchain.com/api_reference/fireworks/llms/langchain_fireworks.llms.Fireworks.html#langchain_fireworks.llms.Fireworks](https://python.langchain.com/api_reference/fireworks/llms/langchain_fireworks.llms.Fireworks.html#langchain_fireworks.llms.Fireworks)