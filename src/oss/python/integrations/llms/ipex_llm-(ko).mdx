---
title: IPEX-LLM
---

> [IPEX-LLM](https://github.com/intel-analytics/ipex-llm)은 매우 낮은 지연 시간으로 Intel CPU 및 GPU(예: iGPU가 있는 로컬 PC, Arc, Flex 및 Max와 같은 독립 GPU)에서 LLM을 실행하기 위한 PyTorch 라이브러리입니다.

- [Intel GPU에서 IPEX-LLM 사용하기](#ipex-llm-on-intel-gpu)
- [Intel CPU에서 IPEX-LLM 사용하기](#ipex-llm-on-intel-cpu)

## IPEX-LLM on Intel GPU

이 예제는 Intel GPU에서 텍스트 생성을 위해 LangChain을 사용하여 `ipex-llm`과 상호작용하는 방법을 다룹니다.

> **참고**
>
> Intel Arc A-Series GPU(Intel Arc A300-Series 또는 Pro A60 제외)를 사용하는 Windows 사용자만 "IPEX-LLM on Intel GPU" 섹션에서 Jupyter notebook을 직접 실행하는 것이 권장됩니다. 다른 경우(예: Linux 사용자, Intel iGPU 등)에는 최상의 경험을 위해 터미널에서 Python 스크립트로 코드를 실행하는 것이 권장됩니다.

### Install Prerequisites

Intel GPU에서 IPEX-LLM의 이점을 활용하려면 도구 설치 및 환경 준비를 위한 몇 가지 전제 조건 단계가 있습니다.

Windows 사용자인 경우 [Install IPEX-LLM on Windows with Intel GPU Guide](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_windows_gpu.md)를 방문하여 [Install Prerequisites](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_windows_gpu.md#install-prerequisites)를 따라 GPU 드라이버를 업데이트하고(선택 사항) Conda를 설치하세요.

Linux 사용자인 경우 [Install IPEX-LLM on Linux with Intel GPU](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux_gpu.md)를 방문하여 [**Install Prerequisites**](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux_gpu.md#install-prerequisites)를 따라 GPU 드라이버, Intel® oneAPI Base Toolkit 2024.0 및 Conda를 설치하세요.

### Setup

전제 조건 설치 후, 모든 전제 조건이 설치된 conda 환경을 생성했어야 합니다. **이 conda 환경에서 jupyter 서비스를 시작하세요**:

```python
pip install -qU langchain langchain-community
```

Intel GPU에서 로컬로 LLM을 실행하기 위해 IEPX-LLM을 설치하세요.

```python
pip install --pre --upgrade ipex-llm[xpu] --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/
```

> **참고**
>
> extra-index-url로 `https://pytorch-extension.intel.com/release-whl/stable/xpu/cn/`을 사용할 수도 있습니다.

### Runtime Configuration

최적의 성능을 위해 장치에 따라 여러 환경 변수를 설정하는 것이 권장됩니다:

#### For Windows Users with Intel Core Ultra integrated GPU

```python
import os

os.environ["SYCL_CACHE_PERSISTENT"] = "1"
os.environ["BIGDL_LLM_XMX_DISABLED"] = "1"
```

#### For Windows Users with Intel Arc A-Series GPU

```python
import os

os.environ["SYCL_CACHE_PERSISTENT"] = "1"
```

> **참고**
>
> 각 모델이 Intel iGPU/Intel Arc A300-Series 또는 Pro A60에서 처음 실행될 때 컴파일하는 데 몇 분이 걸릴 수 있습니다.
>
> 다른 GPU 유형의 경우 Windows 사용자는 [여기](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Overview/install_gpu.md#runtime-configuration)를, Linux 사용자는 [여기](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Overview/install_gpu.md#runtime-configuration-1)를 참조하세요.

### Basic Usage

```python
import warnings

from langchain.chains import LLMChain
from langchain_community.llms import IpexLLM
from langchain_core.prompts import PromptTemplate

warnings.filterwarnings("ignore", category=UserWarning, message=".*padding_mask.*")
```

모델에 대한 prompt template을 지정하세요. 이 예제에서는 [vicuna-1.5](https://huggingface.co/lmsys/vicuna-7b-v1.5) 모델을 사용합니다. 다른 모델을 사용하는 경우 그에 맞는 적절한 template을 선택하세요.

```python
template = "USER: {question}\nASSISTANT:"
prompt = PromptTemplate(template=template, input_variables=["question"])
```

`IpexLLM.from_model_id`를 사용하여 IpexLLM으로 모델을 로컬로 로드하세요. Huggingface 형식으로 모델을 직접 로드하고 추론을 위해 자동으로 low-bit 형식으로 변환합니다. LLM 모델을 Intel GPU에 로드하려면 IpexLLM을 초기화할 때 `model_kwargs`에서 `device`를 `"xpu"`로 설정하세요.

```python
llm = IpexLLM.from_model_id(
    model_id="lmsys/vicuna-7b-v1.5",
    model_kwargs={
        "temperature": 0,
        "max_length": 64,
        "trust_remote_code": True,
        "device": "xpu",
    },
)
```

Chains에서 사용하기

```python
llm_chain = prompt | llm

question = "What is AI?"
output = llm_chain.invoke(question)
```

### Save/Load Low-bit Model

또는 low-bit 모델을 디스크에 한 번 저장하고 나중에 사용하기 위해 `from_model_id` 대신 `from_model_id_low_bit`를 사용하여 다시 로드할 수 있습니다 - 다른 머신에서도 가능합니다. low-bit 모델은 원본 모델보다 훨씬 적은 디스크 공간을 필요로 하므로 공간 효율적입니다. 또한 `from_model_id_low_bit`는 모델 변환 단계를 건너뛰므로 속도와 메모리 사용 측면에서 `from_model_id`보다 더 효율적입니다. LLM 모델을 Intel GPU에 로드하려면 `model_kwargs`에서 `device`를 `"xpu"`로 설정할 수 있습니다.

low-bit 모델을 저장하려면 다음과 같이 `save_low_bit`를 사용하세요.

```python
saved_lowbit_model_path = "./vicuna-7b-1.5-low-bit"  # path to save low-bit model
llm.model.save_low_bit(saved_lowbit_model_path)
del llm
```

저장된 lowbit 모델 경로에서 모델을 다음과 같이 로드하세요.
> low-bit 모델의 저장 경로에는 모델 자체만 포함되고 tokenizer는 포함되지 않습니다. 모든 것을 한 곳에 두려면 원본 모델의 디렉토리에서 low-bit 모델이 저장된 위치로 tokenizer 파일을 수동으로 다운로드하거나 복사해야 합니다.

```python
llm_lowbit = IpexLLM.from_model_id_low_bit(
    model_id=saved_lowbit_model_path,
    tokenizer_id="lmsys/vicuna-7b-v1.5",
    # tokenizer_name=saved_lowbit_model_path,  # copy the tokenizers to saved path if you want to use it this way
    model_kwargs={
        "temperature": 0,
        "max_length": 64,
        "trust_remote_code": True,
        "device": "xpu",
    },
)
```

Chains에서 로드된 모델 사용하기:

```python
llm_chain = prompt | llm_lowbit


question = "What is AI?"
output = llm_chain.invoke(question)
```

## IPEX-LLM on Intel CPU

이 예제는 Intel CPU에서 텍스트 생성을 위해 LangChain을 사용하여 `ipex-llm`과 상호작용하는 방법을 다룹니다.

### Setup

```python
# Update LangChain

pip install -qU langchain langchain-community
```

Intel CPU에서 로컬로 LLM을 실행하기 위해 IEPX-LLM을 설치하세요:

#### For Windows users

```python
pip install --pre --upgrade ipex-llm[all]
```

#### For Linux users

```python
pip install --pre --upgrade ipex-llm[all] --extra-index-url https://download.pytorch.org/whl/cpu
```

### Basic Usage

```python
import warnings

from langchain.chains import LLMChain
from langchain_community.llms import IpexLLM
from langchain_core.prompts import PromptTemplate

warnings.filterwarnings("ignore", category=UserWarning, message=".*padding_mask.*")
```

모델에 대한 prompt template을 지정하세요. 이 예제에서는 [vicuna-1.5](https://huggingface.co/lmsys/vicuna-7b-v1.5) 모델을 사용합니다. 다른 모델을 사용하는 경우 그에 맞는 적절한 template을 선택하세요.

```python
template = "USER: {question}\nASSISTANT:"
prompt = PromptTemplate(template=template, input_variables=["question"])
```

`IpexLLM.from_model_id`를 사용하여 IpexLLM으로 모델을 로컬로 로드하세요. Huggingface 형식으로 모델을 직접 로드하고 추론을 위해 자동으로 low-bit 형식으로 변환합니다.

```python
llm = IpexLLM.from_model_id(
    model_id="lmsys/vicuna-7b-v1.5",
    model_kwargs={"temperature": 0, "max_length": 64, "trust_remote_code": True},
)
```

Chains에서 사용하기:

```python
llm_chain = prompt | llm

question = "What is AI?"
output = llm_chain.invoke(question)
```

### Save/Load Low-bit Model

또는 low-bit 모델을 디스크에 한 번 저장하고 나중에 사용하기 위해 `from_model_id` 대신 `from_model_id_low_bit`를 사용하여 다시 로드할 수 있습니다 - 다른 머신에서도 가능합니다. low-bit 모델은 원본 모델보다 훨씬 적은 디스크 공간을 필요로 하므로 공간 효율적입니다. 또한 `from_model_id_low_bit`는 모델 변환 단계를 건너뛰므로 속도와 메모리 사용 측면에서 `from_model_id`보다 더 효율적입니다.

low-bit 모델을 저장하려면 다음과 같이 `save_low_bit`를 사용하세요:

```python
saved_lowbit_model_path = "./vicuna-7b-1.5-low-bit"  # path to save low-bit model
llm.model.save_low_bit(saved_lowbit_model_path)
del llm
```

저장된 lowbit 모델 경로에서 모델을 다음과 같이 로드하세요.

> low-bit 모델의 저장 경로에는 모델 자체만 포함되고 tokenizer는 포함되지 않습니다. 모든 것을 한 곳에 두려면 원본 모델의 디렉토리에서 low-bit 모델이 저장된 위치로 tokenizer 파일을 수동으로 다운로드하거나 복사해야 합니다.

```python
llm_lowbit = IpexLLM.from_model_id_low_bit(
    model_id=saved_lowbit_model_path,
    tokenizer_id="lmsys/vicuna-7b-v1.5",
    # tokenizer_name=saved_lowbit_model_path,  # copy the tokenizers to saved path if you want to use it this way
    model_kwargs={"temperature": 0, "max_length": 64, "trust_remote_code": True},
)
```

Chains에서 로드된 모델 사용하기:

```python
llm_chain = prompt | llm_lowbit


question = "What is AI?"
output = llm_chain.invoke(question)
```