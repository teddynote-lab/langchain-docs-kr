---
title: LindormVectorStore
---

이 노트북은 Lindorm vector store를 시작하는 방법을 다룹니다.

## Setup

Lindorm vector store에 액세스하려면 Lindorm 계정을 생성하고, ak/sk를 얻고, `langchain-lindorm-integration` integration package를 설치해야 합니다.

```python
pip install -qU "langchain-lindorm-integration"
```

### Credentials

Lindorm에 가입하고 ak/sk를 생성하려면 [여기](https://help.aliyun.com/document_detail/2773369.html?spm=a2c4g.11186623.help-menu-172543.d_2_5_0.2a383f96gr5N3M&scm=20140722.H_2773369._.OR_help-T_cn~zh-V_1)로 이동하세요.

```python
import os


class Config:
    SEARCH_ENDPOINT = os.environ.get("SEARCH_ENDPOINT", "SEARCH_ENDPOINT")
    SEARCH_USERNAME = os.environ.get("SEARCH_USERNAME", "root")
    SEARCH_PWD = os.environ.get("SEARCH_PASSWORD", "<PASSWORD>")
    AI_LLM_ENDPOINT = os.environ.get("AI_ENDPOINT", "<AI_ENDPOINT>")
    AI_USERNAME = os.environ.get("AI_USERNAME", "root")
    AI_PWD = os.environ.get("AI_PASSWORD", "<PASSWORD>")
    AI_DEFAULT_EMBEDDING_MODEL = "bge_m3_model"  # set to your model
```

## Initialization

여기서는 Lindorm AI Service에 배포된 embedding model을 사용합니다.

```python
from langchain_lindorm_integration.embeddings import LindormAIEmbeddings
from langchain_lindorm_integration.vectorstores import LindormVectorStore

embeddings = LindormAIEmbeddings(
    endpoint=Config.AI_LLM_ENDPOINT,
    username=Config.AI_USERNAME,
    password=Config.AI_PWD,
    model_name=Config.AI_DEFAULT_EMBEDDING_MODEL,
)

index = "test_index"
vector = embeddings.embed_query("hello word")
dimension = len(vector)
vector_store = LindormVectorStore(
    lindorm_search_url=Config.SEARCH_ENDPOINT,
    embedding=embeddings,
    http_auth=(Config.SEARCH_USERNAME, Config.SEARCH_PWD),
    dimension=dimension,
    embeddings=embeddings,
    index_name=index,
)
```

## Manage vector store

### Add items to vector store

```python
from langchain_core.documents import Document

document_1 = Document(page_content="foo", metadata={"source": "https://example.com"})

document_2 = Document(page_content="bar", metadata={"source": "https://example.com"})

document_3 = Document(page_content="baz", metadata={"source": "https://example.com"})

documents = [document_1, document_2, document_3]

vector_store.add_documents(documents=documents, ids=["1", "2", "3"])
```

```output
['1', '2', '3']
```

### Delete items from vector store

```python
vector_store.delete(ids=["3"])
```

```output
{'took': 400,
 'timed_out': False,
 'total': 1,
 'deleted': 1,
 'batches': 1,
 'version_conflicts': 0,
 'noops': 0,
 'retries': {'bulk': 0, 'search': 0},
 'throttled_millis': 0,
 'requests_per_second': -1.0,
 'throttled_until_millis': 0,
 'failures': []}
```

## Query vector store

vector store가 생성되고 관련 document가 추가되면, chain이나 agent를 실행하는 동안 이를 쿼리하고 싶을 것입니다.

### Query directly

간단한 similarity search는 다음과 같이 수행할 수 있습니다:

```python
results = vector_store.similarity_search(query="thud", k=1)
for doc in results:
    print(f"* {doc.page_content} [{doc.metadata}]")
```

```output
* foo [{'source': 'https://example.com'}]
```

similarity search를 실행하고 해당 score를 받으려면 다음을 실행할 수 있습니다:

```python
results = vector_store.similarity_search_with_score(query="thud", k=1)
for doc, score in results:
    print(f"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]")
```

```output
* [SIM=0.671268] foo [{'source': 'https://example.com'}]
```

## Usage for retrieval-augmented generation

이 vector store를 retrieval-augmented generation (RAG)에 사용하는 방법에 대한 가이드는 다음 섹션을 참조하세요:

- [Build a RAG app with LangChain](/oss/langchain/rag)
- [Agentic RAG](/oss/langgraph/agentic-rag)
- [Retrieval docs](/oss/langchain/retrieval)

## API reference

모든 LindormVectorStore feature 및 configuration에 대한 자세한 문서는 [API reference](https://pypi.org/project/langchain-lindorm-integration/)를 참조하세요.