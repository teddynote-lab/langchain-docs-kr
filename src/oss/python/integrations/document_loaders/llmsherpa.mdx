---
title: LLM Sherpa
---

이 노트북은 `LLM Sherpa`를 사용하여 다양한 유형의 파일을 로드하는 방법을 다룹니다. `LLM Sherpa`는 DOCX, PPTX, HTML, TXT, XML을 포함한 다양한 파일 형식을 지원합니다.

`LLMSherpaFileLoader`는 LLMSherpa 라이브러리의 일부인 LayoutPDFReader를 사용합니다. 이 도구는 대부분의 PDF to text parser를 사용할 때 손실되는 레이아웃 정보를 보존하면서 PDF를 파싱하도록 설계되었습니다.

다음은 LayoutPDFReader의 주요 기능입니다:

* 섹션과 하위 섹션을 레벨과 함께 식별하고 추출할 수 있습니다.
* 줄을 결합하여 단락을 형성합니다.
* 섹션과 단락 간의 링크를 식별할 수 있습니다.
* 테이블이 발견된 섹션과 함께 테이블을 추출할 수 있습니다.
* 목록과 중첩된 목록을 식별하고 추출할 수 있습니다.
* 페이지에 걸쳐 분산된 콘텐츠를 결합할 수 있습니다.
* 반복되는 헤더와 푸터를 제거할 수 있습니다.
* 워터마크를 제거할 수 있습니다.

[llmsherpa](https://llmsherpa.readthedocs.io/en/latest/) 문서를 확인하세요.

`정보: 이 라이브러리는 일부 pdf 파일에서 실패할 수 있으므로 주의해서 사용하세요.`

```python
# Install package
# !pip install -qU llmsherpa
```

## LLMSherpaFileLoader

내부적으로 LLMSherpaFileLoader는 파일 콘텐츠를 로드하기 위한 몇 가지 전략을 정의합니다: ["sections", "chunks", "html", "text"], `llmsherpa_api_url`을 얻기 위해 [nlm-ingestor](https://github.com/nlmatics/nlm-ingestor)를 설정하거나 기본값을 사용하세요.

### sections strategy: 파일을 섹션으로 파싱하여 반환

```python
from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader

loader = LLMSherpaFileLoader(
    file_path="https://arxiv.org/pdf/2402.14207.pdf",
    new_indent_parser=True,
    apply_ocr=True,
    strategy="sections",
    llmsherpa_api_url="http://localhost:5010/api/parseDocument?renderFormat=all",
)
docs = loader.load()
```

```python
docs[1]
```

```output
Document(page_content='Abstract\nWe study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages.\nThis underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing.\nWe propose STORM, a writing system for the Synthesis of Topic Outlines through\nReferences\nFull-length Article\nTopic\nOutline\n2022 Winter Olympics\nOpening Ceremony\nResearch via Question Asking\nRetrieval and Multi-perspective Question Asking.\nSTORM models the pre-writing stage by\nLLM\n(1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.\nFor evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage.\nWe further gather feedback from experienced Wikipedia editors.\nCompared to articles generated by an outlinedriven retrieval-augmented baseline, more of STORM’s articles are deemed to be organized (by a 25% absolute increase) and broad in coverage (by 10%).\nThe expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.\n1. Can you provide any information about the transportation arrangements for the opening ceremony?\nLLM\n2. Can you provide any information about the budget for the 2022 Winter Olympics opening ceremony?…\nLLM- Role1\nLLM- Role2\nLLM- Role1', metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 1, 'section_title': 'Abstract'})
```

```python
len(docs)
```

```output
79
```

### chunks strategy: 파일을 청크로 파싱하여 반환

```python
from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader

loader = LLMSherpaFileLoader(
    file_path="https://arxiv.org/pdf/2402.14207.pdf",
    new_indent_parser=True,
    apply_ocr=True,
    strategy="chunks",
    llmsherpa_api_url="http://localhost:5010/api/parseDocument?renderFormat=all",
)
docs = loader.load()
```

```python
docs[1]
```

```output
Document(page_content='Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models\nStanford University {shaoyj, yuchengj, tkanell, peterxu, okhattab}@stanford.edu lam@cs.stanford.edu', metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 1, 'chunk_type': 'para'})
```

```python
len(docs)
```

```output
306
```

### html strategy: 파일을 하나의 html 문서로 반환

```python
from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader

loader = LLMSherpaFileLoader(
    file_path="https://arxiv.org/pdf/2402.14207.pdf",
    new_indent_parser=True,
    apply_ocr=True,
    strategy="html",
    llmsherpa_api_url="http://localhost:5010/api/parseDocument?renderFormat=all",
)
docs = loader.load()
```

```python
docs[0].page_content[:400]
```

```output
'<html><h1>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</h1><table><th><td colSpan=1>Yijia Shao</td><td colSpan=1>Yucheng Jiang</td><td colSpan=1>Theodore A. Kanell</td><td colSpan=1>Peter Xu</td></th><tr><td colSpan=1></td><td colSpan=1>Omar Khattab</td><td colSpan=1>Monica S. Lam</td><td colSpan=1></td></tr></table><p>Stanford University {shaoyj, yuchengj, '
```

```python
len(docs)
```

```output
1
```

### text strategy: 파일을 하나의 텍스트 문서로 반환

```python
from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader

loader = LLMSherpaFileLoader(
    file_path="https://arxiv.org/pdf/2402.14207.pdf",
    new_indent_parser=True,
    apply_ocr=True,
    strategy="text",
    llmsherpa_api_url="http://localhost:5010/api/parseDocument?renderFormat=all",
)
docs = loader.load()
```

```python
docs[0].page_content[:400]
```

```output
'Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models\n | Yijia Shao | Yucheng Jiang | Theodore A. Kanell | Peter Xu\n | --- | --- | --- | ---\n |  | Omar Khattab | Monica S. Lam | \n\nStanford University {shaoyj, yuchengj, tkanell, peterxu, okhattab}@stanford.edu lam@cs.stanford.edu\nAbstract\nWe study how to apply large language models to write grounded and organized long'
```

```python
len(docs)
```

```output
1
```