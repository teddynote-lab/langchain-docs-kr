```markdown
---
title: GreenNode
---

>[GreenNode](https://greennode.ai/)는 글로벌 AI 솔루션 제공업체이자 **NVIDIA Preferred Partner**로, 미국, MENA, APAC 지역의 기업들에게 인프라부터 애플리케이션까지 풀스택 AI 역량을 제공합니다. **세계적 수준의 인프라**(LEED Gold, TIA‑942, Uptime Tier III)를 기반으로 운영되는 GreenNode는 기업, 스타트업, 연구자들에게 포괄적인 AI 서비스 제품군을 제공합니다.

이 가이드는 `GreenNodeRerank` retriever를 시작하는 방법에 대한 안내를 제공합니다. 내장 connector를 사용하거나 자체 데이터 소스를 통합하여 문서 검색을 수행할 수 있으며, GreenNode의 reranking 기능을 활용하여 관련성을 향상시킬 수 있습니다.

### Integration 세부 정보

- **Provider**: [GreenNode Serverless AI](https://aiplatform.console.greennode.ai/playground)
- **Model Types**: Reranking models
- **Primary Use Case**: 의미론적 관련성을 기반으로 검색 결과 재정렬
- **Available Models**: [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) 및 기타 고성능 reranking models 포함
- **Scoring**: 쿼리 정렬을 기반으로 문서 후보를 재정렬하는 데 사용되는 관련성 점수 반환

## Setup

GreenNode models에 액세스하려면 GreenNode 계정을 생성하고 API key를 받은 다음 `langchain-greennode` integration package를 설치해야 합니다.

### Credentials

[이 페이지](https://aiplatform.console.greennode.ai/api-keys)로 이동하여 GreenNode AI Platform에 가입하고 API key를 생성하세요. 완료한 후 GREENNODE_API_KEY environment variable을 설정하세요:

```python
import getpass
import os

if not os.getenv("GREENNODE_API_KEY"):
    os.environ["GREENNODE_API_KEY"] = getpass.getpass("Enter your GreenNode API key: ")
```

개별 쿼리에서 자동화된 추적을 원하는 경우, 아래 주석을 해제하여 [LangSmith](https://docs.smith.langchain.com/) API key를 설정할 수도 있습니다:

```python
os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
os.environ["LANGSMITH_TRACING"] = "true"
```

### Installation

이 retriever는 `langchain-greennode` package에 있습니다:

```python
pip install -qU langchain-greennode
```

```output
Note: you may need to restart the kernel to use updated packages.
```

## Instantiation

`GreenNodeRerank` class는 API key와 model name에 대한 선택적 매개변수로 인스턴스화할 수 있습니다:

```python
from langchain_greennode import GreenNodeRerank

# Initialize the embeddings model
reranker = GreenNodeRerank(
    # api_key="YOUR_API_KEY",  # You can pass the API key directly
    model="BAAI/bge-reranker-v2-m3",  # The default embedding model
    top_n=3,
)
```

## Usage

### Reranking Search Results

Reranking models는 의미론적 관련성을 기반으로 초기 검색 결과를 정제하고 재정렬하여 retrieval-augmented generation (RAG) 워크플로우를 향상시킵니다. 아래 예제는 GreenNodeRerank를 base retriever와 통합하여 검색된 문서의 품질을 개선하는 방법을 보여줍니다.

```python
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_greennode import GreenNodeEmbeddings

# Initialize the embeddings model
embeddings = GreenNodeEmbeddings(
    # api_key="YOUR_API_KEY",  # You can pass the API key directly
    model="BAAI/bge-m3"  # The default embedding model
)

# Prepare documents (finance/economics domain)
docs = [
    Document(
        page_content="Inflation represents the rate at which the general level of prices for goods and services rises"
    ),
    Document(
        page_content="Central banks use interest rates to control inflation and stabilize the economy"
    ),
    Document(
        page_content="Cryptocurrencies like Bitcoin operate on decentralized blockchain networks"
    ),
    Document(
        page_content="Stock markets are influenced by corporate earnings, investor sentiment, and economic indicators"
    ),
]

# Create a vector store and a base retriever
vector_store = FAISS.from_documents(docs, embeddings)
base_retriever = vector_store.as_retriever(search_kwargs={"k": 4})


rerank_retriever = ContextualCompressionRetriever(
    base_compressor=reranker, base_retriever=base_retriever
)

# Perform retrieval with reranking
query = "How do central banks fight rising prices?"
results = rerank_retriever.get_relevant_documents(query)

results
```

```output
/var/folders/bs/g52lln652z11zjp98qf9wcy40000gn/T/ipykernel_96362/2544494776.py:41: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.
  results = rerank_retriever.get_relevant_documents(query)
```

```output
[Document(metadata={'relevance_score': 0.125}, page_content='Central banks use interest rates to control inflation and stabilize the economy'),
 Document(metadata={'relevance_score': 0.004913330078125}, page_content='Inflation represents the rate at which the general level of prices for goods and services rises'),
 Document(metadata={'relevance_score': 1.6689300537109375e-05}, page_content='Cryptocurrencies like Bitcoin operate on decentralized blockchain networks')]
```

### Direct Usage

`GreenNodeRerank` class는 관련성 점수를 기반으로 검색된 문서의 reranking을 수행하기 위해 독립적으로 사용할 수 있습니다. 이 기능은 기본 검색 단계(예: keyword 또는 vector search)가 광범위한 후보 집합을 반환하고, 보다 정교한 의미론적 이해를 사용하여 결과를 정제하기 위해 보조 model이 필요한 시나리오에서 특히 유용합니다. 이 class는 쿼리와 후보 문서 목록을 받아 예측된 관련성을 기반으로 재정렬된 목록을 반환합니다.

```python
test_documents = [
    Document(
        page_content="Carson City is the capital city of the American state of Nevada."
    ),
    Document(
        page_content="Washington, D.C. (also known as simply Washington or D.C.) is the capital of the United States."
    ),
    Document(
        page_content="Capital punishment has existed in the United States since beforethe United States was a country."
    ),
    Document(
        page_content="The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan."
    ),
]

test_query = "What is the capital of the United States?"
results = reranker.rerank(test_documents, test_query)
results
```

```output
[{'index': 1, 'relevance_score': 1.0},
 {'index': 0, 'relevance_score': 0.01165771484375},
 {'index': 3, 'relevance_score': 0.0012054443359375}]
```

## Use within a chain

GreenNodeRerank는 LangChain RAG pipeline에서 원활하게 작동합니다. 다음은 GreenNodeRerank를 사용하여 간단한 RAG chain을 생성하는 예제입니다:

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_greennode import ChatGreenNode

# Initialize LLM
llm = ChatGreenNode(model="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B")

# Create a prompt template
prompt = ChatPromptTemplate.from_template(
    """
Answer the question based only on the following context:

Context:
{context}

Question: {question}
"""
)


# Format documents function
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


# Create RAG chain
rag_chain = (
    {"context": rerank_retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Run the chain
answer = rag_chain.invoke("How do central banks fight rising prices?")
answer
```

```output
'\n\nCentral banks combat rising prices, or inflation, by adjusting interest rates. By raising interest rates, they increase the cost of borrowing, which discourages spending and investment. This reduction in demand helps slow down the rate of price increases, thereby controlling inflation and contributing to economic stability.'
```

## API reference

GreenNode Serverless AI API에 대한 자세한 내용은 [GreenNode Serverless AI Documentation](https://aiplatform.console.greennode.ai/api-docs/maas)을 참조하세요.
```