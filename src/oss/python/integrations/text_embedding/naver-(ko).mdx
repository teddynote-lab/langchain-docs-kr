---
title: Naver
---

이 노트북은 CLOVA Studio에서 제공하는 embedding model을 시작하는 방법을 다룹니다. `ClovaXEmbeddings`의 기능 및 구성 옵션에 대한 자세한 문서는 [API reference](https://guide.ncloud-docs.com/docs/clovastudio-dev-langchain#%EC%9E%84%EB%B2%A0%EB%94%A9%EB%8F%84%EA%B5%AC%EC%9D%B4%EC%9A%A9)를 참조하세요.

## Overview

### Integration details

| Provider | Package |
|:--------:|:-------:|
| [Naver](/oss/integrations/providers/naver.mdx) | [langchain-naver](https://pypi.org/project/langchain-naver/) |

## Setup

CLOVA Studio에서 제공하는 embedding model을 사용하기 전에 아래 세 단계를 거쳐야 합니다.

1. [NAVER Cloud Platform](https://www.ncloud.com/) 계정 생성
2. [CLOVA Studio](https://www.ncloud.com/product/aiService/clovaStudio) 사용 신청
3. 사용할 모델의 CLOVA Studio Test App 또는 Service App 생성 ([여기](https://guide.ncloud-docs.com/docs/clovastudio-explorer03#%ED%85%8C%EC%8A%A4%ED%8A%B8%EC%95%B1%EC%83%9D%EC%84%B1) 참조)
4. Test 또는 Service API key 발급 ([여기](https://guide.ncloud-docs.com/docs/clovastudio-explorer-testapp) 참조)

### Credentials

API key로 `CLOVASTUDIO_API_KEY` 환경 변수를 설정하세요.

```python
import getpass
import os

if not os.getenv("CLOVASTUDIO_API_KEY"):
    os.environ["CLOVASTUDIO_API_KEY"] = getpass.getpass("Enter CLOVA Studio API Key: ")
```

### Installation

ClovaXEmbeddings integration은 `langchain_naver` 패키지에 포함되어 있습니다:

```python
# install package
pip install -qU langchain-naver
```

## Instantiation

이제 embeddings 객체를 인스턴스화하고 query 또는 document를 embed할 수 있습니다:

- CLOVA Studio에는 여러 embedding model이 제공됩니다. 자세한 내용은 [여기](https://guide.ncloud-docs.com/docs/en/clovastudio-explorer03#임베딩API)를 참조하세요.
- 특정 사용 사례에 따라 embedding을 정규화해야 할 수 있습니다.

```python
from langchain_naver import ClovaXEmbeddings

embeddings = ClovaXEmbeddings(
    model="clir-emb-dolphin"  # set with the model name of corresponding test/service app. Default is `clir-emb-dolphin`
)
```

## Indexing and Retrieval

Embedding model은 데이터 인덱싱과 이후 검색 모두에서 retrieval-augmented generation (RAG) 플로우에 자주 사용됩니다. 자세한 지침은 [RAG tutorials](/oss/langchain/rag)를 참조하세요.

아래에서는 위에서 초기화한 `embeddings` 객체를 사용하여 데이터를 인덱싱하고 검색하는 방법을 확인할 수 있습니다. 이 예제에서는 `InMemoryVectorStore`에서 샘플 document를 인덱싱하고 검색합니다.

```python
# Create a vector store with a sample text
from langchain_core.vectorstores import InMemoryVectorStore

text = "CLOVA Studio is an AI development tool that allows you to customize your own HyperCLOVA X models."

vectorstore = InMemoryVectorStore.from_texts(
    [text],
    embedding=embeddings,
)

# Use the vectorstore as a retriever
retriever = vectorstore.as_retriever()

# Retrieve the most similar text
retrieved_documents = retriever.invoke("What is CLOVA Studio?")

# show the retrieved document's content
retrieved_documents[0].page_content
```

```output
'CLOVA Studio is an AI development tool that allows you to customize your own HyperCLOVA X models.'
```

## Direct Usage

내부적으로 vectorstore와 retriever 구현은 `embeddings.embed_documents(...)`와 `embeddings.embed_query(...)`를 호출하여 각각 `from_texts`와 retrieval `invoke` 작업에 사용되는 텍스트에 대한 embedding을 생성합니다.

이러한 메서드를 직접 호출하여 자신의 사용 사례에 맞는 embedding을 얻을 수 있습니다.

### Embed single texts

`embed_query`로 단일 텍스트 또는 document를 embed할 수 있습니다:

```python
single_vector = embeddings.embed_query(text)
print(str(single_vector)[:100])  # Show the first 100 characters of the vector
```

```output
[-0.094717406, -0.4077411, -0.5513184, 1.6024436, -1.3235079, -1.0720996, -0.44471845, 1.3665184, 0.
```

### Embed multiple texts

`embed_documents`로 여러 텍스트를 embed할 수 있습니다:

```python
text2 = "LangChain is a framework for building context-aware reasoning applications"
two_vectors = embeddings.embed_documents([text, text2])
for vector in two_vectors:
    print(str(vector)[:100])  # Show the first 100 characters of the vector
```

```output
[-0.094717406, -0.4077411, -0.5513184, 1.6024436, -1.3235079, -1.0720996, -0.44471845, 1.3665184, 0.
[-0.25525448, -0.84877056, -0.6928286, 1.5867524, -1.2930486, -0.8166254, -0.17934391, 1.4236152, 0.
```

## API reference

`ClovaXEmbeddings`의 기능 및 구성 옵션에 대한 자세한 문서는 [API reference](https://guide.ncloud-docs.com/docs/clovastudio-dev-langchain#%EC%9E%84%EB%B2%A0%EB%94%A9%EB%8F%84%EA%B5%AC%EC%9D%B4%EC%9A%A9)를 참조하세요.