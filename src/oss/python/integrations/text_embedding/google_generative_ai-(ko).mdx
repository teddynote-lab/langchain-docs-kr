---
title: Google Generative AI Embeddings (AI Studio & Gemini API)
sidebarTitle: GoogleGenerativeAIEmbeddings
---

[langchain-google-genai](https://pypi.org/project/langchain-google-genai/) 패키지에 있는 `GoogleGenerativeAIEmbeddings` 클래스를 사용하여 Google의 generative AI embeddings 서비스에 연결하세요.

이 가이드는 LangChain을 사용하여 Google의 Generative AI embedding 모델(예: Gemini)을 시작하는 데 도움이 됩니다. `GoogleGenerativeAIEmbeddings`의 기능 및 구성 옵션에 대한 자세한 문서는 [API reference](https://python.langchain.com/v0.2/api_reference/google_genai/embeddings/langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings.html)를 참조하세요.

## Overview

### Integration details

<ItemTable category="text_embedding" item="Google Gemini" />

## Setup

Google Generative AI embedding 모델에 액세스하려면 Google Cloud 프로젝트를 생성하고, Generative Language API를 활성화하고, API 키를 받고, `langchain-google-genai` integration 패키지를 설치해야 합니다.

### Credentials

Google Generative AI 모델을 사용하려면 API 키가 필요합니다. Google AI Studio에서 생성할 수 있습니다. 자세한 지침은 [Google 문서](https://ai.google.dev/gemini-api/docs/api-key)를 참조하세요.

키를 받은 후 환경 변수 `GOOGLE_API_KEY`로 설정하세요:

```python
import getpass
import os

if not os.getenv("GOOGLE_API_KEY"):
    os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter your Google API key: ")
```

모델 호출의 자동 추적을 활성화하려면 [LangSmith](https://docs.smith.langchain.com/) API 키를 설정하세요:

```python
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
```

## Installation

```python
pip install -qU  langchain-google-genai
```

## Usage

```python
from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")
vector = embeddings.embed_query("hello, world!")
vector[:5]
```

```output
[-0.024917153641581535,
 0.012005362659692764,
 -0.003886754624545574,
 -0.05774897709488869,
 0.0020742062479257584]
```

## Batch

처리 속도 향상을 위해 여러 문자열을 한 번에 임베딩할 수도 있습니다:

```python
vectors = embeddings.embed_documents(
    [
        "Today is Monday",
        "Today is Tuesday",
        "Today is April Fools day",
    ]
)
len(vectors), len(vectors[0])
```

```output
(3, 3072)
```

## Indexing and Retrieval

Embedding 모델은 데이터 인덱싱과 나중에 검색하는 과정 모두에서 retrieval-augmented generation (RAG) 플로우에 자주 사용됩니다. 자세한 지침은 [RAG 튜토리얼](/oss/langchain/rag)을 참조하세요.

아래에서는 위에서 초기화한 `embeddings` 객체를 사용하여 데이터를 인덱싱하고 검색하는 방법을 확인할 수 있습니다. 이 예제에서는 `InMemoryVectorStore`에서 샘플 문서를 인덱싱하고 검색합니다.

```python
# Create a vector store with a sample text
from langchain_core.vectorstores import InMemoryVectorStore

text = "LangChain is the framework for building context-aware reasoning applications"

vectorstore = InMemoryVectorStore.from_texts(
    [text],
    embedding=embeddings,
)

# Use the vectorstore as a retriever
retriever = vectorstore.as_retriever()

# Retrieve the most similar text
retrieved_documents = retriever.invoke("What is LangChain?")

# show the retrieved document's content
retrieved_documents[0].page_content
```

```output
'LangChain is the framework for building context-aware reasoning applications'
```

## Task type

`GoogleGenerativeAIEmbeddings`는 선택적으로 `task_type`을 지원하며, 현재 다음 중 하나여야 합니다:

- `SEMANTIC_SIMILARITY`: 텍스트 유사성을 평가하는 데 최적화된 임베딩을 생성하는 데 사용됩니다.
- `CLASSIFICATION`: 미리 설정된 레이블에 따라 텍스트를 분류하는 데 최적화된 임베딩을 생성하는 데 사용됩니다.
- `CLUSTERING`: 유사성을 기반으로 텍스트를 클러스터링하는 데 최적화된 임베딩을 생성하는 데 사용됩니다.
- `RETRIEVAL_DOCUMENT`, `RETRIEVAL_QUERY`, `QUESTION_ANSWERING`, `FACT_VERIFICATION`: 문서 검색 또는 정보 검색에 최적화된 임베딩을 생성하는 데 사용됩니다.
- `CODE_RETRIEVAL_QUERY`: 배열 정렬 또는 연결 리스트 반전과 같은 자연어 쿼리를 기반으로 코드 블록을 검색하는 데 사용됩니다. 코드 블록의 임베딩은 `RETRIEVAL_DOCUMENT`를 사용하여 계산됩니다.

기본적으로 `embed_documents` 메서드에서는 `RETRIEVAL_DOCUMENT`를 사용하고 `embed_query` 메서드에서는 `RETRIEVAL_QUERY`를 사용합니다. task type을 제공하면 모든 메서드에 대해 해당 타입을 사용합니다.

```python
pip install -qU  matplotlib scikit-learn
```

```python
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from sklearn.metrics.pairwise import cosine_similarity

query_embeddings = GoogleGenerativeAIEmbeddings(
    model="models/gemini-embedding-001", task_type="RETRIEVAL_QUERY"
)
doc_embeddings = GoogleGenerativeAIEmbeddings(
    model="models/gemini-embedding-001", task_type="RETRIEVAL_DOCUMENT"
)

q_embed = query_embeddings.embed_query("What is the capital of France?")
d_embed = doc_embeddings.embed_documents(
    ["The capital of France is Paris.", "Philipp is likes to eat pizza."]
)

for i, d in enumerate(d_embed):
    print(f"Document {i + 1}:")
    print(f"Cosine similarity with query: {cosine_similarity([q_embed], [d])[0][0]}")
    print("---")
```

```output
Document 1
Cosine similarity with query: 0.7892893360164779
---
Document 2
Cosine similarity with query: 0.5438283285204146
---
```

## API reference

`GoogleGenerativeAIEmbeddings`의 기능 및 구성 옵션에 대한 자세한 문서는 [API reference](https://python.langchain.com/api_reference/google_genai/embeddings/langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings.html)를 참조하세요.

## Additional Configuration

SDK의 동작을 사용자 정의하기 위해 ChatGoogleGenerativeAI에 다음 매개변수를 전달할 수 있습니다:

- `client_options`: 사용자 정의 `client_options["api_endpoint"]`와 같이 Google API Client에 전달할 [Client Options](https://googleapis.dev/python/google-api-core/latest/client_options.html#module-google.api_core.client_options)
- `transport`: `rest`, `grpc` 또는 `grpc_asyncio`와 같이 사용할 전송 방법