---
title: Cerebras
---

Cerebras에서는 세계에서 가장 크고 빠른 AI 프로세서인 Wafer-Scale Engine-3 (WSE-3)를 개발했습니다. WSE-3로 구동되는 Cerebras CS-3 시스템은 비교할 수 없는 성능과 확장성으로 생성형 AI 학습 및 추론의 표준을 제시하는 새로운 클래스의 AI 슈퍼컴퓨터를 대표합니다.

Cerebras를 추론 제공자로 사용하면 다음을 수행할 수 있습니다:
- AI 추론 워크로드에 대해 전례 없는 속도 달성
- 높은 처리량으로 상업적 구축
- 원활한 클러스터링 기술로 AI 워크로드를 손쉽게 확장

우리의 CS-3 시스템은 빠르고 쉽게 클러스터링되어 세계에서 가장 큰 AI 슈퍼컴퓨터를 만들 수 있으며, 가장 큰 모델을 배치하고 실행하는 것을 간단하게 만듭니다. 선도적인 기업, 연구 기관 및 정부는 이미 Cerebras 솔루션을 사용하여 독점 모델을 개발하고 인기 있는 오픈 소스 모델을 학습하고 있습니다.

Cerebras의 성능을 경험하고 싶으신가요? 더 많은 리소스를 확인하고 Cerebras Cloud 또는 온프레미스 배포를 통해 우리 기술에 액세스하는 옵션을 탐색하려면 [웹사이트](https://cerebras.ai)를 방문하세요!

Cerebras Cloud에 대한 자세한 내용은 [cloud.cerebras.ai](https://cloud.cerebras.ai/)를 방문하세요. API 레퍼런스는 [inference-docs.cerebras.ai](https://inference-docs.cerebras.ai/)에서 확인할 수 있습니다.

## 설치 및 설정
integration package를 설치하세요:

<CodeGroup>
```bash pip
pip install langchain-cerebras
```

```bash uv
uv add langchain-cerebras
```
</CodeGroup>

## API Key
[cloud.cerebras.ai](https://cloud.cerebras.ai/)에서 API Key를 받아 환경 변수에 추가하세요:
```
export CEREBRAS_API_KEY="your-api-key-here"
```

## Chat Model
[사용 예제](/oss/integrations/chat/cerebras)를 참조하세요.