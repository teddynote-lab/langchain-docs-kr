---
title: Portkey
---

[Portkey](https://portkey.ai)는 AI 앱을 위한 제어 패널입니다. 인기 있는 AI Gateway와 Observability Suite를 통해 수백 개의 팀이 **안정적이고**, **비용 효율적이며**, **빠른** 앱을 배포하고 있습니다.

## LangChain을 위한 LLMOps

Portkey는 LangChain에 프로덕션 준비 상태를 제공합니다. Portkey를 사용하면 다음을 수행할 수 있습니다:

- [x] 통합 API를 통해 150개 이상의 모델에 연결
- [x] 모든 요청에 대한 42개 이상의 **메트릭 및 로그** 확인
- [x] **semantic cache**를 활성화하여 지연 시간 및 비용 절감
- [x] 실패한 요청에 대한 자동 **재시도 및 fallback** 구현
- [x] 더 나은 추적 및 분석을 위해 요청에 **custom tags** 추가 및 [더 많은 기능](https://portkey.ai/docs)

## Quickstart - Portkey & LangChain

Portkey는 OpenAI signature와 완전히 호환되므로 @[`ChatOpenAI`] 인터페이스를 통해 Portkey AI Gateway에 연결할 수 있습니다.

- `base_url`을 `PORTKEY_GATEWAY_URL`로 설정
- `createHeaders` 헬퍼 메서드를 사용하여 Portkey에 필요한 헤더를 사용하도록 `default_headers` 추가

시작하려면 [여기에서 가입](https://app.portkey.ai/signup)하여 Portkey API key를 받으세요. (왼쪽 하단의 프로필 아이콘을 클릭한 다음 "Copy API Key"를 클릭) 또는 [자체 환경](https://github.com/Portkey-AI/gateway/blob/main/docs/installation-deployments.md)에 오픈 소스 AI gateway를 배포하세요.

다음으로 Portkey SDK를 설치합니다

```python
pip install -U portkey_ai
```

이제 LangChain의 @[`ChatOpenAI`] 모델을 업데이트하여 Portkey AI Gateway에 연결할 수 있습니다

```python
from langchain_openai import ChatOpenAI
from portkey_ai import createHeaders, PORTKEY_GATEWAY_URL

PORTKEY_API_KEY = "..." # Not needed when hosting your own gateway
PROVIDER_API_KEY = "..." # Add the API key of the AI provider being used

portkey_headers = createHeaders(api_key=PORTKEY_API_KEY,provider="openai")

llm = ChatOpenAI(api_key=PROVIDER_API_KEY, base_url=PORTKEY_GATEWAY_URL, default_headers=portkey_headers)

llm.invoke("What is the meaning of life, universe and everything?")
```

요청은 Portkey AI Gateway를 통해 지정된 `provider`로 라우팅됩니다. Portkey는 또한 계정의 모든 요청을 로깅하기 시작하여 디버깅을 매우 간단하게 만듭니다.

![Portkey에서 LangChain 로그 보기](https://assets.portkey.ai/docs/langchain-logs.gif)

## AI Gateway를 통해 150개 이상의 모델 사용하기

AI gateway의 강력함은 위의 코드 스니펫을 사용하여 AI gateway를 통해 지원되는 20개 이상의 제공업체에서 150개 이상의 모델에 연결할 수 있을 때 나타납니다.

위의 코드를 수정하여 Anthropic의 `claude-3-opus-20240229` 모델을 호출해 보겠습니다.

Portkey는 **[Virtual Keys](https://docs.portkey.ai/docs/product/ai-gateway-streamline-llm-integrations/virtual-keys)**를 지원하며, 이는 보안 vault에서 API key를 저장하고 관리하는 쉬운 방법입니다. Virtual Key를 사용하여 LLM 호출을 시도해 보겠습니다. Portkey의 Virtual Keys 탭으로 이동하여 Anthropic용 새 키를 생성할 수 있습니다.

`virtual_key` 매개변수는 사용 중인 AI 제공업체의 인증 및 제공업체를 설정합니다. 우리의 경우 Anthropic Virtual key를 사용하고 있습니다.

> 해당 인증이 사용되지 않으므로 `api_key`는 비워둘 수 있습니다.

```python
from langchain_openai import ChatOpenAI
from portkey_ai import createHeaders, PORTKEY_GATEWAY_URL

PORTKEY_API_KEY = "..."
VIRTUAL_KEY = "..." # Anthropic's virtual key we copied above

portkey_headers = createHeaders(api_key=PORTKEY_API_KEY,virtual_key=VIRTUAL_KEY)

llm = ChatOpenAI(api_key="X", base_url=PORTKEY_GATEWAY_URL, default_headers=portkey_headers, model="claude-3-opus-20240229")

llm.invoke("What is the meaning of life, universe and everything?")
```

Portkey AI gateway는 Anthropic에 대한 API 요청을 인증하고 사용자가 사용할 수 있도록 OpenAI 형식으로 응답을 반환합니다.

AI gateway는 LangChain의 @[`ChatOpenAI`] 클래스를 확장하여 모든 제공업체와 모든 모델을 호출할 수 있는 단일 인터페이스로 만듭니다.

## 고급 라우팅 - Load Balancing, Fallbacks, Retries

Portkey AI Gateway는 구성 우선 접근 방식을 통해 load-balancing, fallback, 실험 및 canary 테스트와 같은 기능을 LangChain에 제공합니다.

두 개의 대형 모델을 테스트하기 위해 `gpt-4`와 `claude-opus` 간에 트래픽을 50:50으로 분할하려는 **예제**를 살펴보겠습니다. 이에 대한 gateway 구성은 다음과 같습니다:

```python
config = {
    "strategy": {
         "mode": "loadbalance"
    },
    "targets": [{
        "virtual_key": "openai-25654", # OpenAI's virtual key
        "override_params": {"model": "gpt4"},
        "weight": 0.5
    }, {
        "virtual_key": "anthropic-25654", # Anthropic's virtual key
        "override_params": {"model": "claude-3-opus-20240229"},
        "weight": 0.5
    }]
}
```

그런 다음 langchain에서 수행되는 요청에서 이 config를 사용할 수 있습니다.

```python
portkey_headers = createHeaders(
    api_key=PORTKEY_API_KEY,
    config=config
)

llm = ChatOpenAI(api_key="X", base_url=PORTKEY_GATEWAY_URL, default_headers=portkey_headers)

llm.invoke("What is the meaning of life, universe and everything?")
```

LLM이 호출되면 Portkey는 정의된 가중치 비율로 `gpt-4`와 `claude-3-opus-20240229`에 요청을 분산합니다.

더 많은 config 예제는 [여기](https://docs.portkey.ai/docs/api-reference/config-object#examples)에서 찾을 수 있습니다.

## **Chains & Agents 추적**

Portkey의 LangChain 통합은 agent 실행에 대한 완전한 가시성을 제공합니다. [인기 있는 agentic workflow](https://python.langchain.com/docs/use_cases/tool_use/quickstart/#agents)의 예를 살펴보겠습니다.

위와 같이 AI Gateway를 사용하도록 @[`ChatOpenAI`] 클래스만 수정하면 됩니다.

```python
from langchain_classic import hub
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_openai import ChatOpenAI
from langchain.tools import tool
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

prompt = hub.pull("hwchase17/openai-tools-agent")

portkey_headers = createHeaders(
    api_key=PORTKEY_API_KEY,
    virtual_key=OPENAI_VIRTUAL_KEY,
    trace_id="uuid-uuid-uuid-uuid"
)

@tool
def multiply(first_int: int, second_int: int) -> int:
    """Multiply two integers together."""
    return first_int * second_int


@tool
def exponentiate(base: int, exponent: int) -> int:
    "Exponentiate the base to the exponent power."
    return base**exponent


tools = [multiply, exponentiate]

model = ChatOpenAI(api_key="X", base_url=PORTKEY_GATEWAY_URL, default_headers=portkey_headers, temperature=0)

# Construct the OpenAI Tools agent
agent = create_openai_tools_agent(model, tools, prompt)

# Create an agent executor by passing in the agent and tools
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

agent_executor.invoke({
    "input": "Take 3 to the fifth power and multiply that by thirty six, then square the result"
})
```

**Portkey 대시보드에서 trace id와 함께 요청 로그를 볼 수 있습니다:**
![Portkey의 LangChain Agent 로그](https://assets.portkey.ai/docs/agent_tracing.gif)

추가 문서는 여기에서 확인할 수 있습니다:

- Observability - [portkey.ai/docs/product/observability-modern-monitoring-for-llms](https://portkey.ai/docs/product/observability-modern-monitoring-for-llms)
- AI Gateway - [portkey.ai/docs/product/ai-gateway-streamline-llm-integrations](https://portkey.ai/docs/product/ai-gateway-streamline-llm-integrations)
- Prompt Library - [portkey.ai/docs/product/prompt-library](https://portkey.ai/docs/product/prompt-library)

인기 있는 오픈 소스 AI Gateway는 여기에서 확인할 수 있습니다 - [github.com/portkey-ai/gateway](https://github.com/portkey-ai/gateway)

각 기능에 대한 자세한 정보와 사용 방법은 [Portkey 문서를 참조하세요](https://portkey.ai/docs). 질문이 있거나 추가 지원이 필요한 경우 [Twitter](https://twitter.com/portkeyai) 또는 [지원 이메일](mailto:hello@portkey.ai)로 문의하세요.