---
title: DigitalOcean Gradient
---

이 문서는 DigitalOcean Gradient [chat models](/oss/langchain/models) 시작하기를 도와드립니다.

## Overview
### Integration details

| Class | Package | Downloads | Version |
| :--- | :--- | :---: | :---: |
| [ChatGradient](https://python.langchain.com/api_reference/langchain-gradient/chat_models/langchain_gradient.chat_models.ChatGradient.html) | [langchain-gradient](https://python.langchain.com/api_reference/langchain-gradient/) | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-gradient?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-gradient?style=flat-square&label=%20) |


## Setup

langchain-gradient는 DigitalOcean Gradient Platform을 사용합니다.

DigitalOcean에 계정을 생성하고, Gradient Platform에서 `DIGITALOCEAN_INFERENCE_KEY` API key를 획득한 후, `langchain-gradient` integration package를 설치하세요.

### Credentials

[DigitalOcean Gradient](https://www.digitalocean.com/products/gradient)로 이동하세요

1. DigitalOcean Cloud Console에 가입/로그인합니다
2. Gradient Platform으로 이동하여 Serverless Inference로 이동합니다.
3. Create model access key를 클릭하고, 이름을 입력한 후 key를 생성합니다.

완료되면 `DIGITALOCEAN_INFERENCE_KEY` environment variable을 설정하세요:

```python
import os
os.environ["DIGITALOCEAN_INFERENCE_KEY"] = "your-api-key"
```

### Installation

LangChain Gradient integration은 `langchain-gradient` package에 있습니다:

<CodeGroup>
```bash pip
pip install -qU langchain-gradient
```

```bash uv
uv add langchain-gradient
```
</CodeGroup>

## Instantiation

```python
from langchain_gradient import ChatGradient

llm = ChatGradient(
    model="llama3.3-70b-instruct",
    api_key=os.environ.get("DIGITALOCEAN_INFERENCE_KEY")
)
```

## Invocation

```python
messages = [
    (
        "system",
        "You are a creative storyteller. Continue any story prompt you receive in an engaging and imaginative way.",
    ),
    ("human", "Once upon a time, in a village at the edge of a mysterious forest, a young girl named Mira found a glowing stone..."),
]
ai_msg = llm.invoke(messages)
ai_msg
print(ai_msg.content)
```

## Chaining

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate(
    [
        (
            "system",
            "You are a knowledgeable assistant. Carefully read the provided context and answer the user's question. If the answer is present in the context, cite the relevant sentence. If not, reply with \"Not found in context.\"",
        ),
        ("human", "Context: {context}\nQuestion: {question}"),
    ]
)

chain = prompt | llm
chain.invoke(
    {
        "context": (
            "The Eiffel Tower is located in Paris and was completed in 1889. "
            "It was designed by Gustave Eiffel's engineering company. "
            "The tower is one of the most recognizable structures in the world. "
            "The Statue of Liberty was a gift from France to the United States."
        ),
        "question": "Who designed the Eiffel Tower and when was it completed?"
    }
)
```