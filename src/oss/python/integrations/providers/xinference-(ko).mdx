---
title: Xorbits Inference (Xinference)
---

이 페이지는 LangChain에서 [Xinference](https://github.com/xorbitsai/inference)를 사용하는 방법을 설명합니다.

`Xinference`는 노트북에서도 LLM, 음성 인식 모델, 멀티모달 모델을 제공하도록 설계된 강력하고 다재다능한 라이브러리입니다.
Xorbits Inference를 사용하면 단 하나의 명령으로 자체 모델이나 최첨단 내장 모델을 손쉽게 배포하고 제공할 수 있습니다.

## 설치 및 설정

Xinference는 PyPI에서 pip를 통해 설치할 수 있습니다:

<CodeGroup>
```bash pip
pip install "xinference[all]"
```

```bash uv
uv add xinference[all]
```
</CodeGroup>

## LLM

Xinference는 chatglm, baichuan, whisper, vicuna, orca를 포함한 GGML과 호환되는 다양한 모델을 지원합니다. 내장 모델을 확인하려면 다음 명령을 실행하세요:

```bash
xinference list --all
```


### Xinference용 Wrapper

다음을 실행하여 Xinference의 로컬 인스턴스를 시작할 수 있습니다:

```bash
xinference
```

분산 클러스터에 Xinference를 배포할 수도 있습니다. 이를 위해 먼저 실행하려는 서버에서 Xinference supervisor를 시작하세요:

```bash
xinference-supervisor -H "${supervisor_host}"
```


그런 다음 실행하려는 다른 각 서버에서 Xinference worker를 시작하세요:

```bash
xinference-worker -e "http://${supervisor_host}:9997"
```

다음을 실행하여 Xinference의 로컬 인스턴스를 시작할 수도 있습니다:

```bash
xinference
```

Xinference가 실행되면 CLI 또는 Xinference client를 통해 모델 관리를 위한 endpoint에 액세스할 수 있습니다.

로컬 배포의 경우 endpoint는 http://localhost:9997입니다.


클러스터 배포의 경우 endpoint는 http://$\{supervisor_host\}:9997입니다.


그런 다음 모델을 시작해야 합니다. model_size_in_billions 및 quantization을 포함한 모델 이름 및 기타 속성을 지정할 수 있습니다. 명령줄 인터페이스(CLI)를 사용하여 수행할 수 있습니다. 예를 들어:

```bash
xinference launch -n orca -s 3 -q q4_0
```

model uid가 반환됩니다.

사용 예시:

```python
from langchain_community.llms import Xinference

llm = Xinference(
    server_url="http://0.0.0.0:9997",
    model_uid = {model_uid} # replace model_uid with the model UID return from launching the model
)

llm(
    prompt="Q: where can we visit in the capital of France? A:",
    generate_config={"max_tokens": 1024, "stream": True},
)

```

### 사용법

자세한 정보와 상세한 예제는 [xinference LLM 예제](/oss/integrations/llms/xinference)를 참조하세요.

### Embeddings

Xinference는 쿼리 및 문서 embedding도 지원합니다. 더 자세한 데모는 [xinference embedding 예제](/oss/integrations/text_embedding/xinference)를 참조하세요.


### Xinference LangChain partner package 설치

다음과 같이 통합 패키지를 설치하세요:

<CodeGroup>
```bash pip
pip install langchain-xinference
```

```bash uv
uv add langchain-xinference
```
</CodeGroup>

## Chat Models

```python
from langchain_xinference.chat_models import ChatXinference
```

## LLM

```python
from langchain_xinference.llms import Xinference
```