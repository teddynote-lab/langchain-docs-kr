---
title: C Transformers
---

이 페이지는 LangChain 내에서 [C Transformers](https://github.com/marella/ctransformers) 라이브러리를 사용하는 방법을 다룹니다.
설치 및 설정, 그리고 특정 C Transformers wrapper에 대한 참조, 이렇게 두 부분으로 나뉩니다.

## 설치 및 설정

- `pip install ctransformers`로 Python package를 설치합니다
- 지원되는 [GGML model](https://huggingface.co/TheBloke)을 다운로드합니다 ([지원되는 모델](https://github.com/marella/ctransformers#supported-models) 참조)

## Wrappers

### LLM

CTransformers LLM wrapper가 존재하며, 다음과 같이 접근할 수 있습니다:

```python
from langchain_community.llms import CTransformers
```

모든 model에 대해 통합된 interface를 제공합니다:

```python
llm = CTransformers(model='/path/to/ggml-gpt-2.bin', model_type='gpt2')

print(llm.invoke('AI is going to'))
```

`illegal instruction` 오류가 발생하는 경우, `lib='avx'` 또는 `lib='basic'`을 사용해보세요:

```py
llm = CTransformers(model='/path/to/ggml-gpt-2.bin', model_type='gpt2', lib='avx')
```

Hugging Face Hub에 호스팅된 model과 함께 사용할 수 있습니다:

```py
llm = CTransformers(model='marella/gpt-2-ggml')
```

model repo에 여러 model 파일(`.bin` 파일)이 있는 경우, 다음을 사용하여 model 파일을 지정합니다:

```py
llm = CTransformers(model='marella/gpt-2-ggml', model_file='ggml-model.bin')
```

추가 parameter는 `config` parameter를 사용하여 전달할 수 있습니다:

```py
config = {'max_new_tokens': 256, 'repetition_penalty': 1.1}

llm = CTransformers(model='marella/gpt-2-ggml', config=config)
```

사용 가능한 parameter 목록은 [Documentation](https://github.com/marella/ctransformers#config)을 참조하세요.

더 자세한 안내는 [이 notebook](/oss/integrations/llms/ctransformers)을 참조하세요.