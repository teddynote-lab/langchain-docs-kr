---
title: Memcached
---

> [Memcached](https://www.memcached.org/)는 무료 오픈 소스 고성능 분산 메모리 객체 캐싱 시스템으로,
> 범용적인 특성을 가지고 있지만 데이터베이스 부하를 줄여 동적 웹 애플리케이션의 속도를 향상시키는 데 사용됩니다.

이 페이지에서는 이미 실행 중인 Memcached 인스턴스에 연결하기 위한 클라이언트로 [pymemcache](https://github.com/pinterest/pymemcache)를 사용하여 langchain과 Memcached를 함께 사용하는 방법을 다룹니다.

## Installation and Setup

<CodeGroup>
```bash pip
pip install pymemcache
```

```bash uv
uv add pymemcache
```
</CodeGroup>

## LLM Cache

애플리케이션에 Memcached Cache를 통합하려면:
```python3
from langchain.globals import set_llm_cache
from langchain_openai import OpenAI

from langchain_community.cache import MemcachedCache
from pymemcache.client.base import Client

llm = OpenAI(model="gpt-3.5-turbo-instruct", n=2, best_of=2)
set_llm_cache(MemcachedCache(Client('localhost')))

# The first time, it is not yet in cache, so it should take longer
llm.invoke("Which city is the most crowded city in the USA?")

# The second time it is, so it goes faster
llm.invoke("Which city is the most crowded city in the USA?")
```